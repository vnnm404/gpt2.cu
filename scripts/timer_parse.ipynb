{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1090,
   "id": "15f5d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    1: \"Embedding forward\",\n",
    "    2: \"LayerNorm 1\",\n",
    "    3: \"QKV projection\",\n",
    "    4: \"Attention\",\n",
    "    5: \"Attention projection\",\n",
    "    6: \"Residual 2\",\n",
    "    7: \"LayerNorm 2\",\n",
    "    8: \"MLP FC\",\n",
    "    9: \"GELU\",\n",
    "    10: \"MLP projection\",\n",
    "    11: \"Residual 3\",\n",
    "    12: \"Final LayerNorm\",\n",
    "    13: \"Logits\",\n",
    "    14: \"Softmax\",\n",
    "    15: \"Cross-entropy forward\",\n",
    "    16: \"Cross-entropy backward\",\n",
    "    17: \"Logits backward (input gradient)\",\n",
    "    18: \"Embedding weight gradient\",\n",
    "    19: \"Final LayerNorm backward\",\n",
    "    20: \"Residual backward (res_3)\",\n",
    "    21: \"MLP projection backward input\",\n",
    "    22: \"MLP projection backward weight\",\n",
    "    23: \"GELU backward\", \n",
    "    24: \"MLP FC backward input\",\n",
    "    25: \"MLP FC backward weight\",\n",
    "    26: \"LayerNorm 2 backward\",\n",
    "    27: \"Residual backward (res_2)\",\n",
    "    28: \"Attention projection backward input\",\n",
    "    29: \"Attention projection backward weight\",\n",
    "    30: \"Attention backward\",\n",
    "    31: \"QKV backward input\",\n",
    "    32: \"QKV backward weight\",\n",
    "    33: \"LayerNorm 1 backward\",\n",
    "    34: \"Embedding backward\",\n",
    "    35: \"AdamW update\"\n",
    "}\n",
    "\n",
    "op_kernels = {\n",
    "    1: \"embedding_forward_device\",\n",
    "    2: \"layernorm_forward_device\",\n",
    "    3: \"mlp_forward_device\",\n",
    "    4: \"attention_forward_device\",\n",
    "    5: \"mlp_forward_device\",\n",
    "    6: \"residual_forward_device\",\n",
    "    7: \"layernorm_forward_device\",\n",
    "    8: \"mlp_forward_device\",\n",
    "    9: \"gelu_forward_device\",\n",
    "    10: \"mlp_forward_device\",\n",
    "    11: \"residual_forward_device\",\n",
    "    12: \"layernorm_forward_device\",\n",
    "    13: \"mlp_forward_device\",\n",
    "    14: \"softmax_forward_device\",\n",
    "    15: \"cross_entropy_forward_device\",\n",
    "    16: \"cross_entropy_backward_device\",\n",
    "    17: \"mlp_backward_input_device\",\n",
    "    18: \"mlp_backward_weight_device\",\n",
    "    19: \"layernorm_backward_device\",\n",
    "    20: \"residual_backward_device\",\n",
    "    21: \"mlp_backward_input_device\",\n",
    "    22: \"mlp_backward_weight_device\",\n",
    "    23: \"gelu_backward_device\",\n",
    "    24: \"mlp_backward_input_device\",\n",
    "    25: \"mlp_backward_weight_device\",\n",
    "    26: \"layernorm_backward_device\",\n",
    "    27: \"residual_backward_device\",\n",
    "    28: \"mlp_backward_input_device\",\n",
    "    29: \"mlp_backward_weight_device\",\n",
    "    30: \"attention_backward_device\",\n",
    "    31: \"mlp_backward_input_device\",\n",
    "    32: \"mlp_backward_weight_device\",\n",
    "    33: \"layernorm_backward_device\",\n",
    "    34: \"embedding_backward_device\",\n",
    "    35: \"adamw_kernel_device\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "id": "2081171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SM = 28\n",
    "NUM_STEPS = 10\n",
    "streams = {i:{} for i in range(NUM_SM)}\n",
    "with open(\"streams.txt\") as timer_f:\n",
    "    all = timer_f.read()\n",
    "    for line in all.split('\\n')[:-1]:\n",
    "        # print(line)\n",
    "        d = ({x.split('=')[0]:int(x.split('=')[1]) for x in line.split(',')})\n",
    "        # print(d)\n",
    "        streams[d[\"sm\"]][d[\"instr\"]] = d\n",
    "\n",
    "timer = {i:[{} for x in range(NUM_SM)] for i in range(NUM_STEPS)}\n",
    "with open(\"timer.txt\") as timer_f:\n",
    "    all = timer_f.read()\n",
    "    for line in all.split('\\n')[:-1]:\n",
    "        # print(line)\n",
    "        d = ({x.split('=')[0]:int(x.split('=')[1]) for x in line.split(',')})\n",
    "        for k, v in streams[d[\"sm\"]][d[\"instr\"]].items():\n",
    "            d[k] = v\n",
    "        d[\"name\"] = names[d[\"op\"]]\n",
    "        d[\"kernelName\"] = op_kernels[d[\"op\"]]\n",
    "        # print(d)\n",
    "        timer[d[\"step\"]][d[\"sm\"]][d[\"instr\"]] = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "id": "74a87421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 0,\n",
       " 'sm': 0,\n",
       " 'instr': 123,\n",
       " 'bar_enter': 47291392,\n",
       " 'bar_exit': 47293440,\n",
       " 'instr_end': 49072128,\n",
       " 'spin_wait': 0,\n",
       " 'exec_time': 1,\n",
       " 'op': 14,\n",
       " 'layer': -1,\n",
       " 'bar_idx': 122,\n",
       " 'expected': 12568,\n",
       " 'start_b_x': 121,\n",
       " 'end_b_x': 129,\n",
       " 'inc': 1,\n",
       " 'name': 'Softmax',\n",
       " 'kernelName': 'softmax_forward_device'}"
      ]
     },
     "execution_count": 1092,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer[0][0][123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "id": "8d6c9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "id": "e1ea09ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_forward_device \t = \t 0.0ms | \t[0.31744] ms\n",
      "layernorm_forward_device \t = \t 0.031744ms | \t[0.016384 0.016384 0.01536  0.01536  0.016384 0.01536  0.01536  0.01536\n",
      " 0.016384 0.014336 0.017408 0.01536  0.01536  0.016384 0.016384 0.01536\n",
      " 0.016384 0.016384 0.01536  0.01536  0.01536  0.017408 0.01536  0.01536\n",
      " 0.016384] ms\n",
      "mlp_forward_device \t = \t 0.139264ms | \t[ 0.545792  0.193536  0.735232  0.756736  0.570368  0.192512  0.760832\n",
      "  0.755712  0.571392  0.191488  0.734208  0.756736  0.543744  0.190464\n",
      "  0.734208  0.756736  0.570368  0.192512  0.759808  0.755712  0.571392\n",
      "  0.164864  0.734208  0.755712  0.54272   0.192512  0.734208  0.754688\n",
      "  0.570368  0.191488  0.761856  0.754688  0.543744  0.164864  0.734208\n",
      "  0.755712  0.543744  0.191488  0.761856  0.754688  0.571392  0.191488\n",
      "  0.760832  0.647168  0.54272   0.192512  0.734208  0.755712 12.6464  ] ms\n",
      "attention_forward_device \t = \t 0.167936ms | \t[0.472064 0.477184 0.47616  0.477184 0.478208 0.473088 0.47616  0.475136\n",
      " 0.474112 0.477184 0.474112 0.473088] ms\n",
      "residual_forward_device \t = \t 0.16896ms | \t[0.01024  0.011264 0.011264 0.011264 0.01024  0.009216 0.01024  0.01024\n",
      " 0.01024  0.01024  0.011264 0.01024  0.01024  0.011264 0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.011264 0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.191488ms | \t[0.022528 0.023552 0.022528 0.022528 0.022528 0.022528 0.022528 0.023552\n",
      " 0.022528 0.022528 0.021504 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.002048ms | \t[1.778688] ms\n",
      "cross_entropy_backward_device \t = \t 0.159744ms | \t[0.771072] ms\n",
      "mlp_backward_input_device \t = \t 0.873472ms | \t[13.719552  0.973824  0.96256   0.212992  0.72192   0.937984  0.96256\n",
      "  0.24576   0.698368  0.937984  0.924672  0.202752  0.694272  0.903168\n",
      "  0.934912  0.237568  0.69632   0.934912  0.925696  0.202752  0.695296\n",
      "  0.903168  0.93184   0.237568  0.697344  0.940032  0.929792  0.203776\n",
      "  0.69632   0.887808  0.908288  0.232448  0.681984  0.919552  0.91136\n",
      "  0.19968   0.679936  0.88064   0.900096  0.2304    0.676864  0.909312\n",
      "  0.912384  0.198656  0.677888  0.876544  0.902144  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 0.605184ms | \t[14.42816   0.88576   0.948224  0.2304    0.664576  0.87552   0.934912\n",
      "  0.232448  0.638976  0.851968  0.909312  0.221184  0.638976  0.840704\n",
      "  0.900096  0.2304    0.638976  0.850944  0.909312  0.22016   0.641024\n",
      "  0.845824  0.904192  0.231424  0.642048  0.856064  0.914432  0.22016\n",
      "  0.63488   0.828416  0.887808  0.227328  0.627712  0.836608  0.896\n",
      "  0.217088  0.623616  0.821248  0.882688  0.224256  0.623616  0.830464\n",
      "  0.887808  0.216064  0.62464   0.821248  0.879616  0.224256  0.621568] ms\n",
      "layernorm_backward_device \t = \t 3.10272ms | \t[0.029696 0.033792 0.027648 0.03072  0.026624 0.032768 0.0256   0.029696\n",
      " 0.027648 0.031744 0.027648 0.029696 0.024576 0.031744 0.026624 0.028672\n",
      " 0.026624 0.031744 0.026624 0.027648 0.026624 0.032768 0.0256   0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.047104ms | \t[0.013312 0.019456 0.014336 0.021504 0.014336 0.019456 0.014336 0.02048\n",
      " 0.014336 0.019456 0.013312 0.021504 0.013312 0.02048  0.013312 0.02048\n",
      " 0.013312 0.019456 0.013312 0.019456 0.014336 0.019456 0.013312 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.202176ms | \t[0.088064 0.08704  0.086016 0.08704  0.086016 0.086016 0.08704  0.084992\n",
      " 0.08704  0.084992 0.084992 0.084992] ms\n",
      "attention_backward_device \t = \t 0.05632ms | \t[1.523712 1.555456 1.4848   1.50528  1.475584 1.517568 1.482752 1.513472\n",
      " 1.460224 1.518592 1.461248 1.497088] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.163072] ms\n",
      "total_time(0)=173.79942400000004|7.2980480000000005\n",
      "embedding_forward_device \t = \t 0.0ms | \t[0.31744] ms\n",
      "layernorm_forward_device \t = \t 0.031744ms | \t[0.016384 0.017408 0.01536  0.014336 0.014336 0.016384 0.016384 0.016384\n",
      " 0.014336 0.01536  0.016384 0.01536  0.016384 0.01536  0.01536  0.014336\n",
      " 0.017408 0.01536  0.01536  0.01536  0.01536  0.01536  0.01536  0.014336\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.151552ms | \t[ 0.545792  0.193536  0.735232  0.756736  0.570368  0.192512  0.760832\n",
      "  0.755712  0.571392  0.191488  0.733184  0.648192  0.544768  0.190464\n",
      "  0.733184  0.756736  0.569344  0.192512  0.759808  0.755712  0.571392\n",
      "  0.164864  0.734208  0.755712  0.54272   0.192512  0.733184  0.754688\n",
      "  0.570368  0.192512  0.761856  0.754688  0.543744  0.16384   0.733184\n",
      "  0.755712  0.543744  0.191488  0.734208  0.754688  0.571392  0.191488\n",
      "  0.760832  0.754688  0.54272   0.192512  0.734208  0.754688 12.6464  ] ms\n",
      "attention_forward_device \t = \t 0.165888ms | \t[0.473088 0.477184 0.477184 0.47104  0.478208 0.475136 0.477184 0.475136\n",
      " 0.473088 0.47616  0.47616  0.473088] ms\n",
      "residual_forward_device \t = \t 0.164864ms | \t[0.01024  0.01024  0.011264 0.01024  0.01024  0.009216 0.01024  0.011264\n",
      " 0.011264 0.011264 0.01024  0.01024  0.01024  0.011264 0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.011264] ms\n",
      "gelu_forward_device \t = \t 0.22016ms | \t[0.021504 0.022528 0.022528 0.021504 0.022528 0.022528 0.023552 0.022528\n",
      " 0.021504 0.021504 0.022528 0.023552] ms\n",
      "softmax_forward_device \t = \t 0.002048ms | \t[1.782784] ms\n",
      "cross_entropy_backward_device \t = \t 0.15872ms | \t[0.755712] ms\n",
      "mlp_backward_input_device \t = \t 0.898048ms | \t[13.681664  0.974848  0.960512  0.211968  0.621568  0.93696   0.971776\n",
      "  0.243712  0.695296  0.935936  0.925696  0.202752  0.595968  0.902144\n",
      "  0.924672  0.238592  0.69632   0.934912  0.92672   0.202752  0.594944\n",
      "  0.904192  0.928768  0.237568  0.697344  0.941056  0.945152  0.203776\n",
      "  0.595968  0.88576   0.91136   0.232448  0.683008  0.919552  0.910336\n",
      "  0.201728  0.579584  0.88064   0.90112   0.2304    0.674816  0.910336\n",
      "  0.902144  0.198656  0.581632  0.876544  0.903168  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 1.230848ms | \t[14.444544  0.881664  0.946176  0.2304    0.649216  0.872448  0.934912\n",
      "  0.232448  0.63488   0.847872  0.909312  0.221184  0.623616  0.837632\n",
      "  0.90112   0.2304    0.63488   0.847872  0.909312  0.22016   0.625664\n",
      "  0.842752  0.904192  0.231424  0.635904  0.850944  0.912384  0.22016\n",
      "  0.621568  0.82432   0.887808  0.226304  0.623616  0.832512  0.896\n",
      "  0.217088  0.610304  0.817152  0.882688  0.224256  0.618496  0.827392\n",
      "  0.887808  0.216064  0.60928   0.818176  0.879616  0.224256  0.618496] ms\n",
      "layernorm_backward_device \t = \t 3.204096ms | \t[0.028672 0.033792 0.027648 0.03072  0.026624 0.032768 0.0256   0.028672\n",
      " 0.026624 0.031744 0.027648 0.029696 0.0256   0.032768 0.026624 0.028672\n",
      " 0.026624 0.03072  0.0256   0.027648 0.0256   0.031744 0.026624 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.055296ms | \t[0.014336 0.02048  0.013312 0.019456 0.014336 0.021504 0.014336 0.02048\n",
      " 0.014336 0.02048  0.014336 0.02048  0.014336 0.022528 0.013312 0.02048\n",
      " 0.014336 0.02048  0.013312 0.02048  0.014336 0.02048  0.013312 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.246208ms | \t[0.088064 0.088064 0.086016 0.086016 0.086016 0.088064 0.084992 0.086016\n",
      " 0.086016 0.083968 0.084992 0.084992] ms\n",
      "attention_backward_device \t = \t 0.057344ms | \t[1.504256 1.536    1.492992 1.511424 1.498112 1.524736 1.472512 1.517568\n",
      " 1.50016  1.481728 1.4592   1.487872] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.069888] ms\n",
      "total_time(1)=172.865536|8.137728\n",
      "embedding_forward_device \t = \t 0.0ms | \t[0.318464] ms\n",
      "layernorm_forward_device \t = \t 0.021504ms | \t[0.01536  0.016384 0.016384 0.01536  0.014336 0.018432 0.016384 0.016384\n",
      " 0.01536  0.01536  0.016384 0.01536  0.014336 0.01536  0.01536  0.014336\n",
      " 0.018432 0.016384 0.01536  0.01536  0.01536  0.014336 0.01536  0.01536\n",
      " 0.016384] ms\n",
      "mlp_forward_device \t = \t 0.08704ms | \t[ 0.546816  0.193536  0.735232  0.755712  0.571392  0.192512  0.760832\n",
      "  0.755712  0.571392  0.191488  0.733184  0.648192  0.544768  0.191488\n",
      "  0.733184  0.756736  0.569344  0.192512  0.759808  0.755712  0.570368\n",
      "  0.191488  0.734208  0.755712  0.54272   0.191488  0.733184  0.754688\n",
      "  0.570368  0.192512  0.760832  0.754688  0.570368  0.164864  0.734208\n",
      "  0.755712  0.543744  0.191488  0.734208  0.754688  0.571392  0.191488\n",
      "  0.760832  0.754688  0.54272   0.164864  0.734208  0.755712 12.6464  ] ms\n",
      "attention_forward_device \t = \t 0.13824ms | \t[0.478208 0.48128  0.480256 0.47616  0.48128  0.48128  0.482304 0.480256\n",
      " 0.479232 0.48128  0.480256 0.479232] ms\n",
      "residual_forward_device \t = \t 0.169984ms | \t[0.01024  0.01024  0.01024  0.01024  0.011264 0.009216 0.01024  0.01024\n",
      " 0.011264 0.011264 0.01024  0.011264 0.01024  0.01024  0.011264 0.011264\n",
      " 0.011264 0.01024  0.01024  0.01024  0.011264 0.011264 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.216064ms | \t[0.024576 0.023552 0.023552 0.023552 0.022528 0.023552 0.024576 0.022528\n",
      " 0.023552 0.022528 0.021504 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.002048ms | \t[1.8176] ms\n",
      "cross_entropy_backward_device \t = \t 0.123904ms | \t[0.777216] ms\n",
      "mlp_backward_input_device \t = \t 0.310272ms | \t[13.681664  0.971776  0.96256   0.246784  0.621568  0.93696   0.96256\n",
      "  0.243712  0.695296  0.933888  0.939008  0.236544  0.595968  0.90112\n",
      "  0.92672   0.237568  0.69632   0.935936  0.939008  0.237568  0.594944\n",
      "  0.907264  0.93184   0.237568  0.695296  0.941056  0.927744  0.236544\n",
      "  0.595968  0.888832  0.909312  0.232448  0.681984  0.919552  0.909312\n",
      "  0.232448  0.580608  0.88064   0.915456  0.229376  0.67584   0.910336\n",
      "  0.903168  0.2304    0.581632  0.876544  0.909312  0.2304    0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 0.992256ms | \t[14.462976  0.88576   0.946176  0.2304    0.651264  0.874496  0.934912\n",
      "  0.232448  0.637952  0.851968  0.910336  0.22016   0.627712  0.840704\n",
      "  0.90112   0.2304    0.636928  0.850944  0.910336  0.221184  0.630784\n",
      "  0.845824  0.904192  0.231424  0.641024  0.85504   0.913408  0.22016\n",
      "  0.62464   0.827392  0.887808  0.226304  0.627712  0.837632  0.896\n",
      "  0.217088  0.612352  0.821248  0.882688  0.224256  0.621568  0.82944\n",
      "  0.887808  0.216064  0.613376  0.820224  0.88064   0.224256  0.622592] ms\n",
      "layernorm_backward_device \t = \t 3.142656ms | \t[0.029696 0.033792 0.027648 0.029696 0.026624 0.032768 0.026624 0.029696\n",
      " 0.027648 0.032768 0.026624 0.029696 0.026624 0.033792 0.026624 0.028672\n",
      " 0.026624 0.031744 0.026624 0.028672 0.027648 0.032768 0.026624 0.029696\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.0512ms | \t[0.014336 0.022528 0.014336 0.02048  0.014336 0.02048  0.014336 0.021504\n",
      " 0.014336 0.02048  0.014336 0.02048  0.014336 0.021504 0.014336 0.02048\n",
      " 0.014336 0.021504 0.013312 0.019456 0.014336 0.02048  0.014336 0.022528] ms\n",
      "gelu_backward_device \t = \t 1.205248ms | \t[0.091136 0.089088 0.088064 0.088064 0.088064 0.088064 0.088064 0.086016\n",
      " 0.08704  0.086016 0.08704  0.08704 ] ms\n",
      "attention_backward_device \t = \t 0.057344ms | \t[1.569792 1.573888 1.545216 1.563648 1.537024 1.577984 1.54112  1.540096\n",
      " 1.527808 1.531904 1.486848 1.542144] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.177408] ms\n",
      "total_time(2)=174.047232|7.067647999999999\n",
      "embedding_forward_device \t = \t 0.0ms | \t[0.31744] ms\n",
      "layernorm_forward_device \t = \t 0.022528ms | \t[0.016384 0.016384 0.016384 0.01536  0.013312 0.017408 0.017408 0.014336\n",
      " 0.016384 0.01536  0.01536  0.01536  0.016384 0.014336 0.014336 0.014336\n",
      " 0.017408 0.01536  0.01536  0.01536  0.01536  0.014336 0.017408 0.014336\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.147456ms | \t[ 0.546816  0.193536  0.735232  0.756736  0.571392  0.192512  0.761856\n",
      "  0.754688  0.571392  0.191488  0.761856  0.648192  0.544768  0.192512\n",
      "  0.733184  0.756736  0.570368  0.192512  0.759808  0.755712  0.570368\n",
      "  0.191488  0.734208  0.647168  0.54272   0.192512  0.733184  0.755712\n",
      "  0.570368  0.192512  0.761856  0.754688  0.571392  0.164864  0.733184\n",
      "  0.755712  0.543744  0.191488  0.734208  0.754688  0.571392  0.191488\n",
      "  0.760832  0.754688  0.54272   0.164864  0.735232  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.140288ms | \t[0.473088 0.47616  0.478208 0.47104  0.477184 0.47616  0.47104  0.475136\n",
      " 0.47616  0.477184 0.474112 0.473088] ms\n",
      "residual_forward_device \t = \t 0.280576ms | \t[0.011264 0.009216 0.009216 0.01024  0.011264 0.01024  0.011264 0.01024\n",
      " 0.011264 0.01024  0.01024  0.01024  0.011264 0.011264 0.01024  0.011264\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.011264 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.19456ms | \t[0.022528 0.022528 0.024576 0.022528 0.023552 0.022528 0.023552 0.021504\n",
      " 0.021504 0.022528 0.022528 0.021504] ms\n",
      "softmax_forward_device \t = \t 0.018432ms | \t[1.774592] ms\n",
      "cross_entropy_backward_device \t = \t 0.167936ms | \t[0.753664] ms\n",
      "mlp_backward_input_device \t = \t 0.882688ms | \t[13.681664  0.9728    0.976896  0.247808  0.620544  0.937984  0.961536\n",
      "  0.244736  0.695296  0.933888  0.925696  0.236544  0.595968  0.90112\n",
      "  0.92672   0.237568  0.69632   0.935936  0.924672  0.236544  0.596992\n",
      "  0.905216  0.943104  0.239616  0.69632   0.941056  0.92672   0.236544\n",
      "  0.595968  0.888832  0.925696  0.234496  0.681984  0.919552  0.910336\n",
      "  0.232448  0.579584  0.88064   0.903168  0.229376  0.67584   0.910336\n",
      "  0.903168  0.231424  0.581632  0.876544  0.902144  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 1.010688ms | \t[14.466048  0.883712  0.946176  0.239616  0.65024   0.873472  0.934912\n",
      "  0.232448  0.64      0.847872  0.910336  0.2304    0.62464   0.83968\n",
      "  0.90112   0.2304    0.636928  0.848896  0.910336  0.2304    0.62464\n",
      "  0.8448    0.903168  0.2304    0.638976  0.851968  0.913408  0.229376\n",
      "  0.620544  0.826368  0.887808  0.226304  0.628736  0.83456   0.896\n",
      "  0.226304  0.60928   0.818176  0.882688  0.224256  0.622592  0.826368\n",
      "  0.887808  0.22528   0.60928   0.8192    0.879616  0.224256  0.621568] ms\n",
      "layernorm_backward_device \t = \t 3.151872ms | \t[0.029696 0.033792 0.027648 0.03072  0.026624 0.032768 0.026624 0.028672\n",
      " 0.026624 0.031744 0.026624 0.029696 0.027648 0.031744 0.026624 0.028672\n",
      " 0.0256   0.03072  0.0256   0.027648 0.0256   0.031744 0.026624 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.057344ms | \t[0.014336 0.021504 0.013312 0.02048  0.014336 0.021504 0.014336 0.019456\n",
      " 0.014336 0.019456 0.014336 0.02048  0.013312 0.02048  0.014336 0.019456\n",
      " 0.014336 0.019456 0.014336 0.019456 0.014336 0.019456 0.014336 0.019456] ms\n",
      "gelu_backward_device \t = \t 1.227776ms | \t[0.088064 0.08704  0.086016 0.086016 0.086016 0.084992 0.088064 0.083968\n",
      " 0.084992 0.084992 0.084992 0.084992] ms\n",
      "attention_backward_device \t = \t 0.002048ms | \t[1.528832 1.52064  1.4848   1.55136  1.481728 1.499136 1.490944 1.521664\n",
      " 1.467392 1.522688 1.462272 1.455104] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.118016] ms\n",
      "total_time(3)=173.171712|7.855104000000001\n",
      "layernorm_forward_device \t = \t 0.335872ms | \t[0.01536  0.017408 0.01536  0.01536  0.014336 0.016384 0.016384 0.016384\n",
      " 0.016384 0.01536  0.014336 0.018432 0.016384 0.01536  0.013312 0.016384\n",
      " 0.01536  0.01536  0.014336 0.016384 0.01536  0.01536  0.017408 0.01536\n",
      " 0.016384] ms\n",
      "mlp_forward_device \t = \t 0.144384ms | \t[ 0.546816  0.166912  0.735232  0.755712  0.543744  0.192512  0.761856\n",
      "  0.755712  0.570368  0.191488  0.760832  0.648192  0.543744  0.192512\n",
      "  0.733184  0.755712  0.571392  0.192512  0.759808  0.754688  0.570368\n",
      "  0.191488  0.733184  0.647168  0.54272   0.192512  0.733184  0.754688\n",
      "  0.570368  0.192512  0.761856  0.754688  0.571392  0.191488  0.733184\n",
      "  0.755712  0.543744  0.190464  0.734208  0.753664  0.570368  0.191488\n",
      "  0.760832  0.754688  0.570368  0.164864  0.734208  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.142336ms | \t[0.473088 0.47616  0.47616  0.47104  0.47616  0.474112 0.470016 0.47616\n",
      " 0.474112 0.474112 0.478208 0.473088] ms\n",
      "residual_forward_device \t = \t 0.27648ms | \t[0.009216 0.01024  0.01024  0.01024  0.011264 0.01024  0.009216 0.01024\n",
      " 0.01024  0.011264 0.01024  0.009216 0.01024  0.01024  0.011264 0.011264\n",
      " 0.01024  0.01024  0.01024  0.011264 0.01024  0.01024  0.01024  0.011264] ms\n",
      "gelu_forward_device \t = \t 0.192512ms | \t[0.024576 0.024576 0.022528 0.022528 0.022528 0.022528 0.022528 0.022528\n",
      " 0.022528 0.022528 0.022528 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.018432ms | \t[1.773568] ms\n",
      "cross_entropy_backward_device \t = \t 0.167936ms | \t[0.763904] ms\n",
      "mlp_backward_input_device \t = \t 0.652288ms | \t[13.705216  0.9728    0.96256   0.247808  0.620544  0.93696   0.96256\n",
      "  0.244736  0.695296  0.933888  0.925696  0.23552   0.595968  0.90112\n",
      "  0.933888  0.237568  0.695296  0.934912  0.924672  0.236544  0.596992\n",
      "  0.907264  0.930816  0.237568  0.69632   0.941056  0.927744  0.238592\n",
      "  0.595968  0.888832  0.907264  0.232448  0.684032  0.919552  0.910336\n",
      "  0.232448  0.579584  0.88064   0.900096  0.229376  0.676864  0.910336\n",
      "  0.915456  0.231424  0.581632  0.876544  0.903168  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 1.01376ms | \t[14.459904  0.874496  0.946176  0.239616  0.656384  0.871424  0.934912\n",
      "  0.232448  0.638976  0.840704  0.909312  0.2304    0.62976   0.838656\n",
      "  0.90112   0.2304    0.636928  0.840704  0.909312  0.229376  0.635904\n",
      "  0.843776  0.904192  0.231424  0.638976  0.8448    0.913408  0.229376\n",
      "  0.62976   0.825344  0.887808  0.226304  0.62976   0.826368  0.894976\n",
      "  0.226304  0.618496  0.818176  0.882688  0.224256  0.621568  0.8192\n",
      "  0.887808  0.22528   0.617472  0.817152  0.88064   0.224256  0.621568] ms\n",
      "layernorm_backward_device \t = \t 3.117056ms | \t[0.029696 0.029696 0.027648 0.03072  0.026624 0.028672 0.026624 0.029696\n",
      " 0.026624 0.028672 0.027648 0.029696 0.026624 0.029696 0.0256   0.028672\n",
      " 0.026624 0.028672 0.0256   0.027648 0.026624 0.028672 0.026624 0.028672\n",
      " 0.0256  ] ms\n",
      "residual_backward_device \t = \t 0.06656ms | \t[0.014336 0.022528 0.013312 0.02048  0.014336 0.021504 0.014336 0.02048\n",
      " 0.014336 0.022528 0.013312 0.02048  0.014336 0.02048  0.013312 0.019456\n",
      " 0.013312 0.02048  0.013312 0.02048  0.014336 0.02048  0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.284096ms | \t[0.088064 0.08704  0.084992 0.086016 0.08704  0.084992 0.086016 0.084992\n",
      " 0.083968 0.083968 0.086016 0.084992] ms\n",
      "attention_backward_device \t = \t 0.003072ms | \t[1.544192 1.531904 1.506304 1.553408 1.509376 1.521664 1.5104   1.526784\n",
      " 1.488896 1.53088  1.51552  1.481728] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.106752] ms\n",
      "total_time(4)=173.04371199999997|7.965696\n",
      "layernorm_forward_device \t = \t 0.34304ms | \t[0.018432 0.017408 0.016384 0.01536  0.01536  0.017408 0.016384 0.014336\n",
      " 0.01536  0.016384 0.01536  0.018432 0.016384 0.01536  0.01536  0.01536\n",
      " 0.014336 0.01536  0.01536  0.01536  0.01536  0.016384 0.016384 0.01536\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.069632ms | \t[ 0.545792  0.166912  0.735232  0.756736  0.54272   0.191488  0.735232\n",
      "  0.754688  0.570368  0.191488  0.760832  0.755712  0.543744  0.192512\n",
      "  0.733184  0.755712  0.571392  0.191488  0.760832  0.755712  0.570368\n",
      "  0.191488  0.759808  0.647168  0.54272   0.192512  0.733184  0.754688\n",
      "  0.570368  0.191488  0.761856  0.754688  0.570368  0.191488  0.733184\n",
      "  0.647168  0.543744  0.190464  0.734208  0.753664  0.570368  0.190464\n",
      "  0.760832  0.754688  0.570368  0.16384   0.734208  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.142336ms | \t[0.480256 0.483328 0.48128  0.479232 0.483328 0.482304 0.478208 0.482304\n",
      " 0.48128  0.479232 0.483328 0.483328] ms\n",
      "residual_forward_device \t = \t 0.273408ms | \t[0.011264 0.011264 0.011264 0.009216 0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.011264 0.01024  0.01024  0.011264 0.01024  0.01024  0.011264\n",
      " 0.01024  0.01024  0.011264 0.01024  0.01024  0.011264 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.19456ms | \t[0.023552 0.022528 0.022528 0.022528 0.022528 0.022528 0.023552 0.022528\n",
      " 0.022528 0.022528 0.023552 0.02048 ] ms\n",
      "softmax_forward_device \t = \t 0.019456ms | \t[1.814528] ms\n",
      "cross_entropy_backward_device \t = \t 0.126976ms | \t[0.777216] ms\n",
      "mlp_backward_input_device \t = \t 0.292864ms | \t[13.705216  0.973824  0.96256   0.246784  0.723968  0.93696   0.973824\n",
      "  0.243712  0.69632   0.93696   0.924672  0.236544  0.69632   0.900096\n",
      "  0.924672  0.238592  0.695296  0.932864  0.924672  0.236544  0.695296\n",
      "  0.905216  0.928768  0.237568  0.697344  0.943104  0.945152  0.236544\n",
      "  0.694272  0.887808  0.909312  0.232448  0.684032  0.922624  0.909312\n",
      "  0.234496  0.677888  0.879616  0.903168  0.231424  0.67584   0.91136\n",
      "  0.90112   0.2304    0.677888  0.876544  0.903168  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 0.402432ms | \t[14.456832  0.877568  0.946176  0.239616  0.648192  0.87552   0.934912\n",
      "  0.222208  0.64      0.83968   0.909312  0.229376  0.622592  0.840704\n",
      "  0.90112   0.219136  0.637952  0.83968   0.91136   0.229376  0.625664\n",
      "  0.8448    0.904192  0.221184  0.638976  0.842752  0.914432  0.229376\n",
      "  0.620544  0.827392  0.887808  0.216064  0.628736  0.825344  0.894976\n",
      "  0.226304  0.608256  0.820224  0.882688  0.214016  0.621568  0.820224\n",
      "  0.888832  0.22528   0.608256  0.821248  0.879616  0.214016  0.623616] ms\n",
      "layernorm_backward_device \t = \t 3.16928ms | \t[0.03072  0.03072  0.027648 0.03072  0.026624 0.028672 0.026624 0.029696\n",
      " 0.027648 0.028672 0.027648 0.029696 0.027648 0.029696 0.026624 0.029696\n",
      " 0.026624 0.028672 0.026624 0.028672 0.026624 0.028672 0.026624 0.028672\n",
      " 0.027648] ms\n",
      "residual_backward_device \t = \t 0.059392ms | \t[0.013312 0.021504 0.014336 0.019456 0.01536  0.021504 0.01536  0.021504\n",
      " 0.014336 0.021504 0.014336 0.021504 0.014336 0.02048  0.013312 0.021504\n",
      " 0.016384 0.021504 0.012288 0.02048  0.014336 0.02048  0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.270784ms | \t[0.090112 0.088064 0.088064 0.088064 0.088064 0.08704  0.089088 0.086016\n",
      " 0.08704  0.083968 0.08704  0.086016] ms\n",
      "attention_backward_device \t = \t 0.062464ms | \t[1.575936 1.579008 1.523712 1.573888 1.519616 1.577984 1.534976 1.545216\n",
      " 1.54112  1.54112  1.54112  1.511424] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.205056] ms\n",
      "total_time(5)=174.16908800000002|6.9765120000000005\n",
      "layernorm_forward_device \t = \t 0.360448ms | \t[0.017408 0.017408 0.014336 0.01536  0.01536  0.014336 0.01536  0.01536\n",
      " 0.01536  0.01536  0.01536  0.016384 0.016384 0.01536  0.01536  0.014336\n",
      " 0.014336 0.017408 0.01536  0.014336 0.01536  0.01536  0.01536  0.016384\n",
      " 0.016384] ms\n",
      "mlp_forward_device \t = \t 0.198656ms | \t[ 0.57344   0.166912  0.735232  0.756736  0.54272   0.192512  0.734208\n",
      "  0.754688  0.570368  0.191488  0.761856  0.755712  0.543744  0.164864\n",
      "  0.733184  0.755712  0.543744  0.192512  0.760832  0.755712  0.570368\n",
      "  0.191488  0.760832  0.647168  0.543744  0.193536  0.733184  0.754688\n",
      "  0.571392  0.192512  0.761856  0.754688  0.570368  0.191488  0.734208\n",
      "  0.647168  0.543744  0.191488  0.734208  0.753664  0.570368  0.191488\n",
      "  0.759808  0.754688  0.570368  0.191488  0.734208  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.13824ms | \t[0.468992 0.475136 0.472064 0.468992 0.473088 0.47104  0.467968 0.474112\n",
      " 0.472064 0.467968 0.47104  0.474112] ms\n",
      "residual_forward_device \t = \t 0.27648ms | \t[0.01024  0.01024  0.009216 0.01024  0.011264 0.01024  0.01024  0.011264\n",
      " 0.011264 0.01024  0.01024  0.01024  0.01024  0.009216 0.01024  0.01024\n",
      " 0.01024  0.009216 0.011264 0.01024  0.01024  0.01024  0.01024  0.009216] ms\n",
      "gelu_forward_device \t = \t 0.195584ms | \t[0.023552 0.021504 0.022528 0.021504 0.023552 0.022528 0.022528 0.022528\n",
      " 0.021504 0.022528 0.022528 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.019456ms | \t[1.743872] ms\n",
      "cross_entropy_backward_device \t = \t 0.198656ms | \t[0.74752] ms\n",
      "mlp_backward_input_device \t = \t 1.099776ms | \t[13.705216  0.9728    0.963584  0.247808  0.723968  0.93696   0.963584\n",
      "  0.24576   0.697344  0.935936  0.94208   0.237568  0.695296  0.902144\n",
      "  0.925696  0.237568  0.695296  0.933888  0.943104  0.239616  0.69632\n",
      "  0.905216  0.928768  0.239616  0.69632   0.94208   0.927744  0.237568\n",
      "  0.694272  0.887808  0.912384  0.232448  0.685056  0.922624  0.910336\n",
      "  0.234496  0.677888  0.878592  0.918528  0.231424  0.676864  0.91136\n",
      "  0.904192  0.2304    0.677888  0.876544  0.91136   0.231424  0.67584 ] ms\n",
      "mlp_backward_weight_device \t = \t 0.36864ms | \t[14.418944  0.873472  0.946176  0.239616  0.928768  0.871424  0.934912\n",
      "  0.222208  0.643072  0.838656  0.910336  0.229376  0.888832  0.837632\n",
      "  0.90112   0.22016   0.641024  0.837632  0.909312  0.2304    0.879616\n",
      "  0.841728  0.904192  0.219136  0.644096  0.840704  0.912384  0.229376\n",
      "  0.879616  0.823296  0.887808  0.217088  0.630784  0.825344  0.894976\n",
      "  0.226304  0.867328  0.816128  0.881664  0.214016  0.626688  0.818176\n",
      "  0.887808  0.224256  0.86016   0.816128  0.879616  0.214016  0.625664] ms\n",
      "layernorm_backward_device \t = \t 1.62304ms | \t[0.029696 0.03072  0.026624 0.029696 0.0256   0.028672 0.0256   0.028672\n",
      " 0.0256   0.028672 0.026624 0.028672 0.0256   0.028672 0.024576 0.027648\n",
      " 0.0256   0.027648 0.0256   0.027648 0.0256   0.027648 0.024576 0.029696\n",
      " 0.0256  ] ms\n",
      "residual_backward_device \t = \t 0.080896ms | \t[0.012288 0.02048  0.013312 0.02048  0.013312 0.02048  0.014336 0.019456\n",
      " 0.014336 0.022528 0.013312 0.02048  0.013312 0.02048  0.013312 0.019456\n",
      " 0.014336 0.02048  0.014336 0.018432 0.013312 0.019456 0.013312 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.301504ms | \t[0.08704  0.084992 0.083968 0.083968 0.083968 0.082944 0.084992 0.082944\n",
      " 0.083968 0.080896 0.083968 0.08192 ] ms\n",
      "attention_backward_device \t = \t 0.062464ms | \t[1.54112  1.488896 1.478656 1.485824 1.509376 1.555456 1.508352 1.440768\n",
      " 1.440768 1.43872  1.4336   1.5104  ] ms\n",
      "adamw_kernel_device \t = \t 0.551936ms | \t[11.948032] ms\n",
      "total_time(6)=174.42201600000004|6.475776\n",
      "layernorm_forward_device \t = \t 0.362496ms | \t[0.017408 0.017408 0.01536  0.01536  0.01536  0.01536  0.017408 0.01536\n",
      " 0.016384 0.01536  0.01536  0.016384 0.016384 0.01536  0.01536  0.014336\n",
      " 0.014336 0.017408 0.01536  0.016384 0.01536  0.014336 0.014336 0.01536\n",
      " 0.016384] ms\n",
      "mlp_forward_device \t = \t 0.20992ms | \t[ 0.57344   0.166912  0.735232  0.756736  0.54272   0.191488  0.733184\n",
      "  0.754688  0.570368  0.191488  0.760832  0.755712  0.54272   0.164864\n",
      "  0.733184  0.755712  0.54272   0.191488  0.734208  0.755712  0.570368\n",
      "  0.191488  0.760832  0.754688  0.543744  0.192512  0.73216   0.754688\n",
      "  0.570368  0.191488  0.76288   0.754688  0.570368  0.191488  0.760832\n",
      "  0.647168  0.543744  0.191488  0.733184  0.753664  0.571392  0.191488\n",
      "  0.759808  0.754688  0.569344  0.191488  0.734208  0.647168 12.628992] ms\n",
      "attention_forward_device \t = \t 0.141312ms | \t[0.467968 0.473088 0.47104  0.467968 0.473088 0.47104  0.466944 0.472064\n",
      " 0.470016 0.468992 0.472064 0.472064] ms\n",
      "residual_forward_device \t = \t 0.27648ms | \t[0.01024  0.009216 0.009216 0.01024  0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.01024  0.011264 0.009216 0.01024  0.009216 0.01024  0.01024\n",
      " 0.01024  0.009216 0.011264 0.01024  0.01024  0.011264 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.196608ms | \t[0.022528 0.022528 0.021504 0.021504 0.023552 0.021504 0.022528 0.022528\n",
      " 0.021504 0.022528 0.022528 0.021504] ms\n",
      "softmax_forward_device \t = \t 0.019456ms | \t[1.755136] ms\n",
      "cross_entropy_backward_device \t = \t 0.187392ms | \t[0.75264] ms\n",
      "mlp_backward_input_device \t = \t 1.053696ms | \t[13.705216  0.937984  0.973824  0.247808  0.723968  0.93696   0.961536\n",
      "  0.244736  0.697344  0.90112   0.92672   0.236544  0.69632   0.902144\n",
      "  0.92672   0.237568  0.695296  0.900096  0.924672  0.236544  0.695296\n",
      "  0.905216  0.943104  0.237568  0.697344  0.908288  0.92672   0.237568\n",
      "  0.694272  0.886784  0.925696  0.232448  0.685056  0.889856  0.910336\n",
      "  0.232448  0.677888  0.879616  0.902144  0.229376  0.676864  0.877568\n",
      "  0.902144  0.231424  0.677888  0.878592  0.902144  0.231424  0.67584 ] ms\n",
      "mlp_backward_weight_device \t = \t 0.59904ms | \t[14.459904  0.873472  0.935936  0.239616  0.717824  0.872448  0.933888\n",
      "  0.222208  0.65024   0.83968   0.899072  0.229376  0.692224  0.838656\n",
      "  0.90112   0.219136  0.649216  0.83968   0.898048  0.229376  0.692224\n",
      "  0.843776  0.903168  0.22016   0.651264  0.843776  0.902144  0.229376\n",
      "  0.68608   0.825344  0.887808  0.216064  0.638976  0.827392  0.884736\n",
      "  0.226304  0.669696  0.8192    0.882688  0.214016  0.633856  0.818176\n",
      "  0.878592  0.224256  0.674816  0.8192    0.88064   0.214016  0.633856] ms\n",
      "layernorm_backward_device \t = \t 2.767872ms | \t[0.03072  0.029696 0.03072  0.03072  0.0256   0.028672 0.028672 0.028672\n",
      " 0.026624 0.028672 0.029696 0.029696 0.0256   0.028672 0.029696 0.028672\n",
      " 0.026624 0.028672 0.028672 0.027648 0.026624 0.027648 0.028672 0.028672\n",
      " 0.0256  ] ms\n",
      "residual_backward_device \t = \t 0.05632ms | \t[0.012288 0.02048  0.013312 0.02048  0.014336 0.021504 0.014336 0.02048\n",
      " 0.014336 0.02048  0.013312 0.02048  0.013312 0.019456 0.013312 0.019456\n",
      " 0.014336 0.02048  0.014336 0.018432 0.013312 0.021504 0.014336 0.019456] ms\n",
      "gelu_backward_device \t = \t 1.28512ms | \t[0.088064 0.083968 0.083968 0.084992 0.083968 0.083968 0.083968 0.082944\n",
      " 0.082944 0.082944 0.083968 0.08192 ] ms\n",
      "attention_backward_device \t = \t 0.063488ms | \t[1.54624  1.502208 1.502208 1.50016  1.502208 1.529856 1.509376 1.439744\n",
      " 1.445888 1.497088 1.441792 1.45408 ] ms\n",
      "adamw_kernel_device \t = \t 0.551936ms | \t[12.06272] ms\n",
      "total_time(7)=173.23622400000005|7.771136\n",
      "layernorm_forward_device \t = \t 0.347136ms | \t[0.017408 0.017408 0.01536  0.01536  0.01536  0.01536  0.017408 0.01536\n",
      " 0.016384 0.014336 0.016384 0.01536  0.01536  0.016384 0.016384 0.01536\n",
      " 0.014336 0.017408 0.01536  0.014336 0.016384 0.01536  0.01536  0.017408\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.136192ms | \t[ 0.572416  0.193536  0.735232  0.756736  0.54272   0.191488  0.733184\n",
      "  0.755712  0.570368  0.191488  0.760832  0.755712  0.571392  0.16384\n",
      "  0.733184  0.755712  0.54272   0.192512  0.733184  0.755712  0.569344\n",
      "  0.190464  0.761856  0.754688  0.543744  0.164864  0.733184  0.754688\n",
      "  0.543744  0.192512  0.76288   0.754688  0.570368  0.191488  0.759808\n",
      "  0.647168  0.543744  0.191488  0.734208  0.754688  0.571392  0.191488\n",
      "  0.759808  0.755712  0.570368  0.191488  0.734208  0.647168 12.630016] ms\n",
      "attention_forward_device \t = \t 0.140288ms | \t[0.474112 0.478208 0.477184 0.473088 0.478208 0.475136 0.472064 0.47616\n",
      " 0.475136 0.474112 0.477184 0.47616 ] ms\n",
      "residual_forward_device \t = \t 0.27648ms | \t[0.01024  0.01024  0.011264 0.01024  0.01024  0.01024  0.009216 0.01024\n",
      " 0.01024  0.011264 0.01024  0.011264 0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.01024  0.009216 0.011264 0.01024  0.011264 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.193536ms | \t[0.023552 0.022528 0.021504 0.022528 0.022528 0.022528 0.021504 0.022528\n",
      " 0.023552 0.022528 0.022528 0.021504] ms\n",
      "softmax_forward_device \t = \t 0.019456ms | \t[1.78688] ms\n",
      "cross_entropy_backward_device \t = \t 0.155648ms | \t[0.759808] ms\n",
      "mlp_backward_input_device \t = \t 0.78336ms | \t[13.674496  0.937984  0.96256   0.248832  0.72192   0.93696   0.825344\n",
      "  0.244736  0.695296  0.904192  0.925696  0.236544  0.694272  0.90112\n",
      "  0.797696  0.237568  0.695296  0.899072  0.924672  0.236544  0.69632\n",
      "  0.903168  0.797696  0.237568  0.695296  0.908288  0.930816  0.238592\n",
      "  0.694272  0.886784  0.77824   0.234496  0.685056  0.888832  0.908288\n",
      "  0.232448  0.676864  0.879616  0.772096  0.229376  0.67584   0.877568\n",
      "  0.915456  0.2304    0.678912  0.876544  0.77312   0.232448  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 1.439744ms | \t[14.465024  0.872448  0.935936  0.238592  0.672768  0.871424  0.934912\n",
      "  0.222208  0.641024  0.838656  0.899072  0.2304    0.646144  0.836608\n",
      "  0.90112   0.219136  0.64512   0.837632  0.900096  0.2304    0.649216\n",
      "  0.841728  0.904192  0.221184  0.647168  0.842752  0.902144  0.229376\n",
      "  0.644096  0.823296  0.887808  0.216064  0.635904  0.82432   0.884736\n",
      "  0.226304  0.630784  0.817152  0.882688  0.214016  0.62464   0.818176\n",
      "  0.878592  0.22528   0.630784  0.818176  0.879616  0.214016  0.627712] ms\n",
      "layernorm_backward_device \t = \t 3.048448ms | \t[0.029696 0.03072  0.029696 0.03072  0.026624 0.029696 0.028672 0.029696\n",
      " 0.026624 0.028672 0.028672 0.028672 0.0256   0.029696 0.029696 0.029696\n",
      " 0.026624 0.028672 0.029696 0.028672 0.0256   0.028672 0.029696 0.029696\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.053248ms | \t[0.013312 0.021504 0.014336 0.02048  0.01536  0.02048  0.013312 0.02048\n",
      " 0.013312 0.021504 0.014336 0.02048  0.013312 0.02048  0.01536  0.02048\n",
      " 0.013312 0.02048  0.014336 0.019456 0.013312 0.019456 0.013312 0.019456] ms\n",
      "gelu_backward_device \t = \t 1.303552ms | \t[0.08704  0.08704  0.084992 0.086016 0.084992 0.084992 0.086016 0.084992\n",
      " 0.083968 0.082944 0.084992 0.084992] ms\n",
      "attention_backward_device \t = \t 0.063488ms | \t[1.562624 1.516544 1.522688 1.527808 1.516544 1.557504 1.521664 1.45408\n",
      " 1.499136 1.458176 1.504256 1.458176] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.110848] ms\n",
      "total_time(8)=172.50304000000003|8.510464\n",
      "layernorm_forward_device \t = \t 0.34816ms | \t[0.01536  0.017408 0.01536  0.01536  0.014336 0.014336 0.016384 0.016384\n",
      " 0.016384 0.016384 0.01536  0.014336 0.017408 0.01536  0.01536  0.01536\n",
      " 0.014336 0.016384 0.01536  0.01536  0.01536  0.01536  0.01536  0.018432\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.169984ms | \t[ 0.57344   0.193536  0.735232  0.648192  0.543744  0.191488  0.733184\n",
      "  0.755712  0.570368  0.191488  0.760832  0.755712  0.572416  0.164864\n",
      "  0.733184  0.755712  0.54272   0.192512  0.733184  0.754688  0.570368\n",
      "  0.191488  0.761856  0.754688  0.543744  0.165888  0.733184  0.754688\n",
      "  0.543744  0.192512  0.735232  0.754688  0.570368  0.191488  0.760832\n",
      "  0.754688  0.543744  0.191488  0.734208  0.754688  0.571392  0.191488\n",
      "  0.759808  0.755712  0.569344  0.191488  0.761856  0.646144 12.628992] ms\n",
      "attention_forward_device \t = \t 0.142336ms | \t[0.472064 0.472064 0.475136 0.473088 0.475136 0.474112 0.472064 0.475136\n",
      " 0.473088 0.472064 0.47616  0.474112] ms\n",
      "residual_forward_device \t = \t 0.273408ms | \t[0.01024  0.01024  0.01024  0.011264 0.011264 0.011264 0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.011264\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.18944ms | \t[0.022528 0.023552 0.023552 0.022528 0.023552 0.022528 0.022528 0.022528\n",
      " 0.021504 0.022528 0.022528 0.023552] ms\n",
      "softmax_forward_device \t = \t 0.019456ms | \t[1.770496] ms\n",
      "cross_entropy_backward_device \t = \t 0.171008ms | \t[0.758784] ms\n",
      "mlp_backward_input_device \t = \t 0.644096ms | \t[13.674496  0.937984  0.960512  0.247808  0.723968  0.974848  0.832512\n",
      "  0.243712  0.695296  0.904192  0.925696  0.237568  0.69632   0.933888\n",
      "  0.7936    0.241664  0.697344  0.90112   0.925696  0.237568  0.69632\n",
      "  0.941056  0.796672  0.237568  0.695296  0.909312  0.946176  0.237568\n",
      "  0.695296  0.918528  0.779264  0.232448  0.685056  0.888832  0.912384\n",
      "  0.23552   0.676864  0.914432  0.77312   0.229376  0.67584   0.877568\n",
      "  0.902144  0.2304    0.677888  0.914432  0.775168  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 1.210368ms | \t[14.47424   0.874496  0.93696   0.239616  0.669696  0.872448  0.935936\n",
      "  0.223232  0.65536   0.83968   0.900096  0.229376  0.646144  0.838656\n",
      "  0.902144  0.219136  0.654336  0.83968   0.899072  0.2304    0.646144\n",
      "  0.842752  0.904192  0.222208  0.657408  0.843776  0.902144  0.229376\n",
      "  0.638976  0.825344  0.888832  0.217088  0.644096  0.825344  0.884736\n",
      "  0.226304  0.627712  0.817152  0.883712  0.21504   0.64      0.820224\n",
      "  0.878592  0.224256  0.626688  0.818176  0.881664  0.21504   0.637952] ms\n",
      "layernorm_backward_device \t = \t 2.99008ms | \t[0.029696 0.029696 0.03072  0.03072  0.0256   0.029696 0.028672 0.029696\n",
      " 0.026624 0.028672 0.029696 0.029696 0.0256   0.028672 0.028672 0.028672\n",
      " 0.0256   0.027648 0.028672 0.027648 0.0256   0.028672 0.029696 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.058368ms | \t[0.014336 0.02048  0.014336 0.019456 0.014336 0.02048  0.013312 0.021504\n",
      " 0.014336 0.02048  0.013312 0.02048  0.014336 0.02048  0.014336 0.02048\n",
      " 0.014336 0.02048  0.014336 0.02048  0.012288 0.019456 0.016384 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.287168ms | \t[0.089088 0.086016 0.084992 0.084992 0.084992 0.083968 0.086016 0.083968\n",
      " 0.083968 0.08192  0.084992 0.082944] ms\n",
      "attention_backward_device \t = \t 0.055296ms | \t[1.526784 1.559552 1.53088  1.553408 1.538048 1.5616   1.4848   1.50528\n",
      " 1.447936 1.52064  1.504256 1.528832] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.048384] ms\n",
      "total_time(9)=172.87987199999998|8.11008\n",
      "layernorm_forward_device \t = \t 0.344064ms | \t[0.01536  0.018432 0.014336 0.01536  0.014336 0.01536  0.016384 0.016384\n",
      " 0.016384 0.016384 0.016384 0.01536  0.018432 0.016384 0.014336 0.01536\n",
      " 0.01536  0.014336 0.01536  0.016384 0.016384 0.01536  0.014336 0.017408\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.166912ms | \t[ 0.57344   0.193536  0.735232  0.648192  0.54272   0.191488  0.734208\n",
      "  0.755712  0.570368  0.191488  0.760832  0.755712  0.572416  0.191488\n",
      "  0.734208  0.755712  0.543744  0.192512  0.733184  0.753664  0.570368\n",
      "  0.191488  0.760832  0.754688  0.571392  0.164864  0.733184  0.754688\n",
      "  0.543744  0.192512  0.734208  0.754688  0.570368  0.191488  0.760832\n",
      "  0.754688  0.543744  0.16384   0.734208  0.754688  0.543744  0.191488\n",
      "  0.760832  0.754688  0.570368  0.191488  0.761856  0.646144 12.602368] ms\n",
      "attention_forward_device \t = \t 0.139264ms | \t[0.472064 0.472064 0.475136 0.474112 0.475136 0.47616  0.472064 0.475136\n",
      " 0.472064 0.472064 0.474112 0.473088] ms\n",
      "residual_forward_device \t = \t 0.273408ms | \t[0.01024  0.01024  0.01024  0.011264 0.01024  0.011264 0.01024  0.01024\n",
      " 0.01024  0.01024  0.009216 0.01024  0.011264 0.01024  0.01024  0.011264\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.193536ms | \t[0.024576 0.022528 0.022528 0.021504 0.022528 0.022528 0.023552 0.022528\n",
      " 0.022528 0.022528 0.022528 0.023552] ms\n",
      "softmax_forward_device \t = \t 0.047104ms | \t[1.753088] ms\n",
      "cross_entropy_backward_device \t = \t 0.188416ms | \t[0.755712] ms\n",
      "mlp_backward_input_device \t = \t 1.052672ms | \t[13.674496  0.940032  0.963584  0.247808  0.72192   0.974848  0.826368\n",
      "  0.243712  0.695296  0.902144  0.937984  0.236544  0.695296  0.932864\n",
      "  0.7936    0.237568  0.695296  0.90112   0.94208   0.237568  0.69632\n",
      "  0.939008  0.796672  0.237568  0.69632   0.908288  0.927744  0.237568\n",
      "  0.694272  0.919552  0.780288  0.232448  0.685056  0.887808  0.909312\n",
      "  0.232448  0.676864  0.914432  0.786432  0.229376  0.676864  0.877568\n",
      "  0.904192  0.2304    0.678912  0.914432  0.780288  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 1.195008ms | \t[14.466048  0.8704    0.934912  0.239616  0.658432  0.986112  0.935936\n",
      "  0.223232  0.65536   0.837632  0.899072  0.2304    0.633856  0.945152\n",
      "  0.902144  0.22016   0.653312  0.837632  0.899072  0.2304    0.63488\n",
      "  0.949248  0.904192  0.222208  0.658432  0.83968   0.902144  0.229376\n",
      "  0.630784  0.928768  0.887808  0.217088  0.644096  0.823296  0.884736\n",
      "  0.226304  0.617472  0.927744  0.883712  0.21504   0.637952  0.816128\n",
      "  0.877568  0.224256  0.618496  0.925696  0.88064   0.21504   0.637952] ms\n",
      "layernorm_backward_device \t = \t 3.060736ms | \t[0.029696 0.029696 0.029696 0.03072  0.026624 0.028672 0.028672 0.028672\n",
      " 0.027648 0.028672 0.029696 0.028672 0.026624 0.029696 0.028672 0.028672\n",
      " 0.0256   0.028672 0.029696 0.027648 0.026624 0.028672 0.028672 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.057344ms | \t[0.013312 0.021504 0.013312 0.019456 0.013312 0.02048  0.01536  0.02048\n",
      " 0.014336 0.02048  0.013312 0.02048  0.014336 0.021504 0.014336 0.02048\n",
      " 0.014336 0.02048  0.013312 0.019456 0.012288 0.02048  0.013312 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 0.66048ms | \t[0.088064 0.086016 0.084992 0.083968 0.084992 0.083968 0.086016 0.082944\n",
      " 0.084992 0.083968 0.082944 0.082944] ms\n",
      "attention_backward_device \t = \t 0.05632ms | \t[1.548288 1.47968  1.509376 1.476608 1.491968 1.467392 1.513472 1.44896\n",
      " 1.49504  1.44384  1.528832 1.439744] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.09856] ms\n",
      "total_time(10)=173.023232|7.986176\n",
      "layernorm_forward_device \t = \t 0.349184ms | \t[0.016384 0.018432 0.01536  0.01536  0.01536  0.016384 0.01536  0.01536\n",
      " 0.01536  0.01536  0.01536  0.01536  0.017408 0.016384 0.01536  0.016384\n",
      " 0.016384 0.01536  0.018432 0.016384 0.014336 0.014336 0.01536  0.018432\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.114688ms | \t[ 0.57344   0.193536  0.761856  0.648192  0.543744  0.191488  0.734208\n",
      "  0.755712  0.571392  0.191488  0.759808  0.756736  0.570368  0.190464\n",
      "  0.734208  0.648192  0.54272   0.192512  0.733184  0.755712  0.570368\n",
      "  0.192512  0.761856  0.754688  0.571392  0.164864  0.733184  0.754688\n",
      "  0.543744  0.191488  0.735232  0.754688  0.570368  0.191488  0.759808\n",
      "  0.754688  0.543744  0.16384   0.734208  0.754688  0.543744  0.191488\n",
      "  0.733184  0.754688  0.570368  0.191488  0.761856  0.755712 12.601344] ms\n",
      "attention_forward_device \t = \t 0.139264ms | \t[0.477184 0.47616  0.477184 0.478208 0.473088 0.478208 0.477184 0.478208\n",
      " 0.478208 0.47616  0.478208 0.478208] ms\n",
      "residual_forward_device \t = \t 0.272384ms | \t[0.011264 0.01024  0.01024  0.01024  0.01024  0.011264 0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.011264 0.01024  0.01024  0.01024  0.011264\n",
      " 0.011264 0.01024  0.01024  0.011264 0.01024  0.01024  0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.19456ms | \t[0.023552 0.022528 0.022528 0.022528 0.023552 0.022528 0.022528 0.023552\n",
      " 0.022528 0.021504 0.022528 0.023552] ms\n",
      "softmax_forward_device \t = \t 0.047104ms | \t[1.799168] ms\n",
      "cross_entropy_backward_device \t = \t 0.144384ms | \t[0.765952] ms\n",
      "mlp_backward_input_device \t = \t 0.749568ms | \t[13.715456  0.937984  0.975872  0.247808  0.72192   0.9728    0.823296\n",
      "  0.244736  0.695296  0.900096  0.925696  0.236544  0.694272  0.933888\n",
      "  0.7936    0.237568  0.69632   0.90112   0.924672  0.236544  0.69632\n",
      "  0.940032  0.804864  0.239616  0.69632   0.907264  0.92672   0.237568\n",
      "  0.694272  0.919552  0.790528  0.232448  0.683008  0.887808  0.91136\n",
      "  0.234496  0.676864  0.914432  0.77312   0.229376  0.676864  0.877568\n",
      "  0.902144  0.233472  0.677888  0.914432  0.772096  0.231424  0.678912] ms\n",
      "mlp_backward_weight_device \t = \t 1.175552ms | \t[14.44864   0.873472  0.93696   0.239616  0.667648  0.88064   0.935936\n",
      "  0.223232  0.654336  0.83968   0.900096  0.229376  0.64      0.848896\n",
      "  0.902144  0.22016   0.653312  0.83968   0.900096  0.2304    0.646144\n",
      "  0.854016  0.904192  0.222208  0.658432  0.843776  0.904192  0.229376\n",
      "  0.64      0.833536  0.888832  0.217088  0.644096  0.826368  0.88576\n",
      "  0.226304  0.627712  0.827392  0.883712  0.21504   0.638976  0.818176\n",
      "  0.879616  0.224256  0.627712  0.827392  0.88064   0.21504   0.637952] ms\n",
      "layernorm_backward_device \t = \t 3.019776ms | \t[0.029696 0.03072  0.027648 0.03072  0.027648 0.029696 0.026624 0.029696\n",
      " 0.027648 0.028672 0.027648 0.028672 0.026624 0.029696 0.026624 0.028672\n",
      " 0.026624 0.028672 0.026624 0.028672 0.026624 0.028672 0.026624 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.059392ms | \t[0.014336 0.021504 0.01536  0.02048  0.014336 0.021504 0.014336 0.02048\n",
      " 0.014336 0.02048  0.014336 0.02048  0.014336 0.021504 0.013312 0.02048\n",
      " 0.014336 0.022528 0.013312 0.02048  0.014336 0.02048  0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.230848ms | \t[0.090112 0.086016 0.088064 0.088064 0.08704  0.08704  0.08704  0.084992\n",
      " 0.084992 0.084992 0.084992 0.086016] ms\n",
      "attention_backward_device \t = \t 0.054272ms | \t[1.567744 1.539072 1.522688 1.488896 1.52064  1.51552  1.529856 1.481728\n",
      " 1.508352 1.458176 1.502208 1.473536] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.16] ms\n",
      "total_time(11)=172.975104|8.101888\n",
      "layernorm_forward_device \t = \t 0.357376ms | \t[0.016384 0.016384 0.016384 0.01536  0.01536  0.01536  0.014336 0.017408\n",
      " 0.016384 0.01536  0.01536  0.014336 0.016384 0.016384 0.016384 0.01536\n",
      " 0.01536  0.014336 0.018432 0.01536  0.014336 0.01536  0.01536  0.01536\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.16896ms | \t[ 0.57344   0.193536  0.76288   0.648192  0.543744  0.191488  0.734208\n",
      "  0.755712  0.571392  0.191488  0.760832  0.756736  0.570368  0.190464\n",
      "  0.73216   0.648192  0.543744  0.192512  0.733184  0.755712  0.570368\n",
      "  0.192512  0.760832  0.754688  0.571392  0.192512  0.733184  0.754688\n",
      "  0.543744  0.191488  0.735232  0.754688  0.570368  0.191488  0.760832\n",
      "  0.754688  0.569344  0.16384   0.734208  0.753664  0.543744  0.191488\n",
      "  0.733184  0.754688  0.570368  0.191488  0.760832  0.755712 12.602368] ms\n",
      "attention_forward_device \t = \t 0.113664ms | \t[0.474112 0.472064 0.474112 0.474112 0.47104  0.47616  0.47616  0.472064\n",
      " 0.475136 0.47104  0.475136 0.475136] ms\n",
      "residual_forward_device \t = \t 0.246784ms | \t[0.01024  0.01024  0.01024  0.01024  0.01024  0.011264 0.01024  0.01024\n",
      " 0.009216 0.009216 0.01024  0.01024  0.01024  0.01024  0.009216 0.01024\n",
      " 0.01024  0.011264 0.01024  0.011264 0.01024  0.01024  0.01024  0.009216] ms\n",
      "gelu_forward_device \t = \t 0.193536ms | \t[0.0256   0.021504 0.022528 0.021504 0.022528 0.022528 0.022528 0.021504\n",
      " 0.021504 0.021504 0.023552 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.047104ms | \t[1.765376] ms\n",
      "cross_entropy_backward_device \t = \t 0.177152ms | \t[0.76288] ms\n",
      "mlp_backward_input_device \t = \t 0.642048ms | \t[13.715456  0.940032  0.964608  0.248832  0.72192   0.971776  0.963584\n",
      "  0.20992   0.695296  0.904192  0.925696  0.236544  0.694272  0.933888\n",
      "  0.930816  0.203776  0.695296  0.902144  0.924672  0.236544  0.694272\n",
      "  0.939008  0.930816  0.2048    0.695296  0.908288  0.930816  0.237568\n",
      "  0.695296  0.920576  0.908288  0.19968   0.683008  0.887808  0.91136\n",
      "  0.233472  0.678912  0.912384  0.902144  0.197632  0.676864  0.878592\n",
      "  0.91648   0.2304    0.67584   0.909312  0.902144  0.198656  0.677888] ms\n",
      "mlp_backward_weight_device \t = \t 0.602112ms | \t[14.435328  0.872448  0.93696   0.239616  0.67072   0.882688  0.934912\n",
      "  0.223232  0.64512   0.83968   0.900096  0.2304    0.644096  0.84992\n",
      "  0.902144  0.22016   0.643072  0.83968   0.900096  0.2304    0.644096\n",
      "  0.854016  0.904192  0.222208  0.648192  0.842752  0.904192  0.2304\n",
      "  0.64      0.835584  0.888832  0.217088  0.631808  0.826368  0.88576\n",
      "  0.226304  0.626688  0.827392  0.883712  0.21504   0.627712  0.817152\n",
      "  0.878592  0.224256  0.627712  0.828416  0.881664  0.21504   0.627712] ms\n",
      "layernorm_backward_device \t = \t 3.091456ms | \t[0.03072  0.029696 0.027648 0.03072  0.026624 0.029696 0.0256   0.029696\n",
      " 0.027648 0.028672 0.026624 0.028672 0.026624 0.028672 0.0256   0.028672\n",
      " 0.026624 0.028672 0.0256   0.027648 0.026624 0.028672 0.026624 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.068608ms | \t[0.014336 0.021504 0.014336 0.02048  0.014336 0.02048  0.014336 0.02048\n",
      " 0.013312 0.021504 0.014336 0.02048  0.014336 0.022528 0.013312 0.019456\n",
      " 0.013312 0.022528 0.014336 0.019456 0.01536  0.019456 0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.225728ms | \t[0.088064 0.086016 0.084992 0.086016 0.086016 0.084992 0.084992 0.086016\n",
      " 0.084992 0.083968 0.082944 0.086016] ms\n",
      "attention_backward_device \t = \t 0.055296ms | \t[1.586176 1.537024 1.548288 1.494016 1.548288 1.543168 1.550336 1.445888\n",
      " 1.53088  1.43872  1.526784 1.502208] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.045312] ms\n",
      "total_time(12)=173.44921599999998|7.539712000000001\n",
      "layernorm_forward_device \t = \t 0.354304ms | \t[0.016384 0.018432 0.016384 0.01536  0.01536  0.016384 0.014336 0.017408\n",
      " 0.01536  0.01536  0.01536  0.01536  0.01536  0.01536  0.01536  0.01536\n",
      " 0.01536  0.014336 0.016384 0.01536  0.014336 0.01536  0.01536  0.014336\n",
      " 0.019456] ms\n",
      "mlp_forward_device \t = \t 0.172032ms | \t[ 0.572416  0.193536  0.761856  0.755712  0.543744  0.191488  0.733184\n",
      "  0.754688  0.571392  0.192512  0.760832  0.756736  0.570368  0.191488\n",
      "  0.760832  0.648192  0.543744  0.192512  0.733184  0.755712  0.570368\n",
      "  0.191488  0.760832  0.754688  0.570368  0.191488  0.733184  0.647168\n",
      "  0.543744  0.191488  0.735232  0.754688  0.570368  0.191488  0.760832\n",
      "  0.754688  0.570368  0.16384   0.733184  0.754688  0.543744  0.191488\n",
      "  0.733184  0.754688  0.570368  0.192512  0.760832  0.754688 12.601344] ms\n",
      "attention_forward_device \t = \t 0.113664ms | \t[0.474112 0.47104  0.47616  0.473088 0.470016 0.474112 0.474112 0.47104\n",
      " 0.474112 0.47104  0.47616  0.474112] ms\n",
      "residual_forward_device \t = \t 0.248832ms | \t[0.01024  0.01024  0.01024  0.011264 0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.011264 0.011264 0.009216\n",
      " 0.01024  0.011264 0.009216 0.01024  0.01024  0.01024  0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.16896ms | \t[0.022528 0.022528 0.022528 0.022528 0.023552 0.021504 0.022528 0.022528\n",
      " 0.023552 0.021504 0.023552 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.047104ms | \t[1.755136] ms\n",
      "cross_entropy_backward_device \t = \t 0.185344ms | \t[0.758784] ms\n",
      "mlp_backward_input_device \t = \t 0.822272ms | \t[13.715456  0.937984  0.960512  0.247808  0.723968  0.9728    0.971776\n",
      "  0.20992   0.695296  0.905216  0.925696  0.236544  0.695296  0.933888\n",
      "  0.924672  0.205824  0.695296  0.90112   0.925696  0.236544  0.694272\n",
      "  0.937984  0.929792  0.203776  0.695296  0.907264  0.945152  0.236544\n",
      "  0.695296  0.919552  0.910336  0.19968   0.685056  0.887808  0.909312\n",
      "  0.23552   0.678912  0.912384  0.903168  0.197632  0.676864  0.877568\n",
      "  0.902144  0.2304    0.67584   0.909312  0.904192  0.198656  0.678912] ms\n",
      "mlp_backward_weight_device \t = \t 0.598016ms | \t[14.386176  0.869376  0.935936  0.239616  0.667648  0.88064   0.943104\n",
      "  0.223232  0.643072  0.837632  0.900096  0.2304    0.64512   0.846848\n",
      "  0.909312  0.22016   0.641024  0.836608  0.900096  0.2304    0.646144\n",
      "  0.851968  0.912384  0.222208  0.64512   0.841728  0.903168  0.229376\n",
      "  0.64      0.833536  0.896     0.218112  0.630784  0.82432   0.88576\n",
      "  0.22528   0.626688  0.825344  0.889856  0.21504   0.626688  0.816128\n",
      "  0.879616  0.224256  0.626688  0.825344  0.886784  0.21504   0.625664] ms\n",
      "layernorm_backward_device \t = \t 3.11296ms | \t[0.029696 0.029696 0.027648 0.03072  0.026624 0.03072  0.026624 0.028672\n",
      " 0.026624 0.029696 0.026624 0.029696 0.027648 0.028672 0.026624 0.027648\n",
      " 0.026624 0.028672 0.0256   0.028672 0.026624 0.027648 0.0256   0.029696\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.075776ms | \t[0.013312 0.021504 0.013312 0.019456 0.013312 0.019456 0.014336 0.019456\n",
      " 0.013312 0.02048  0.014336 0.02048  0.014336 0.02048  0.014336 0.02048\n",
      " 0.013312 0.019456 0.014336 0.018432 0.012288 0.02048  0.013312 0.019456] ms\n",
      "gelu_backward_device \t = \t 1.25952ms | \t[0.08704  0.08704  0.086016 0.086016 0.084992 0.084992 0.086016 0.084992\n",
      " 0.083968 0.083968 0.083968 0.083968] ms\n",
      "attention_backward_device \t = \t 0.054272ms | \t[1.593344 1.473536 1.554432 1.46432  1.557504 1.4592   1.558528 1.440768\n",
      " 1.49504  1.486848 1.478656 1.522688] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[11.977728] ms\n",
      "total_time(13)=173.16044800000003|7.763968000000001\n",
      "layernorm_forward_device \t = \t 0.349184ms | \t[0.01536  0.016384 0.016384 0.014336 0.016384 0.01536  0.01536  0.017408\n",
      " 0.01536  0.014336 0.016384 0.01536  0.01536  0.017408 0.016384 0.014336\n",
      " 0.016384 0.014336 0.016384 0.01536  0.01536  0.016384 0.01536  0.014336\n",
      " 0.019456] ms\n",
      "mlp_forward_device \t = \t 0.145408ms | \t[ 0.572416  0.193536  0.761856  0.755712  0.543744  0.164864  0.734208\n",
      "  0.755712  0.543744  0.192512  0.760832  0.756736  0.570368  0.191488\n",
      "  0.760832  0.648192  0.543744  0.192512  0.734208  0.755712  0.570368\n",
      "  0.190464  0.760832  0.755712  0.570368  0.192512  0.73216   0.647168\n",
      "  0.543744  0.192512  0.735232  0.755712  0.570368  0.192512  0.760832\n",
      "  0.755712  0.570368  0.191488  0.733184  0.754688  0.543744  0.191488\n",
      "  0.733184  0.754688  0.569344  0.192512  0.760832  0.755712 12.649472] ms\n",
      "attention_forward_device \t = \t 0.13824ms | \t[0.47616  0.474112 0.475136 0.475136 0.47104  0.478208 0.477184 0.472064\n",
      " 0.475136 0.47616  0.477184 0.47616 ] ms\n",
      "residual_forward_device \t = \t 0.246784ms | \t[0.011264 0.01024  0.01024  0.01024  0.011264 0.01024  0.011264 0.01024\n",
      " 0.009216 0.011264 0.011264 0.01024  0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.011264 0.011264 0.01024  0.011264 0.011264 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.165888ms | \t[0.022528 0.022528 0.022528 0.022528 0.022528 0.023552 0.022528 0.022528\n",
      " 0.021504 0.021504 0.022528 0.021504] ms\n",
      "softmax_forward_device \t = \t 0.001024ms | \t[1.782784] ms\n",
      "cross_entropy_backward_device \t = \t 0.15872ms | \t[0.755712] ms\n",
      "mlp_backward_input_device \t = \t 0.913408ms | \t[13.71648   0.939008  0.963584  0.247808  0.72192   0.9728    0.964608\n",
      "  0.20992   0.697344  0.904192  0.939008  0.236544  0.69632   0.933888\n",
      "  0.924672  0.203776  0.695296  0.902144  0.943104  0.239616  0.694272\n",
      "  0.937984  0.928768  0.205824  0.69632   0.908288  0.927744  0.236544\n",
      "  0.697344  0.919552  0.909312  0.19968   0.683008  0.886784  0.910336\n",
      "  0.232448  0.678912  0.912384  0.91648   0.198656  0.677888  0.879616\n",
      "  0.903168  0.232448  0.67584   0.910336  0.910336  0.198656  0.678912] ms\n",
      "mlp_backward_weight_device \t = \t 0.562176ms | \t[14.416896  0.87552   0.93696   0.239616  0.663552  0.88576   0.944128\n",
      "  0.223232  0.64      0.841728  0.900096  0.2304    0.638976  0.850944\n",
      "  0.910336  0.22016   0.638976  0.840704  0.899072  0.2304    0.641024\n",
      "  0.856064  0.913408  0.222208  0.641024  0.845824  0.904192  0.229376\n",
      "  0.632832  0.837632  0.897024  0.217088  0.628736  0.826368  0.88576\n",
      "  0.226304  0.622592  0.830464  0.888832  0.21504   0.622592  0.820224\n",
      "  0.879616  0.22528   0.621568  0.830464  0.886784  0.21504   0.622592] ms\n",
      "layernorm_backward_device \t = \t 3.122176ms | \t[0.029696 0.029696 0.028672 0.033792 0.026624 0.029696 0.026624 0.032768\n",
      " 0.027648 0.028672 0.026624 0.032768 0.026624 0.028672 0.026624 0.031744\n",
      " 0.026624 0.028672 0.0256   0.03072  0.0256   0.028672 0.026624 0.032768\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.049152ms | \t[0.014336 0.02048  0.014336 0.02048  0.014336 0.02048  0.014336 0.019456\n",
      " 0.014336 0.02048  0.014336 0.019456 0.013312 0.02048  0.013312 0.019456\n",
      " 0.016384 0.02048  0.013312 0.018432 0.013312 0.018432 0.013312 0.019456] ms\n",
      "gelu_backward_device \t = \t 1.2032ms | \t[0.08704  0.088064 0.086016 0.086016 0.084992 0.086016 0.084992 0.083968\n",
      " 0.084992 0.083968 0.083968 0.08704 ] ms\n",
      "attention_backward_device \t = \t 0.055296ms | \t[1.58208  1.48992  1.518592 1.482752 1.55136  1.492992 1.550336 1.458176\n",
      " 1.476608 1.453056 1.466368 1.458176] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.055552] ms\n",
      "total_time(14)=173.321216|7.661568000000001\n",
      "layernorm_forward_device \t = \t 0.34816ms | \t[0.01536  0.016384 0.017408 0.01536  0.01536  0.01536  0.014336 0.017408\n",
      " 0.016384 0.01536  0.016384 0.01536  0.014336 0.016384 0.01536  0.01536\n",
      " 0.014336 0.013312 0.013312 0.01536  0.01536  0.01536  0.016384 0.014336\n",
      " 0.018432] ms\n",
      "mlp_forward_device \t = \t 0.150528ms | \t[ 0.572416  0.193536  0.761856  0.755712  0.543744  0.164864  0.734208\n",
      "  0.755712  0.543744  0.192512  0.735232  0.755712  0.571392  0.191488\n",
      "  0.760832  0.755712  0.543744  0.192512  0.733184  0.755712  0.570368\n",
      "  0.192512  0.761856  0.755712  0.570368  0.192512  0.761856  0.647168\n",
      "  0.543744  0.192512  0.734208  0.754688  0.571392  0.191488  0.760832\n",
      "  0.755712  0.570368  0.191488  0.733184  0.647168  0.543744  0.191488\n",
      "  0.733184  0.755712  0.569344  0.192512  0.760832  0.755712 12.649472] ms\n",
      "attention_forward_device \t = \t 0.140288ms | \t[0.477184 0.473088 0.477184 0.473088 0.473088 0.477184 0.47616  0.472064\n",
      " 0.47616  0.474112 0.472064 0.47616 ] ms\n",
      "residual_forward_device \t = \t 0.249856ms | \t[0.011264 0.011264 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.011264 0.01024  0.01024  0.011264 0.01024  0.009216 0.01024\n",
      " 0.01024  0.011264 0.01024  0.01024  0.01024  0.009216 0.011264 0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.171008ms | \t[0.021504 0.022528 0.022528 0.022528 0.022528 0.021504 0.024576 0.022528\n",
      " 0.022528 0.021504 0.023552 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.001024ms | \t[1.901568] ms\n",
      "cross_entropy_backward_device \t = \t 0.04096ms | \t[0.753664] ms\n",
      "mlp_backward_input_device \t = \t 0.794624ms | \t[11.715584  0.940032  0.973824  0.248832  0.72192   0.971776  0.96256\n",
      "  0.20992   0.595968  0.903168  0.925696  0.236544  0.694272  0.935936\n",
      "  0.924672  0.203776  0.595968  0.90112   0.924672  0.236544  0.695296\n",
      "  0.937984  0.939008  0.205824  0.598016  0.907264  0.92672   0.236544\n",
      "  0.694272  0.919552  0.922624  0.19968   0.585728  0.887808  0.909312\n",
      "  0.232448  0.676864  0.913408  0.903168  0.196608  0.57856   0.876544\n",
      "  0.903168  0.231424  0.677888  0.910336  0.902144  0.198656  0.580608] ms\n",
      "mlp_backward_weight_device \t = \t 3.186688ms | \t[14.42304   0.872448  0.93696   0.239616  0.658432  0.88064   0.943104\n",
      "  0.223232  0.623616  0.836608  0.900096  0.2304    0.633856  0.846848\n",
      "  0.910336  0.22016   0.62464   0.836608  0.900096  0.2304    0.635904\n",
      "  0.851968  0.912384  0.222208  0.626688  0.840704  0.904192  0.2304\n",
      "  0.630784  0.832512  0.897024  0.217088  0.617472  0.823296  0.88576\n",
      "  0.226304  0.61952   0.826368  0.888832  0.21504   0.608256  0.816128\n",
      "  0.879616  0.22528   0.618496  0.826368  0.886784  0.21504   0.608256] ms\n",
      "layernorm_backward_device \t = \t 3.227648ms | \t[0.029696 0.03072  0.026624 0.033792 0.026624 0.029696 0.0256   0.032768\n",
      " 0.026624 0.028672 0.027648 0.032768 0.027648 0.028672 0.0256   0.031744\n",
      " 0.026624 0.028672 0.026624 0.03072  0.026624 0.028672 0.0256   0.032768\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.05632ms | \t[0.012288 0.021504 0.014336 0.019456 0.014336 0.02048  0.014336 0.019456\n",
      " 0.014336 0.02048  0.014336 0.02048  0.014336 0.019456 0.014336 0.019456\n",
      " 0.014336 0.019456 0.013312 0.02048  0.013312 0.02048  0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.253376ms | \t[0.088064 0.08704  0.086016 0.086016 0.086016 0.08704  0.086016 0.084992\n",
      " 0.086016 0.084992 0.083968 0.084992] ms\n",
      "attention_backward_device \t = \t 0.054272ms | \t[1.589248 1.522688 1.547264 1.52064  1.543168 1.497088 1.55136  1.439744\n",
      " 1.488896 1.442816 1.4848   1.473536] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.140544] ms\n",
      "total_time(15)=170.85644800000003|10.225663999999998\n",
      "layernorm_forward_device \t = \t 0.33792ms | \t[0.016384 0.016384 0.018432 0.016384 0.01536  0.016384 0.01536  0.01536\n",
      " 0.01536  0.01536  0.01536  0.016384 0.01536  0.016384 0.016384 0.01536\n",
      " 0.01536  0.01536  0.014336 0.018432 0.014336 0.01536  0.014336 0.014336\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.095232ms | \t[ 0.574464  0.192512  0.761856  0.756736  0.570368  0.164864  0.734208\n",
      "  0.755712  0.543744  0.192512  0.734208  0.755712  0.571392  0.191488\n",
      "  0.760832  0.755712  0.543744  0.164864  0.734208  0.755712  0.543744\n",
      "  0.192512  0.761856  0.754688  0.570368  0.192512  0.760832  0.647168\n",
      "  0.543744  0.192512  0.735232  0.754688  0.571392  0.190464  0.760832\n",
      "  0.754688  0.570368  0.191488  0.73216   0.647168  0.544768  0.191488\n",
      "  0.734208  0.755712  0.569344  0.192512  0.760832  0.755712 12.649472] ms\n",
      "attention_forward_device \t = \t 0.137216ms | \t[0.483328 0.475136 0.480256 0.478208 0.478208 0.48128  0.479232 0.475136\n",
      " 0.482304 0.479232 0.47616  0.479232] ms\n",
      "residual_forward_device \t = \t 0.27648ms | \t[0.01024  0.011264 0.01024  0.01024  0.01024  0.011264 0.01024  0.011264\n",
      " 0.01024  0.01024  0.01024  0.011264 0.011264 0.011264 0.009216 0.01024\n",
      " 0.011264 0.011264 0.011264 0.011264 0.011264 0.01024  0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.164864ms | \t[0.022528 0.022528 0.023552 0.021504 0.022528 0.021504 0.023552 0.022528\n",
      " 0.022528 0.022528 0.022528 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.001024ms | \t[1.939456] ms\n",
      "cross_entropy_forward_device \t = \t 0.0ms | \t[0.001024] ms\n",
      "cross_entropy_backward_device \t = \t 0.0ms | \t[0.771072] ms\n",
      "mlp_backward_input_device \t = \t 0.4864ms | \t[11.715584  0.941056  0.96256   0.247808  0.723968  0.969728  0.963584\n",
      "  0.244736  0.595968  0.905216  0.924672  0.237568  0.693248  0.934912\n",
      "  0.93184   0.237568  0.595968  0.902144  0.923648  0.236544  0.695296\n",
      "  0.940032  0.928768  0.236544  0.596992  0.909312  0.929792  0.237568\n",
      "  0.694272  0.9216    0.908288  0.231424  0.586752  0.888832  0.908288\n",
      "  0.232448  0.676864  0.913408  0.899072  0.229376  0.579584  0.876544\n",
      "  0.915456  0.2304    0.676864  0.91136   0.90112   0.2304    0.580608] ms\n",
      "mlp_backward_weight_device \t = \t 2.985984ms | \t[14.431232  0.876544  0.93696   0.239616  0.661504  0.88576   0.946176\n",
      "  0.223232  0.627712  0.840704  0.90112   0.229376  0.637952  0.850944\n",
      "  0.91136   0.221184  0.627712  0.840704  0.900096  0.2304    0.641024\n",
      "  0.856064  0.914432  0.222208  0.630784  0.8448    0.904192  0.2304\n",
      "  0.63488   0.836608  0.897024  0.217088  0.617472  0.827392  0.88576\n",
      "  0.226304  0.621568  0.82944   0.889856  0.21504   0.611328  0.820224\n",
      "  0.878592  0.22528   0.622592  0.82944   0.888832  0.21504   0.613376] ms\n",
      "layernorm_backward_device \t = \t 3.1744ms | \t[0.029696 0.029696 0.027648 0.032768 0.027648 0.029696 0.0256   0.031744\n",
      " 0.027648 0.028672 0.027648 0.032768 0.026624 0.028672 0.026624 0.031744\n",
      " 0.027648 0.028672 0.026624 0.031744 0.026624 0.028672 0.026624 0.032768\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.049152ms | \t[0.014336 0.021504 0.014336 0.021504 0.014336 0.02048  0.014336 0.021504\n",
      " 0.014336 0.021504 0.014336 0.022528 0.013312 0.021504 0.014336 0.02048\n",
      " 0.014336 0.02048  0.013312 0.02048  0.014336 0.02048  0.01536  0.021504] ms\n",
      "gelu_backward_device \t = \t 1.207296ms | \t[0.089088 0.089088 0.084992 0.088064 0.08704  0.088064 0.08704  0.08704\n",
      " 0.08704  0.084992 0.086016 0.08704 ] ms\n",
      "attention_backward_device \t = \t 0.054272ms | \t[1.60768  1.538048 1.563648 1.547264 1.5616   1.504256 1.563648 1.460224\n",
      " 1.540096 1.47968  1.54112  1.456128] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.155904] ms\n",
      "total_time(16)=171.55174399999999|9.520128\n",
      "layernorm_forward_device \t = \t 0.344064ms | \t[0.016384 0.016384 0.016384 0.01536  0.01536  0.014336 0.01536  0.014336\n",
      " 0.017408 0.016384 0.01536  0.01536  0.01536  0.016384 0.016384 0.016384\n",
      " 0.016384 0.014336 0.014336 0.016384 0.01536  0.014336 0.01536  0.01536\n",
      " 0.01536 ] ms\n",
      "mlp_forward_device \t = \t 0.157696ms | \t[ 0.57344   0.193536  0.761856  0.756736  0.571392  0.164864  0.734208\n",
      "  0.755712  0.543744  0.191488  0.734208  0.755712  0.571392  0.192512\n",
      "  0.760832  0.755712  0.54272   0.164864  0.734208  0.754688  0.54272\n",
      "  0.192512  0.735232  0.755712  0.569344  0.192512  0.760832  0.754688\n",
      "  0.544768  0.192512  0.735232  0.754688  0.571392  0.192512  0.761856\n",
      "  0.755712  0.570368  0.191488  0.761856  0.647168  0.543744  0.191488\n",
      "  0.734208  0.755712  0.570368  0.191488  0.760832  0.755712 12.63104 ] ms\n",
      "attention_forward_device \t = \t 0.13824ms | \t[0.475136 0.473088 0.47616  0.472064 0.473088 0.475136 0.473088 0.472064\n",
      " 0.47616  0.474112 0.472064 0.47616 ] ms\n",
      "residual_forward_device \t = \t 0.167936ms | \t[0.011264 0.009216 0.01024  0.011264 0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.009216 0.01024  0.01024  0.01024  0.011264 0.009216 0.011264\n",
      " 0.011264 0.011264 0.011264 0.01024  0.01024  0.009216 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.161792ms | \t[0.024576 0.021504 0.022528 0.021504 0.022528 0.021504 0.022528 0.023552\n",
      " 0.022528 0.022528 0.023552 0.021504] ms\n",
      "softmax_forward_device \t = \t 0.018432ms | \t[1.898496] ms\n",
      "cross_entropy_backward_device \t = \t 0.045056ms | \t[0.756736] ms\n",
      "mlp_backward_input_device \t = \t 1.078272ms | \t[11.715584  0.941056  0.960512  0.247808  0.723968  0.969728  0.973824\n",
      "  0.243712  0.595968  0.905216  0.925696  0.236544  0.694272  0.935936\n",
      "  0.924672  0.24064   0.598016  0.902144  0.92672   0.236544  0.695296\n",
      "  0.939008  0.928768  0.237568  0.59904   0.908288  0.945152  0.236544\n",
      "  0.694272  0.920576  0.91136   0.232448  0.584704  0.888832  0.91648\n",
      "  0.23552   0.67584   0.914432  0.903168  0.229376  0.579584  0.876544\n",
      "  0.90112   0.2304    0.677888  0.91136   0.905216  0.231424  0.581632] ms\n",
      "mlp_backward_weight_device \t = \t 2.968576ms | \t[14.424064  0.877568  0.93696   0.239616  0.663552  0.881664  0.945152\n",
      "  0.232448  0.62464   0.83968   0.90112   0.2304    0.636928  0.847872\n",
      "  0.910336  0.229376  0.62464   0.838656  0.900096  0.2304    0.638976\n",
      "  0.852992  0.913408  0.2304    0.628736  0.843776  0.904192  0.2304\n",
      "  0.632832  0.833536  0.897024  0.227328  0.613376  0.825344  0.884736\n",
      "  0.22528   0.621568  0.825344  0.89088   0.224256  0.608256  0.8192\n",
      "  0.879616  0.22528   0.623616  0.826368  0.887808  0.224256  0.607232] ms\n",
      "layernorm_backward_device \t = \t 3.21024ms | \t[0.029696 0.029696 0.027648 0.032768 0.026624 0.029696 0.0256   0.032768\n",
      " 0.026624 0.028672 0.027648 0.032768 0.026624 0.029696 0.026624 0.03072\n",
      " 0.026624 0.028672 0.0256   0.031744 0.026624 0.028672 0.026624 0.031744\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.055296ms | \t[0.014336 0.02048  0.013312 0.02048  0.013312 0.019456 0.013312 0.021504\n",
      " 0.014336 0.019456 0.014336 0.02048  0.014336 0.02048  0.013312 0.019456\n",
      " 0.014336 0.021504 0.013312 0.019456 0.014336 0.019456 0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.23392ms | \t[0.086016 0.08704  0.084992 0.084992 0.084992 0.084992 0.086016 0.086016\n",
      " 0.083968 0.083968 0.082944 0.086016] ms\n",
      "attention_backward_device \t = \t 0.003072ms | \t[1.57184  1.49504  1.472512 1.492992 1.482752 1.492992 1.481728 1.463296\n",
      " 1.47968  1.462272 1.478656 1.435648] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.061696] ms\n",
      "total_time(17)=170.855424|10.133504\n",
      "layernorm_forward_device \t = \t 0.351232ms | \t[0.016384 0.016384 0.017408 0.016384 0.014336 0.01536  0.016384 0.014336\n",
      " 0.016384 0.016384 0.01536  0.01536  0.016384 0.014336 0.016384 0.01536\n",
      " 0.01536  0.01536  0.014336 0.017408 0.01536  0.01536  0.01536  0.01536\n",
      " 0.016384] ms\n",
      "mlp_forward_device \t = \t 0.164864ms | \t[ 0.574464  0.193536  0.761856  0.756736  0.571392  0.192512  0.734208\n",
      "  0.755712  0.543744  0.191488  0.733184  0.755712  0.570368  0.191488\n",
      "  0.760832  0.755712  0.571392  0.164864  0.734208  0.755712  0.54272\n",
      "  0.191488  0.734208  0.755712  0.570368  0.192512  0.759808  0.754688\n",
      "  0.543744  0.164864  0.735232  0.754688  0.543744  0.191488  0.761856\n",
      "  0.755712  0.570368  0.191488  0.761856  0.646144  0.543744  0.191488\n",
      "  0.734208  0.754688  0.570368  0.191488  0.760832  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.13824ms | \t[0.475136 0.474112 0.47616  0.475136 0.47104  0.47616  0.47104  0.47104\n",
      " 0.47616  0.475136 0.472064 0.47616 ] ms\n",
      "residual_forward_device \t = \t 0.165888ms | \t[0.011264 0.011264 0.011264 0.009216 0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.011264 0.011264 0.009216 0.01024\n",
      " 0.01024  0.01024  0.011264 0.011264 0.01024  0.01024  0.01024  0.011264] ms\n",
      "gelu_forward_device \t = \t 0.164864ms | \t[0.023552 0.022528 0.023552 0.022528 0.021504 0.022528 0.022528 0.021504\n",
      " 0.021504 0.023552 0.022528 0.023552] ms\n",
      "softmax_forward_device \t = \t 0.018432ms | \t[1.897472] ms\n",
      "cross_entropy_backward_device \t = \t 0.045056ms | \t[0.82432] ms\n",
      "mlp_backward_input_device \t = \t 0.605184ms | \t[11.715584  0.943104  0.96256   0.247808  0.723968  0.970752  0.963584\n",
      "  0.243712  0.595968  0.90624   0.939008  0.236544  0.69632   0.935936\n",
      "  0.92672   0.237568  0.595968  0.902144  0.941056  0.236544  0.695296\n",
      "  0.937984  0.928768  0.239616  0.598016  0.909312  0.929792  0.237568\n",
      "  0.694272  0.9216    0.910336  0.232448  0.584704  0.888832  0.909312\n",
      "  0.232448  0.676864  0.913408  0.915456  0.2304    0.579584  0.876544\n",
      "  0.903168  0.231424  0.67584   0.91136   0.910336  0.2304    0.581632] ms\n",
      "mlp_backward_weight_device \t = \t 2.952192ms | \t[14.406656  0.873472  0.93696   0.239616  0.662528  0.873472  0.944128\n",
      "  0.232448  0.62976   0.838656  0.900096  0.2304    0.636928  0.840704\n",
      "  0.909312  0.229376  0.631808  0.837632  0.900096  0.2304    0.638976\n",
      "  0.8448    0.912384  0.2304    0.635904  0.841728  0.904192  0.2304\n",
      "  0.631808  0.825344  0.897024  0.226304  0.618496  0.825344  0.88576\n",
      "  0.226304  0.620544  0.820224  0.89088   0.223232  0.617472  0.818176\n",
      "  0.879616  0.22528   0.621568  0.8192    0.89088   0.224256  0.61952 ] ms\n",
      "layernorm_backward_device \t = \t 3.187712ms | \t[0.029696 0.029696 0.026624 0.03072  0.026624 0.029696 0.026624 0.029696\n",
      " 0.027648 0.028672 0.026624 0.028672 0.026624 0.029696 0.0256   0.028672\n",
      " 0.026624 0.028672 0.0256   0.027648 0.026624 0.028672 0.026624 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.067584ms | \t[0.014336 0.02048  0.013312 0.022528 0.014336 0.02048  0.014336 0.021504\n",
      " 0.013312 0.02048  0.014336 0.021504 0.013312 0.019456 0.014336 0.02048\n",
      " 0.014336 0.019456 0.01536  0.019456 0.012288 0.018432 0.014336 0.022528] ms\n",
      "gelu_backward_device \t = \t 1.288192ms | \t[0.08704  0.08704  0.083968 0.086016 0.083968 0.086016 0.084992 0.083968\n",
      " 0.083968 0.084992 0.083968 0.086016] ms\n",
      "attention_backward_device \t = \t 0.003072ms | \t[1.591296 1.519616 1.547264 1.511424 1.536    1.497088 1.555456 1.485824\n",
      " 1.534976 1.4848   1.485824 1.478656] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.156928] ms\n",
      "total_time(18)=171.40940799999998|9.702399999999999\n",
      "layernorm_forward_device \t = \t 0.346112ms | \t[0.016384 0.016384 0.01536  0.01536  0.01536  0.01536  0.01536  0.014336\n",
      " 0.017408 0.016384 0.016384 0.01536  0.016384 0.014336 0.018432 0.017408\n",
      " 0.016384 0.01536  0.01536  0.017408 0.016384 0.01536  0.01536  0.01536\n",
      " 0.01536 ] ms\n",
      "mlp_forward_device \t = \t 0.082944ms | \t[ 0.574464  0.193536  0.760832  0.756736  0.570368  0.192512  0.734208\n",
      "  0.647168  0.543744  0.192512  0.733184  0.755712  0.570368  0.191488\n",
      "  0.759808  0.755712  0.570368  0.164864  0.733184  0.754688  0.543744\n",
      "  0.191488  0.734208  0.755712  0.569344  0.191488  0.759808  0.754688\n",
      "  0.54272   0.164864  0.735232  0.755712  0.543744  0.191488  0.735232\n",
      "  0.754688  0.571392  0.191488  0.761856  0.754688  0.543744  0.191488\n",
      "  0.733184  0.754688  0.570368  0.191488  0.760832  0.755712 12.63104 ] ms\n",
      "attention_forward_device \t = \t 0.137216ms | \t[0.483328 0.480256 0.479232 0.48128  0.480256 0.482304 0.482304 0.478208\n",
      " 0.480256 0.48128  0.478208 0.48128 ] ms\n",
      "residual_forward_device \t = \t 0.167936ms | \t[0.01024  0.011264 0.01024  0.01024  0.011264 0.01024  0.011264 0.01024\n",
      " 0.01024  0.01024  0.01024  0.011264 0.01024  0.01024  0.01024  0.011264\n",
      " 0.01024  0.01024  0.011264 0.011264 0.01024  0.011264 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.191488ms | \t[0.024576 0.022528 0.023552 0.022528 0.023552 0.022528 0.023552 0.021504\n",
      " 0.021504 0.022528 0.023552 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.018432ms | \t[1.810432] ms\n",
      "cross_entropy_backward_device \t = \t 0.131072ms | \t[0.838656] ms\n",
      "mlp_backward_input_device \t = \t 0.221184ms | \t[13.676544  0.940032  0.973824  0.247808  0.722944  0.971776  0.960512\n",
      "  0.243712  0.69632   0.904192  0.925696  0.237568  0.695296  0.934912\n",
      "  0.923648  0.236544  0.694272  0.90112   0.924672  0.236544  0.694272\n",
      "  0.944128  0.941056  0.239616  0.697344  0.90624   0.92672   0.236544\n",
      "  0.693248  0.922624  0.923648  0.232448  0.683008  0.887808  0.909312\n",
      "  0.232448  0.67584   0.913408  0.903168  0.229376  0.67584   0.876544\n",
      "  0.902144  0.231424  0.674816  0.914432  0.90112   0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 0.427008ms | \t[14.491648  0.87552   0.93696   0.228352  0.661504  0.873472  0.944128\n",
      "  0.232448  0.622592  0.83968   0.90112   0.218112  0.636928  0.840704\n",
      "  0.910336  0.2304    0.623616  0.840704  0.90112   0.219136  0.641024\n",
      "  0.843776  0.914432  0.2304    0.626688  0.8448    0.904192  0.219136\n",
      "  0.631808  0.825344  0.897024  0.227328  0.611328  0.827392  0.88576\n",
      "  0.216064  0.622592  0.8192    0.89088   0.223232  0.608256  0.8192\n",
      "  0.879616  0.21504   0.623616  0.818176  0.89088   0.224256  0.607232] ms\n",
      "layernorm_backward_device \t = \t 3.139584ms | \t[0.029696 0.03072  0.028672 0.03072  0.027648 0.029696 0.026624 0.028672\n",
      " 0.027648 0.028672 0.027648 0.028672 0.027648 0.028672 0.026624 0.029696\n",
      " 0.026624 0.028672 0.0256   0.028672 0.026624 0.028672 0.026624 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.06144ms | \t[0.014336 0.02048  0.01536  0.021504 0.014336 0.02048  0.01536  0.022528\n",
      " 0.014336 0.02048  0.014336 0.02048  0.01536  0.02048  0.014336 0.02048\n",
      " 0.014336 0.02048  0.016384 0.021504 0.014336 0.02048  0.013312 0.021504] ms\n",
      "gelu_backward_device \t = \t 1.27488ms | \t[0.089088 0.088064 0.086016 0.08704  0.086016 0.088064 0.086016 0.088064\n",
      " 0.084992 0.086016 0.083968 0.084992] ms\n",
      "attention_backward_device \t = \t 0.062464ms | \t[1.611776 1.579008 1.5616   1.529856 1.56672  1.553408 1.57184  1.502208\n",
      " 1.548288 1.497088 1.540096 1.509376] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.23168] ms\n",
      "total_time(19)=174.350336|6.811648000000001\n",
      "layernorm_forward_device \t = \t 0.35328ms | \t[0.01536  0.017408 0.014336 0.016384 0.01536  0.01536  0.016384 0.014336\n",
      " 0.016384 0.01536  0.01536  0.01536  0.016384 0.01536  0.016384 0.016384\n",
      " 0.01536  0.01536  0.01536  0.014336 0.01536  0.016384 0.014336 0.016384\n",
      " 0.01536 ] ms\n",
      "mlp_forward_device \t = \t 0.21504ms | \t[ 0.574464  0.193536  0.760832  0.756736  0.570368  0.192512  0.734208\n",
      "  0.647168  0.543744  0.192512  0.733184  0.755712  0.570368  0.192512\n",
      "  0.760832  0.755712  0.570368  0.192512  0.733184  0.755712  0.54272\n",
      "  0.191488  0.734208  0.755712  0.569344  0.192512  0.759808  0.754688\n",
      "  0.570368  0.164864  0.735232  0.755712  0.543744  0.191488  0.734208\n",
      "  0.754688  0.571392  0.190464  0.760832  0.754688  0.543744  0.16384\n",
      "  0.733184  0.754688  0.543744  0.191488  0.760832  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.139264ms | \t[0.473088 0.470016 0.467968 0.47104  0.472064 0.47104  0.47104  0.468992\n",
      " 0.472064 0.47104  0.468992 0.47104 ] ms\n",
      "residual_forward_device \t = \t 0.16384ms | \t[0.01024  0.009216 0.011264 0.01024  0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.011264 0.01024  0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.012288 0.01024  0.01024  0.01024  0.011264] ms\n",
      "gelu_forward_device \t = \t 0.19456ms | \t[0.022528 0.022528 0.022528 0.021504 0.023552 0.022528 0.022528 0.022528\n",
      " 0.022528 0.021504 0.022528 0.021504] ms\n",
      "softmax_forward_device \t = \t 0.019456ms | \t[1.750016] ms\n",
      "cross_entropy_backward_device \t = \t 0.191488ms | \t[0.823296] ms\n",
      "mlp_backward_input_device \t = \t 1.10592ms | \t[13.679616  0.93696   0.965632  0.247808  0.723968  0.9728    0.963584\n",
      "  0.24576   0.697344  0.905216  0.924672  0.236544  0.694272  0.935936\n",
      "  0.934912  0.237568  0.694272  0.90112   0.924672  0.236544  0.694272\n",
      "  0.937984  0.930816  0.237568  0.69632   0.907264  0.930816  0.24064\n",
      "  0.694272  0.922624  0.907264  0.232448  0.684032  0.888832  0.909312\n",
      "  0.232448  0.676864  0.914432  0.900096  0.229376  0.67584   0.878592\n",
      "  0.91648   0.2304    0.67584   0.914432  0.902144  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 0.436224ms | \t[14.518272  0.872448  0.93696   0.229376  0.666624  0.871424  0.944128\n",
      "  0.231424  0.892928  0.836608  0.90112   0.219136  0.64      0.837632\n",
      "  0.909312  0.229376  0.888832  0.836608  0.900096  0.219136  0.644096\n",
      "  0.843776  0.912384  0.2304    0.87552   0.841728  0.904192  0.219136\n",
      "  0.636928  0.825344  0.897024  0.226304  0.867328  0.823296  0.88576\n",
      "  0.216064  0.625664  0.818176  0.89088   0.224256  0.856064  0.815104\n",
      "  0.878592  0.21504   0.625664  0.815104  0.887808  0.224256  0.867328] ms\n",
      "layernorm_backward_device \t = \t 1.554432ms | \t[0.029696 0.029696 0.026624 0.03072  0.0256   0.029696 0.0256   0.028672\n",
      " 0.0256   0.027648 0.0256   0.028672 0.026624 0.027648 0.0256   0.027648\n",
      " 0.0256   0.027648 0.026624 0.028672 0.0256   0.027648 0.0256   0.028672\n",
      " 0.0256  ] ms\n",
      "residual_backward_device \t = \t 0.077824ms | \t[0.013312 0.02048  0.012288 0.02048  0.014336 0.019456 0.014336 0.02048\n",
      " 0.014336 0.02048  0.013312 0.021504 0.013312 0.02048  0.014336 0.021504\n",
      " 0.014336 0.019456 0.013312 0.019456 0.014336 0.019456 0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.306624ms | \t[0.086016 0.086016 0.08192  0.083968 0.083968 0.084992 0.083968 0.083968\n",
      " 0.082944 0.082944 0.082944 0.082944] ms\n",
      "attention_backward_device \t = \t 0.063488ms | \t[1.502208 1.51552  1.463296 1.465344 1.457152 1.475584 1.463296 1.473536\n",
      " 1.511424 1.478656 1.491968 1.458176] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.06784] ms\n",
      "total_time(20)=174.64832|6.372352000000001\n",
      "layernorm_forward_device \t = \t 0.361472ms | \t[0.01536  0.016384 0.014336 0.016384 0.014336 0.01536  0.01536  0.016384\n",
      " 0.01536  0.01536  0.01536  0.01536  0.01536  0.01536  0.017408 0.01536\n",
      " 0.016384 0.01536  0.01536  0.01536  0.017408 0.01536  0.01536  0.01536\n",
      " 0.016384] ms\n",
      "mlp_forward_device \t = \t 0.203776ms | \t[ 0.574464  0.193536  0.761856  0.756736  0.570368  0.191488  0.761856\n",
      "  0.647168  0.543744  0.192512  0.734208  0.755712  0.571392  0.191488\n",
      "  0.759808  0.755712  0.570368  0.191488  0.733184  0.647168  0.54272\n",
      "  0.191488  0.734208  0.755712  0.569344  0.191488  0.759808  0.754688\n",
      "  0.570368  0.164864  0.734208  0.755712  0.543744  0.191488  0.734208\n",
      "  0.754688  0.570368  0.190464  0.760832  0.754688  0.543744  0.16384\n",
      "  0.733184  0.755712  0.54272   0.191488  0.734208  0.754688 12.618752] ms\n",
      "attention_forward_device \t = \t 0.141312ms | \t[0.473088 0.470016 0.468992 0.47104  0.47104  0.467968 0.47104  0.468992\n",
      " 0.473088 0.47104  0.467968 0.473088] ms\n",
      "residual_forward_device \t = \t 0.273408ms | \t[0.011264 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024\n",
      " 0.011264 0.01024  0.009216 0.01024  0.01024  0.011264 0.01024  0.01024\n",
      " 0.009216 0.01024  0.01024  0.011264 0.01024  0.011264 0.009216 0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.193536ms | \t[0.023552 0.022528 0.024576 0.022528 0.022528 0.022528 0.022528 0.022528\n",
      " 0.022528 0.022528 0.022528 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.03072ms | \t[1.748992] ms\n",
      "cross_entropy_backward_device \t = \t 0.19456ms | \t[0.823296] ms\n",
      "mlp_backward_input_device \t = \t 1.119232ms | \t[13.678592  0.93696   0.960512  0.246784  0.723968  0.937984  0.970752\n",
      "  0.243712  0.697344  0.905216  0.925696  0.236544  0.695296  0.90112\n",
      "  0.924672  0.237568  0.694272  0.90112   0.925696  0.236544  0.694272\n",
      "  0.904192  0.928768  0.237568  0.69632   0.90624   0.946176  0.237568\n",
      "  0.694272  0.889856  0.909312  0.232448  0.685056  0.888832  0.910336\n",
      "  0.23552   0.676864  0.881664  0.900096  0.229376  0.67584   0.877568\n",
      "  0.902144  0.2304    0.674816  0.881664  0.903168  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 0.64ms | \t[14.528512  0.874496  0.93696   0.229376  0.67584   0.871424  0.932864\n",
      "  0.232448  0.692224  0.837632  0.90112   0.219136  0.649216  0.838656\n",
      "  0.899072  0.2304    0.690176  0.838656  0.900096  0.219136  0.652288\n",
      "  0.842752  0.902144  0.2304    0.690176  0.843776  0.903168  0.219136\n",
      "  0.64512   0.82432   0.886784  0.226304  0.67584   0.825344  0.88576\n",
      "  0.21504   0.633856  0.8192    0.881664  0.223232  0.674816  0.8192\n",
      "  0.879616  0.21504   0.633856  0.818176  0.877568  0.224256  0.672768] ms\n",
      "layernorm_backward_device \t = \t 2.702336ms | \t[0.032768 0.029696 0.026624 0.029696 0.029696 0.029696 0.0256   0.028672\n",
      " 0.029696 0.028672 0.026624 0.028672 0.028672 0.028672 0.026624 0.027648\n",
      " 0.029696 0.028672 0.024576 0.026624 0.028672 0.027648 0.0256   0.027648\n",
      " 0.028672] ms\n",
      "residual_backward_device \t = \t 0.059392ms | \t[0.013312 0.02048  0.013312 0.019456 0.013312 0.02048  0.014336 0.02048\n",
      " 0.013312 0.02048  0.014336 0.02048  0.013312 0.02048  0.014336 0.02048\n",
      " 0.014336 0.019456 0.012288 0.019456 0.014336 0.019456 0.013312 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.291264ms | \t[0.086016 0.086016 0.082944 0.084992 0.082944 0.084992 0.08192  0.082944\n",
      " 0.083968 0.083968 0.082944 0.083968] ms\n",
      "attention_backward_device \t = \t 0.06144ms | \t[1.49504  1.497088 1.472512 1.47456  1.476608 1.491968 1.468416 1.478656\n",
      " 1.47968  1.44384  1.518592 1.449984] ms\n",
      "adamw_kernel_device \t = \t 0.54784ms | \t[12.061696] ms\n",
      "total_time(21)=173.18502400000003|7.820288000000001\n",
      "layernorm_forward_device \t = \t 0.352256ms | \t[0.016384 0.017408 0.014336 0.018432 0.01536  0.016384 0.01536  0.01536\n",
      " 0.01536  0.018432 0.016384 0.01536  0.016384 0.016384 0.016384 0.01536\n",
      " 0.01536  0.01536  0.01536  0.016384 0.017408 0.016384 0.01536  0.01536\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.140288ms | \t[ 0.546816  0.193536  0.76288   0.756736  0.570368  0.191488  0.760832\n",
      "  0.647168  0.543744  0.192512  0.733184  0.755712  0.571392  0.191488\n",
      "  0.760832  0.755712  0.571392  0.191488  0.733184  0.647168  0.543744\n",
      "  0.191488  0.735232  0.755712  0.569344  0.191488  0.759808  0.754688\n",
      "  0.570368  0.191488  0.735232  0.755712  0.543744  0.191488  0.734208\n",
      "  0.755712  0.571392  0.190464  0.760832  0.754688  0.570368  0.16384\n",
      "  0.733184  0.755712  0.54272   0.191488  0.734208  0.755712 12.618752] ms\n",
      "attention_forward_device \t = \t 0.139264ms | \t[0.47616  0.475136 0.473088 0.47616  0.474112 0.473088 0.47616  0.47616\n",
      " 0.47616  0.47616  0.474112 0.478208] ms\n",
      "residual_forward_device \t = \t 0.24576ms | \t[0.009216 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024\n",
      " 0.012288 0.01024  0.011264 0.01024  0.01024  0.01024  0.009216 0.011264\n",
      " 0.009216 0.01024  0.01024  0.01024  0.01024  0.01024  0.009216 0.011264] ms\n",
      "gelu_forward_device \t = \t 0.191488ms | \t[0.021504 0.023552 0.022528 0.023552 0.022528 0.022528 0.022528 0.021504\n",
      " 0.022528 0.022528 0.021504 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.03072ms | \t[1.774592] ms\n",
      "cross_entropy_backward_device \t = \t 0.166912ms | \t[0.764928] ms\n",
      "mlp_backward_input_device \t = \t 0.687104ms | \t[13.68064   0.937984  0.825344  0.247808  0.72192   0.934912  0.961536\n",
      "  0.243712  0.697344  0.904192  0.804864  0.236544  0.69632   0.902144\n",
      "  0.924672  0.237568  0.69632   0.90112   0.807936  0.237568  0.695296\n",
      "  0.905216  0.930816  0.237568  0.69632   0.90624   0.795648  0.237568\n",
      "  0.694272  0.888832  0.91136   0.232448  0.685056  0.888832  0.781312\n",
      "  0.232448  0.676864  0.881664  0.915456  0.229376  0.676864  0.877568\n",
      "  0.774144  0.2304    0.678912  0.88064   0.909312  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 1.393664ms | \t[14.544896  0.872448  0.93696   0.229376  0.6656    0.8704    0.933888\n",
      "  0.232448  0.647168  0.836608  0.90112   0.219136  0.643072  0.836608\n",
      "  0.900096  0.2304    0.648192  0.836608  0.900096  0.219136  0.64512\n",
      "  0.840704  0.902144  0.231424  0.65024   0.841728  0.904192  0.219136\n",
      "  0.641024  0.82432   0.886784  0.227328  0.636928  0.82432   0.886784\n",
      "  0.216064  0.62976   0.818176  0.882688  0.224256  0.632832  0.816128\n",
      "  0.879616  0.214016  0.627712  0.816128  0.878592  0.224256  0.631808] ms\n",
      "layernorm_backward_device \t = \t 2.9696ms | \t[0.031744 0.029696 0.027648 0.03072  0.029696 0.029696 0.0256   0.028672\n",
      " 0.029696 0.028672 0.027648 0.029696 0.028672 0.029696 0.0256   0.028672\n",
      " 0.029696 0.027648 0.024576 0.028672 0.029696 0.027648 0.026624 0.028672\n",
      " 0.029696] ms\n",
      "residual_backward_device \t = \t 0.055296ms | \t[0.01536  0.021504 0.013312 0.02048  0.014336 0.02048  0.014336 0.021504\n",
      " 0.014336 0.019456 0.014336 0.021504 0.013312 0.02048  0.013312 0.021504\n",
      " 0.014336 0.021504 0.013312 0.02048  0.014336 0.019456 0.014336 0.019456] ms\n",
      "gelu_backward_device \t = \t 1.312768ms | \t[0.08704  0.08704  0.084992 0.086016 0.083968 0.086016 0.084992 0.086016\n",
      " 0.084992 0.083968 0.083968 0.084992] ms\n",
      "attention_backward_device \t = \t 0.062464ms | \t[1.54624  1.532928 1.522688 1.522688 1.517568 1.533952 1.52064  1.499136\n",
      " 1.508352 1.492992 1.50016  1.501184] ms\n",
      "embedding_backward_device \t = \t 0.0ms | \t[0.543744] ms\n",
      "adamw_kernel_device \t = \t 0.001024ms | \t[12.145664] ms\n",
      "total_time(22)=173.325312|7.748608000000001\n",
      "layernorm_forward_device \t = \t 0.35328ms | \t[0.016384 0.017408 0.014336 0.016384 0.01536  0.01536  0.01536  0.01536\n",
      " 0.014336 0.018432 0.01536  0.01536  0.016384 0.01536  0.01536  0.01536\n",
      " 0.01536  0.01536  0.01536  0.01536  0.016384 0.01536  0.016384 0.01536\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.169984ms | \t[ 0.546816  0.193536  0.735232  0.755712  0.570368  0.191488  0.761856\n",
      "  0.754688  0.543744  0.192512  0.733184  0.755712  0.571392  0.192512\n",
      "  0.761856  0.756736  0.570368  0.192512  0.760832  0.647168  0.54272\n",
      "  0.192512  0.735232  0.755712  0.570368  0.192512  0.759808  0.754688\n",
      "  0.571392  0.191488  0.735232  0.647168  0.543744  0.191488  0.734208\n",
      "  0.755712  0.570368  0.190464  0.760832  0.753664  0.570368  0.164864\n",
      "  0.734208  0.755712  0.54272   0.192512  0.734208  0.754688 12.618752] ms\n",
      "attention_forward_device \t = \t 0.140288ms | \t[0.474112 0.474112 0.472064 0.475136 0.473088 0.472064 0.474112 0.475136\n",
      " 0.470016 0.475136 0.472064 0.477184] ms\n",
      "residual_forward_device \t = \t 0.244736ms | \t[0.01024  0.011264 0.01024  0.01024  0.011264 0.01024  0.01024  0.01024\n",
      " 0.011264 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.009216 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.188416ms | \t[0.023552 0.022528 0.022528 0.023552 0.022528 0.022528 0.022528 0.023552\n",
      " 0.022528 0.022528 0.021504 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.03072ms | \t[1.768448] ms\n",
      "cross_entropy_backward_device \t = \t 0.171008ms | \t[0.764928] ms\n",
      "mlp_backward_input_device \t = \t 0.86528ms | \t[13.679616  0.9728    0.833536  0.247808  0.723968  0.935936  0.961536\n",
      "  0.243712  0.697344  0.934912  0.7936    0.236544  0.69632   0.902144\n",
      "  0.92672   0.237568  0.69632   0.939008  0.792576  0.236544  0.695296\n",
      "  0.90624   0.94208   0.239616  0.69632   0.941056  0.794624  0.237568\n",
      "  0.694272  0.889856  0.927744  0.232448  0.685056  0.920576  0.781312\n",
      "  0.232448  0.676864  0.88064   0.905216  0.229376  0.676864  0.913408\n",
      "  0.77312   0.233472  0.677888  0.879616  0.902144  0.231424  0.678912] ms\n",
      "mlp_backward_weight_device \t = \t 1.197056ms | \t[14.431232  0.874496  0.937984  0.2304    0.68096   0.874496  0.933888\n",
      "  0.232448  0.64512   0.837632  0.90112   0.221184  0.653312  0.838656\n",
      "  0.900096  0.2304    0.64512   0.838656  0.90112   0.22016   0.656384\n",
      "  0.8448    0.904192  0.231424  0.649216  0.841728  0.904192  0.22016\n",
      "  0.65024   0.826368  0.88576   0.227328  0.630784  0.82432   0.886784\n",
      "  0.217088  0.637952  0.8192    0.881664  0.224256  0.62976   0.817152\n",
      "  0.88064   0.21504   0.638976  0.8192    0.878592  0.224256  0.628736] ms\n",
      "layernorm_backward_device \t = \t 3.033088ms | \t[0.033792 0.03072  0.027648 0.029696 0.028672 0.029696 0.0256   0.028672\n",
      " 0.029696 0.028672 0.026624 0.029696 0.028672 0.028672 0.026624 0.028672\n",
      " 0.029696 0.028672 0.026624 0.027648 0.028672 0.028672 0.026624 0.028672\n",
      " 0.029696] ms\n",
      "residual_backward_device \t = \t 0.05632ms | \t[0.014336 0.021504 0.013312 0.021504 0.014336 0.02048  0.013312 0.02048\n",
      " 0.014336 0.019456 0.014336 0.021504 0.013312 0.02048  0.014336 0.021504\n",
      " 0.012288 0.02048  0.013312 0.019456 0.013312 0.02048  0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.288192ms | \t[0.086016 0.086016 0.083968 0.086016 0.083968 0.084992 0.083968 0.083968\n",
      " 0.083968 0.083968 0.083968 0.084992] ms\n",
      "attention_backward_device \t = \t 0.05632ms | \t[1.508352 1.485824 1.486848 1.508352 1.487872 1.537024 1.481728 1.480704\n",
      " 1.529856 1.481728 1.522688 1.514496] ms\n",
      "adamw_kernel_device \t = \t 0.54784ms | \t[12.1088] ms\n",
      "total_time(23)=172.705792|8.342528\n",
      "layernorm_forward_device \t = \t 0.349184ms | \t[0.016384 0.017408 0.01536  0.01536  0.01536  0.016384 0.016384 0.016384\n",
      " 0.01536  0.018432 0.016384 0.01536  0.016384 0.014336 0.01536  0.017408\n",
      " 0.016384 0.01536  0.01536  0.016384 0.01536  0.016384 0.01536  0.01536\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.162816ms | \t[ 0.546816  0.193536  0.734208  0.755712  0.570368  0.191488  0.760832\n",
      "  0.754688  0.543744  0.164864  0.734208  0.755712  0.544768  0.192512\n",
      "  0.761856  0.756736  0.571392  0.191488  0.760832  0.646144  0.543744\n",
      "  0.192512  0.735232  0.754688  0.570368  0.191488  0.759808  0.754688\n",
      "  0.571392  0.191488  0.733184  0.647168  0.543744  0.192512  0.733184\n",
      "  0.755712  0.570368  0.191488  0.760832  0.754688  0.570368  0.191488\n",
      "  0.734208  0.755712  0.54272   0.191488  0.734208  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.165888ms | \t[0.475136 0.475136 0.47104  0.475136 0.474112 0.472064 0.47616  0.472064\n",
      " 0.47104  0.474112 0.473088 0.475136] ms\n",
      "residual_forward_device \t = \t 0.244736ms | \t[0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.011264\n",
      " 0.011264 0.01024  0.011264 0.01024  0.011264 0.011264 0.01024  0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.01024  0.009216] ms\n",
      "gelu_forward_device \t = \t 0.190464ms | \t[0.023552 0.021504 0.022528 0.023552 0.022528 0.022528 0.022528 0.023552\n",
      " 0.022528 0.022528 0.022528 0.021504] ms\n",
      "softmax_forward_device \t = \t 0.018432ms | \t[1.769472] ms\n",
      "cross_entropy_backward_device \t = \t 0.173056ms | \t[0.756736] ms\n",
      "mlp_backward_input_device \t = \t 1.10592ms | \t[13.679616  0.971776  0.825344  0.247808  0.72192   0.934912  0.963584\n",
      "  0.243712  0.697344  0.935936  0.792576  0.236544  0.69632   0.902144\n",
      "  0.93184   0.237568  0.69632   0.937984  0.794624  0.236544  0.695296\n",
      "  0.905216  0.928768  0.237568  0.69632   0.94208   0.797696  0.237568\n",
      "  0.694272  0.889856  0.907264  0.232448  0.685056  0.919552  0.780288\n",
      "  0.232448  0.676864  0.881664  0.900096  0.229376  0.676864  0.913408\n",
      "  0.782336  0.231424  0.677888  0.879616  0.903168  0.231424  0.678912] ms\n",
      "mlp_backward_weight_device \t = \t 1.229824ms | \t[14.434304  0.986112  0.937984  0.229376  0.678912  0.8704    0.933888\n",
      "  0.232448  0.633856  0.945152  0.90112   0.221184  0.653312  0.836608\n",
      "  0.899072  0.2304    0.63488   0.949248  0.90112   0.22016   0.654336\n",
      "  0.840704  0.902144  0.2304    0.636928  0.950272  0.904192  0.22016\n",
      "  0.649216  0.82432   0.88576   0.226304  0.622592  0.93184   0.886784\n",
      "  0.217088  0.638976  0.817152  0.881664  0.224256  0.618496  0.923648\n",
      "  0.88064   0.216064  0.637952  0.816128  0.878592  0.224256  0.618496] ms\n",
      "layernorm_backward_device \t = \t 3.095552ms | \t[0.032768 0.03072  0.026624 0.03072  0.028672 0.03072  0.026624 0.029696\n",
      " 0.028672 0.028672 0.027648 0.029696 0.029696 0.029696 0.026624 0.028672\n",
      " 0.029696 0.028672 0.026624 0.028672 0.028672 0.028672 0.026624 0.028672\n",
      " 0.028672] ms\n",
      "residual_backward_device \t = \t 0.05632ms | \t[0.014336 0.02048  0.013312 0.02048  0.014336 0.019456 0.013312 0.02048\n",
      " 0.014336 0.02048  0.013312 0.021504 0.013312 0.02048  0.013312 0.021504\n",
      " 0.013312 0.021504 0.013312 0.02048  0.014336 0.019456 0.013312 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 0.658432ms | \t[0.086016 0.08704  0.084992 0.084992 0.084992 0.083968 0.084992 0.084992\n",
      " 0.083968 0.084992 0.082944 0.084992] ms\n",
      "attention_backward_device \t = \t 0.055296ms | \t[1.50528  1.499136 1.467392 1.512448 1.46944  1.487872 1.46944  1.487872\n",
      " 1.4592   1.491968 1.481728 1.461248] ms\n",
      "adamw_kernel_device \t = \t 0.548864ms | \t[12.07808] ms\n",
      "total_time(24)=172.95872000000003|8.054784000000001\n",
      "layernorm_forward_device \t = \t 0.346112ms | \t[0.016384 0.016384 0.016384 0.01536  0.017408 0.016384 0.014336 0.016384\n",
      " 0.01536  0.018432 0.01536  0.016384 0.01536  0.01536  0.014336 0.018432\n",
      " 0.016384 0.016384 0.01536  0.01536  0.014336 0.01536  0.01536  0.01536\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.120832ms | \t[ 0.546816  0.193536  0.734208  0.755712  0.570368  0.192512  0.760832\n",
      "  0.754688  0.543744  0.164864  0.734208  0.756736  0.544768  0.192512\n",
      "  0.734208  0.755712  0.570368  0.192512  0.760832  0.755712  0.54272\n",
      "  0.192512  0.734208  0.754688  0.570368  0.192512  0.760832  0.754688\n",
      "  0.571392  0.191488  0.760832  0.647168  0.543744  0.191488  0.733184\n",
      "  0.755712  0.571392  0.190464  0.760832  0.754688  0.570368  0.191488\n",
      "  0.734208  0.647168  0.543744  0.192512  0.734208  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.16384ms | \t[0.477184 0.477184 0.475136 0.479232 0.47616  0.475136 0.477184 0.477184\n",
      " 0.475136 0.478208 0.477184 0.474112] ms\n",
      "residual_forward_device \t = \t 0.246784ms | \t[0.01024  0.011264 0.01024  0.011264 0.01024  0.01024  0.01024  0.01024\n",
      " 0.012288 0.01024  0.009216 0.01024  0.01024  0.01024  0.011264 0.011264\n",
      " 0.01024  0.011264 0.01024  0.011264 0.011264 0.009216 0.01024  0.01024 ] ms\n",
      "gelu_forward_device \t = \t 0.191488ms | \t[0.0256   0.022528 0.023552 0.021504 0.022528 0.023552 0.022528 0.022528\n",
      " 0.022528 0.022528 0.022528 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.018432ms | \t[1.794048] ms\n",
      "cross_entropy_backward_device \t = \t 0.147456ms | \t[0.770048] ms\n",
      "mlp_backward_input_device \t = \t 0.764928ms | \t[13.709312  0.9728    0.82432   0.246784  0.723968  0.934912  0.974848\n",
      "  0.244736  0.697344  0.934912  0.794624  0.236544  0.694272  0.902144\n",
      "  0.92672   0.241664  0.699392  0.937984  0.7936    0.236544  0.698368\n",
      "  0.904192  0.928768  0.237568  0.69632   0.941056  0.809984  0.237568\n",
      "  0.69632   0.888832  0.909312  0.232448  0.683008  0.919552  0.780288\n",
      "  0.234496  0.678912  0.88064   0.904192  0.229376  0.676864  0.912384\n",
      "  0.772096  0.2304    0.677888  0.879616  0.903168  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 1.183744ms | \t[14.427136  0.883712  0.937984  0.2304    0.679936  0.8704    0.934912\n",
      "  0.232448  0.641024  0.847872  0.90112   0.221184  0.653312  0.837632\n",
      "  0.90112   0.229376  0.641024  0.847872  0.90112   0.22016   0.65536\n",
      "  0.843776  0.904192  0.231424  0.64512   0.851968  0.904192  0.22016\n",
      "  0.648192  0.826368  0.887808  0.226304  0.628736  0.835584  0.886784\n",
      "  0.217088  0.636928  0.818176  0.882688  0.224256  0.627712  0.826368\n",
      "  0.88064   0.216064  0.637952  0.817152  0.879616  0.224256  0.62976 ] ms\n",
      "layernorm_backward_device \t = \t 3.049472ms | \t[0.03072  0.03072  0.027648 0.03072  0.027648 0.03072  0.026624 0.029696\n",
      " 0.026624 0.028672 0.026624 0.029696 0.0256   0.029696 0.026624 0.028672\n",
      " 0.026624 0.028672 0.026624 0.028672 0.026624 0.028672 0.026624 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.064512ms | \t[0.014336 0.023552 0.014336 0.021504 0.01536  0.02048  0.014336 0.02048\n",
      " 0.013312 0.021504 0.014336 0.021504 0.014336 0.02048  0.013312 0.021504\n",
      " 0.014336 0.02048  0.012288 0.02048  0.013312 0.019456 0.014336 0.02048 ] ms\n",
      "gelu_backward_device \t = \t 1.236992ms | \t[0.089088 0.088064 0.08704  0.086016 0.08704  0.086016 0.08704  0.086016\n",
      " 0.08704  0.084992 0.086016 0.086016] ms\n",
      "attention_backward_device \t = \t 0.05632ms | \t[1.540096 1.537024 1.491968 1.532928 1.477632 1.534976 1.50016  1.503232\n",
      " 1.501184 1.503232 1.504256 1.472512] ms\n",
      "adamw_kernel_device \t = \t 0.549888ms | \t[12.118016] ms\n",
      "total_time(25)=172.91468799999998|8.1408\n",
      "layernorm_forward_device \t = \t 0.346112ms | \t[0.016384 0.017408 0.01536  0.014336 0.017408 0.016384 0.014336 0.01536\n",
      " 0.01536  0.01536  0.016384 0.014336 0.016384 0.01536  0.014336 0.017408\n",
      " 0.016384 0.01536  0.014336 0.01536  0.01536  0.018432 0.01536  0.01536\n",
      " 0.016384] ms\n",
      "mlp_forward_device \t = \t 0.154624ms | \t[ 0.546816  0.193536  0.734208  0.756736  0.569344  0.192512  0.760832\n",
      "  0.755712  0.571392  0.164864  0.734208  0.756736  0.544768  0.192512\n",
      "  0.734208  0.755712  0.570368  0.192512  0.759808  0.755712  0.543744\n",
      "  0.164864  0.734208  0.754688  0.54272   0.192512  0.760832  0.754688\n",
      "  0.570368  0.192512  0.760832  0.647168  0.543744  0.192512  0.733184\n",
      "  0.754688  0.571392  0.190464  0.760832  0.754688  0.570368  0.191488\n",
      "  0.733184  0.647168  0.543744  0.192512  0.734208  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.164864ms | \t[0.475136 0.475136 0.470016 0.47616  0.474112 0.474112 0.475136 0.475136\n",
      " 0.472064 0.477184 0.474112 0.47104 ] ms\n",
      "residual_forward_device \t = \t 0.274432ms | \t[0.011264 0.01024  0.01024  0.011264 0.009216 0.01024  0.01024  0.011264\n",
      " 0.011264 0.011264 0.009216 0.01024  0.01024  0.01024  0.011264 0.01024\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.009216 0.01024  0.009216] ms\n",
      "gelu_forward_device \t = \t 0.193536ms | \t[0.023552 0.022528 0.023552 0.021504 0.023552 0.021504 0.022528 0.022528\n",
      " 0.022528 0.022528 0.023552 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.018432ms | \t[1.765376] ms\n",
      "cross_entropy_backward_device \t = \t 0.177152ms | \t[0.76288] ms\n",
      "mlp_backward_input_device \t = \t 0.713728ms | \t[13.720576  0.974848  0.96256   0.211968  0.722944  0.937984  0.96256\n",
      "  0.243712  0.698368  0.937984  0.941056  0.203776  0.694272  0.902144\n",
      "  0.924672  0.237568  0.695296  0.935936  0.943104  0.203776  0.69632\n",
      "  0.903168  0.928768  0.237568  0.69632   0.940032  0.927744  0.203776\n",
      "  0.697344  0.887808  0.909312  0.232448  0.681984  0.919552  0.912384\n",
      "  0.19968   0.679936  0.88064   0.918528  0.229376  0.67584   0.909312\n",
      "  0.903168  0.197632  0.677888  0.876544  0.912384  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 0.564224ms | \t[14.425088  0.883712  0.937984  0.2304    0.67072   0.873472  0.934912\n",
      "  0.232448  0.643072  0.848896  0.90112   0.221184  0.643072  0.838656\n",
      "  0.900096  0.2304    0.64512   0.851968  0.90112   0.221184  0.644096\n",
      "  0.841728  0.904192  0.231424  0.649216  0.854016  0.905216  0.22016\n",
      "  0.64      0.825344  0.887808  0.226304  0.628736  0.837632  0.886784\n",
      "  0.217088  0.626688  0.820224  0.881664  0.224256  0.627712  0.828416\n",
      "  0.88064   0.21504   0.627712  0.816128  0.879616  0.224256  0.627712] ms\n",
      "layernorm_backward_device \t = \t 3.101696ms | \t[0.029696 0.029696 0.027648 0.03072  0.026624 0.028672 0.0256   0.029696\n",
      " 0.026624 0.028672 0.026624 0.029696 0.027648 0.028672 0.0256   0.028672\n",
      " 0.027648 0.028672 0.026624 0.028672 0.026624 0.028672 0.026624 0.028672\n",
      " 0.026624] ms\n",
      "residual_backward_device \t = \t 0.070656ms | \t[0.014336 0.02048  0.013312 0.019456 0.013312 0.02048  0.014336 0.021504\n",
      " 0.016384 0.02048  0.013312 0.02048  0.014336 0.02048  0.013312 0.02048\n",
      " 0.014336 0.02048  0.013312 0.02048  0.014336 0.019456 0.013312 0.021504] ms\n",
      "gelu_backward_device \t = \t 1.226752ms | \t[0.088064 0.08704  0.086016 0.086016 0.086016 0.084992 0.083968 0.083968\n",
      " 0.084992 0.083968 0.082944 0.084992] ms\n",
      "attention_backward_device \t = \t 0.057344ms | \t[1.502208 1.559552 1.524736 1.55136  1.470464 1.531904 1.46944  1.52576\n",
      " 1.494016 1.52064  1.511424 1.516544] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[11.995136] ms\n",
      "total_time(26)=173.31609599999996|7.614464\n",
      "layernorm_forward_device \t = \t 0.354304ms | \t[0.016384 0.017408 0.01536  0.01536  0.01536  0.016384 0.01536  0.01536\n",
      " 0.01536  0.01536  0.018432 0.01536  0.01536  0.01536  0.01536  0.016384\n",
      " 0.016384 0.01536  0.014336 0.016384 0.01536  0.018432 0.01536  0.016384\n",
      " 0.017408] ms\n",
      "mlp_forward_device \t = \t 0.166912ms | \t[ 0.545792  0.193536  0.734208  0.756736  0.569344  0.192512  0.760832\n",
      "  0.755712  0.571392  0.164864  0.733184  0.756736  0.543744  0.191488\n",
      "  0.734208  0.755712  0.570368  0.192512  0.759808  0.755712  0.543744\n",
      "  0.164864  0.734208  0.755712  0.54272   0.192512  0.734208  0.754688\n",
      "  0.570368  0.191488  0.761856  0.754688  0.543744  0.192512  0.733184\n",
      "  0.754688  0.571392  0.191488  0.761856  0.754688  0.570368  0.191488\n",
      "  0.759808  0.647168  0.543744  0.192512  0.733184  0.755712 12.630016] ms\n",
      "attention_forward_device \t = \t 0.166912ms | \t[0.47104  0.474112 0.473088 0.475136 0.474112 0.472064 0.47616  0.474112\n",
      " 0.472064 0.474112 0.472064 0.47104 ] ms\n",
      "residual_forward_device \t = \t 0.166912ms | \t[0.01024  0.01024  0.011264 0.01024  0.009216 0.009216 0.01024  0.011264\n",
      " 0.011264 0.011264 0.009216 0.01024  0.01024  0.011264 0.01024  0.011264\n",
      " 0.01024  0.01024  0.01024  0.01024  0.01024  0.009216 0.01024  0.009216] ms\n",
      "gelu_forward_device \t = \t 0.193536ms | \t[0.023552 0.022528 0.021504 0.022528 0.022528 0.021504 0.022528 0.022528\n",
      " 0.021504 0.022528 0.022528 0.022528] ms\n",
      "softmax_forward_device \t = \t 0.019456ms | \t[1.773568] ms\n",
      "cross_entropy_backward_device \t = \t 0.169984ms | \t[0.76288] ms\n",
      "mlp_backward_input_device \t = \t 0.661504ms | \t[13.720576  0.974848  0.9728    0.212992  0.72192   0.937984  0.960512\n",
      "  0.244736  0.697344  0.93696   0.925696  0.202752  0.694272  0.903168\n",
      "  0.92672   0.237568  0.695296  0.935936  0.924672  0.202752  0.695296\n",
      "  0.907264  0.943104  0.239616  0.697344  0.941056  0.927744  0.202752\n",
      "  0.69632   0.888832  0.924672  0.232448  0.681984  0.920576  0.910336\n",
      "  0.19968   0.678912  0.88064   0.902144  0.229376  0.67584   0.909312\n",
      "  0.902144  0.198656  0.677888  0.878592  0.902144  0.231424  0.676864] ms\n",
      "mlp_backward_weight_device \t = \t 0.576512ms | \t[14.386176  0.88064   0.946176  0.2304    0.666624  0.8704    0.934912\n",
      "  0.232448  0.644096  0.846848  0.909312  0.221184  0.64      0.836608\n",
      "  0.90112   0.2304    0.64512   0.847872  0.909312  0.22016   0.64512\n",
      "  0.83968   0.904192  0.231424  0.649216  0.850944  0.912384  0.22016\n",
      "  0.636928  0.82432   0.887808  0.226304  0.632832  0.831488  0.894976\n",
      "  0.217088  0.625664  0.818176  0.882688  0.224256  0.62976   0.826368\n",
      "  0.887808  0.216064  0.625664  0.816128  0.88064   0.224256  0.628736] ms\n",
      "layernorm_backward_device \t = \t 3.100672ms | \t[0.029696 0.029696 0.026624 0.029696 0.027648 0.028672 0.0256   0.029696\n",
      " 0.026624 0.028672 0.026624 0.029696 0.026624 0.03072  0.0256   0.028672\n",
      " 0.026624 0.028672 0.026624 0.027648 0.0256   0.028672 0.026624 0.028672\n",
      " 0.0256  ] ms\n",
      "residual_backward_device \t = \t 0.069632ms | \t[0.013312 0.02048  0.013312 0.019456 0.01536  0.02048  0.013312 0.02048\n",
      " 0.014336 0.02048  0.014336 0.019456 0.014336 0.019456 0.013312 0.019456\n",
      " 0.013312 0.019456 0.013312 0.019456 0.013312 0.019456 0.014336 0.019456] ms\n",
      "gelu_backward_device \t = \t 1.256448ms | \t[0.088064 0.08704  0.083968 0.084992 0.086016 0.084992 0.084992 0.084992\n",
      " 0.084992 0.084992 0.084992 0.084992] ms\n",
      "attention_backward_device \t = \t 0.057344ms | \t[1.509376 1.570816 1.534976 1.570816 1.461248 1.5616   1.46944  1.54112\n",
      " 1.429504 1.53088  1.522688 1.531904] ms\n",
      "adamw_kernel_device \t = \t 0.550912ms | \t[12.002304] ms\n",
      "total_time(27)=173.43078400000002|7.5110399999999995\n"
     ]
    }
   ],
   "source": [
    "for sm_no, sm in enumerate(timer[0]):\n",
    "    kern_times = defaultdict(list)\n",
    "    wait_times = defaultdict(list)\n",
    "    for instr_no, instr in sm.items():\n",
    "        kern_times[instr[\"kernelName\"]].append(instr[\"instr_end\"] - instr[\"bar_exit\"])\n",
    "        wait_times[instr[\"kernelName\"]].append(instr[\"bar_exit\"] - instr[\"bar_enter\"])\n",
    "    count = 0\n",
    "    wait_count = 0\n",
    "    for k, times in kern_times.items():\n",
    "        count += np.sum(times) / (1_000_000) \n",
    "        wait_count += np.sum(wait_times[k]) / (1_000_000)\n",
    "        print(f\"{k} \\t = \\t {np.sum(wait_times[k]) / (1_000_000)}ms | \\t{np.array(times) / (1_000_000)} ms\")\n",
    "    print(f\"total_time({sm_no})={count}|{wait_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26444de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "id": "37c3b787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_forward_device \t = \t 0.0 ms\n",
      "layernorm_forward_device \t = \t 0.031744 ms\n",
      "mlp_forward_device \t = \t 0.139264 ms\n",
      "attention_forward_device \t = \t 0.167936 ms\n",
      "residual_forward_device \t = \t 0.16896 ms\n",
      "gelu_forward_device \t = \t 0.191488 ms\n",
      "softmax_forward_device \t = \t 0.002048 ms\n",
      "cross_entropy_backward_device \t = \t 0.159744 ms\n",
      "mlp_backward_input_device \t = \t 0.873472 ms\n",
      "mlp_backward_weight_device \t = \t 0.605184 ms\n",
      "layernorm_backward_device \t = \t 3.10272 ms\n",
      "residual_backward_device \t = \t 0.047104 ms\n",
      "gelu_backward_device \t = \t 1.202176 ms\n",
      "attention_backward_device \t = \t 0.05632 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(0)=7.2980480000000005\n",
      "embedding_forward_device \t = \t 0.0 ms\n",
      "layernorm_forward_device \t = \t 0.031744 ms\n",
      "mlp_forward_device \t = \t 0.151552 ms\n",
      "attention_forward_device \t = \t 0.165888 ms\n",
      "residual_forward_device \t = \t 0.164864 ms\n",
      "gelu_forward_device \t = \t 0.22016 ms\n",
      "softmax_forward_device \t = \t 0.002048 ms\n",
      "cross_entropy_backward_device \t = \t 0.15872 ms\n",
      "mlp_backward_input_device \t = \t 0.898048 ms\n",
      "mlp_backward_weight_device \t = \t 1.230848 ms\n",
      "layernorm_backward_device \t = \t 3.204096 ms\n",
      "residual_backward_device \t = \t 0.055296 ms\n",
      "gelu_backward_device \t = \t 1.246208 ms\n",
      "attention_backward_device \t = \t 0.057344 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(1)=8.137728\n",
      "embedding_forward_device \t = \t 0.0 ms\n",
      "layernorm_forward_device \t = \t 0.021504 ms\n",
      "mlp_forward_device \t = \t 0.08704 ms\n",
      "attention_forward_device \t = \t 0.13824 ms\n",
      "residual_forward_device \t = \t 0.169984 ms\n",
      "gelu_forward_device \t = \t 0.216064 ms\n",
      "softmax_forward_device \t = \t 0.002048 ms\n",
      "cross_entropy_backward_device \t = \t 0.123904 ms\n",
      "mlp_backward_input_device \t = \t 0.310272 ms\n",
      "mlp_backward_weight_device \t = \t 0.992256 ms\n",
      "layernorm_backward_device \t = \t 3.142656 ms\n",
      "residual_backward_device \t = \t 0.0512 ms\n",
      "gelu_backward_device \t = \t 1.205248 ms\n",
      "attention_backward_device \t = \t 0.057344 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(2)=7.067647999999999\n",
      "embedding_forward_device \t = \t 0.0 ms\n",
      "layernorm_forward_device \t = \t 0.022528 ms\n",
      "mlp_forward_device \t = \t 0.147456 ms\n",
      "attention_forward_device \t = \t 0.140288 ms\n",
      "residual_forward_device \t = \t 0.280576 ms\n",
      "gelu_forward_device \t = \t 0.19456 ms\n",
      "softmax_forward_device \t = \t 0.018432 ms\n",
      "cross_entropy_backward_device \t = \t 0.167936 ms\n",
      "mlp_backward_input_device \t = \t 0.882688 ms\n",
      "mlp_backward_weight_device \t = \t 1.010688 ms\n",
      "layernorm_backward_device \t = \t 3.151872 ms\n",
      "residual_backward_device \t = \t 0.057344 ms\n",
      "gelu_backward_device \t = \t 1.227776 ms\n",
      "attention_backward_device \t = \t 0.002048 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(3)=7.855104000000001\n",
      "layernorm_forward_device \t = \t 0.335872 ms\n",
      "mlp_forward_device \t = \t 0.144384 ms\n",
      "attention_forward_device \t = \t 0.142336 ms\n",
      "residual_forward_device \t = \t 0.27648 ms\n",
      "gelu_forward_device \t = \t 0.192512 ms\n",
      "softmax_forward_device \t = \t 0.018432 ms\n",
      "cross_entropy_backward_device \t = \t 0.167936 ms\n",
      "mlp_backward_input_device \t = \t 0.652288 ms\n",
      "mlp_backward_weight_device \t = \t 1.01376 ms\n",
      "layernorm_backward_device \t = \t 3.117056 ms\n",
      "residual_backward_device \t = \t 0.06656 ms\n",
      "gelu_backward_device \t = \t 1.284096 ms\n",
      "attention_backward_device \t = \t 0.003072 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(4)=7.965696\n",
      "layernorm_forward_device \t = \t 0.34304 ms\n",
      "mlp_forward_device \t = \t 0.069632 ms\n",
      "attention_forward_device \t = \t 0.142336 ms\n",
      "residual_forward_device \t = \t 0.273408 ms\n",
      "gelu_forward_device \t = \t 0.19456 ms\n",
      "softmax_forward_device \t = \t 0.019456 ms\n",
      "cross_entropy_backward_device \t = \t 0.126976 ms\n",
      "mlp_backward_input_device \t = \t 0.292864 ms\n",
      "mlp_backward_weight_device \t = \t 0.402432 ms\n",
      "layernorm_backward_device \t = \t 3.16928 ms\n",
      "residual_backward_device \t = \t 0.059392 ms\n",
      "gelu_backward_device \t = \t 1.270784 ms\n",
      "attention_backward_device \t = \t 0.062464 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(5)=6.9765120000000005\n",
      "layernorm_forward_device \t = \t 0.360448 ms\n",
      "mlp_forward_device \t = \t 0.198656 ms\n",
      "attention_forward_device \t = \t 0.13824 ms\n",
      "residual_forward_device \t = \t 0.27648 ms\n",
      "gelu_forward_device \t = \t 0.195584 ms\n",
      "softmax_forward_device \t = \t 0.019456 ms\n",
      "cross_entropy_backward_device \t = \t 0.198656 ms\n",
      "mlp_backward_input_device \t = \t 1.099776 ms\n",
      "mlp_backward_weight_device \t = \t 0.36864 ms\n",
      "layernorm_backward_device \t = \t 1.62304 ms\n",
      "residual_backward_device \t = \t 0.080896 ms\n",
      "gelu_backward_device \t = \t 1.301504 ms\n",
      "attention_backward_device \t = \t 0.062464 ms\n",
      "adamw_kernel_device \t = \t 0.551936 ms\n",
      "wait_time(6)=6.475776\n",
      "layernorm_forward_device \t = \t 0.362496 ms\n",
      "mlp_forward_device \t = \t 0.20992 ms\n",
      "attention_forward_device \t = \t 0.141312 ms\n",
      "residual_forward_device \t = \t 0.27648 ms\n",
      "gelu_forward_device \t = \t 0.196608 ms\n",
      "softmax_forward_device \t = \t 0.019456 ms\n",
      "cross_entropy_backward_device \t = \t 0.187392 ms\n",
      "mlp_backward_input_device \t = \t 1.053696 ms\n",
      "mlp_backward_weight_device \t = \t 0.59904 ms\n",
      "layernorm_backward_device \t = \t 2.767872 ms\n",
      "residual_backward_device \t = \t 0.05632 ms\n",
      "gelu_backward_device \t = \t 1.28512 ms\n",
      "attention_backward_device \t = \t 0.063488 ms\n",
      "adamw_kernel_device \t = \t 0.551936 ms\n",
      "wait_time(7)=7.771136\n",
      "layernorm_forward_device \t = \t 0.347136 ms\n",
      "mlp_forward_device \t = \t 0.136192 ms\n",
      "attention_forward_device \t = \t 0.140288 ms\n",
      "residual_forward_device \t = \t 0.27648 ms\n",
      "gelu_forward_device \t = \t 0.193536 ms\n",
      "softmax_forward_device \t = \t 0.019456 ms\n",
      "cross_entropy_backward_device \t = \t 0.155648 ms\n",
      "mlp_backward_input_device \t = \t 0.78336 ms\n",
      "mlp_backward_weight_device \t = \t 1.439744 ms\n",
      "layernorm_backward_device \t = \t 3.048448 ms\n",
      "residual_backward_device \t = \t 0.053248 ms\n",
      "gelu_backward_device \t = \t 1.303552 ms\n",
      "attention_backward_device \t = \t 0.063488 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(8)=8.510464\n",
      "layernorm_forward_device \t = \t 0.34816 ms\n",
      "mlp_forward_device \t = \t 0.169984 ms\n",
      "attention_forward_device \t = \t 0.142336 ms\n",
      "residual_forward_device \t = \t 0.273408 ms\n",
      "gelu_forward_device \t = \t 0.18944 ms\n",
      "softmax_forward_device \t = \t 0.019456 ms\n",
      "cross_entropy_backward_device \t = \t 0.171008 ms\n",
      "mlp_backward_input_device \t = \t 0.644096 ms\n",
      "mlp_backward_weight_device \t = \t 1.210368 ms\n",
      "layernorm_backward_device \t = \t 2.99008 ms\n",
      "residual_backward_device \t = \t 0.058368 ms\n",
      "gelu_backward_device \t = \t 1.287168 ms\n",
      "attention_backward_device \t = \t 0.055296 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(9)=8.11008\n",
      "layernorm_forward_device \t = \t 0.344064 ms\n",
      "mlp_forward_device \t = \t 0.166912 ms\n",
      "attention_forward_device \t = \t 0.139264 ms\n",
      "residual_forward_device \t = \t 0.273408 ms\n",
      "gelu_forward_device \t = \t 0.193536 ms\n",
      "softmax_forward_device \t = \t 0.047104 ms\n",
      "cross_entropy_backward_device \t = \t 0.188416 ms\n",
      "mlp_backward_input_device \t = \t 1.052672 ms\n",
      "mlp_backward_weight_device \t = \t 1.195008 ms\n",
      "layernorm_backward_device \t = \t 3.060736 ms\n",
      "residual_backward_device \t = \t 0.057344 ms\n",
      "gelu_backward_device \t = \t 0.66048 ms\n",
      "attention_backward_device \t = \t 0.05632 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(10)=7.986176\n",
      "layernorm_forward_device \t = \t 0.349184 ms\n",
      "mlp_forward_device \t = \t 0.114688 ms\n",
      "attention_forward_device \t = \t 0.139264 ms\n",
      "residual_forward_device \t = \t 0.272384 ms\n",
      "gelu_forward_device \t = \t 0.19456 ms\n",
      "softmax_forward_device \t = \t 0.047104 ms\n",
      "cross_entropy_backward_device \t = \t 0.144384 ms\n",
      "mlp_backward_input_device \t = \t 0.749568 ms\n",
      "mlp_backward_weight_device \t = \t 1.175552 ms\n",
      "layernorm_backward_device \t = \t 3.019776 ms\n",
      "residual_backward_device \t = \t 0.059392 ms\n",
      "gelu_backward_device \t = \t 1.230848 ms\n",
      "attention_backward_device \t = \t 0.054272 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(11)=8.101888\n",
      "layernorm_forward_device \t = \t 0.357376 ms\n",
      "mlp_forward_device \t = \t 0.16896 ms\n",
      "attention_forward_device \t = \t 0.113664 ms\n",
      "residual_forward_device \t = \t 0.246784 ms\n",
      "gelu_forward_device \t = \t 0.193536 ms\n",
      "softmax_forward_device \t = \t 0.047104 ms\n",
      "cross_entropy_backward_device \t = \t 0.177152 ms\n",
      "mlp_backward_input_device \t = \t 0.642048 ms\n",
      "mlp_backward_weight_device \t = \t 0.602112 ms\n",
      "layernorm_backward_device \t = \t 3.091456 ms\n",
      "residual_backward_device \t = \t 0.068608 ms\n",
      "gelu_backward_device \t = \t 1.225728 ms\n",
      "attention_backward_device \t = \t 0.055296 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(12)=7.539712000000001\n",
      "layernorm_forward_device \t = \t 0.354304 ms\n",
      "mlp_forward_device \t = \t 0.172032 ms\n",
      "attention_forward_device \t = \t 0.113664 ms\n",
      "residual_forward_device \t = \t 0.248832 ms\n",
      "gelu_forward_device \t = \t 0.16896 ms\n",
      "softmax_forward_device \t = \t 0.047104 ms\n",
      "cross_entropy_backward_device \t = \t 0.185344 ms\n",
      "mlp_backward_input_device \t = \t 0.822272 ms\n",
      "mlp_backward_weight_device \t = \t 0.598016 ms\n",
      "layernorm_backward_device \t = \t 3.11296 ms\n",
      "residual_backward_device \t = \t 0.075776 ms\n",
      "gelu_backward_device \t = \t 1.25952 ms\n",
      "attention_backward_device \t = \t 0.054272 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(13)=7.763968000000001\n",
      "layernorm_forward_device \t = \t 0.349184 ms\n",
      "mlp_forward_device \t = \t 0.145408 ms\n",
      "attention_forward_device \t = \t 0.13824 ms\n",
      "residual_forward_device \t = \t 0.246784 ms\n",
      "gelu_forward_device \t = \t 0.165888 ms\n",
      "softmax_forward_device \t = \t 0.001024 ms\n",
      "cross_entropy_backward_device \t = \t 0.15872 ms\n",
      "mlp_backward_input_device \t = \t 0.913408 ms\n",
      "mlp_backward_weight_device \t = \t 0.562176 ms\n",
      "layernorm_backward_device \t = \t 3.122176 ms\n",
      "residual_backward_device \t = \t 0.049152 ms\n",
      "gelu_backward_device \t = \t 1.2032 ms\n",
      "attention_backward_device \t = \t 0.055296 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(14)=7.661568000000001\n",
      "layernorm_forward_device \t = \t 0.34816 ms\n",
      "mlp_forward_device \t = \t 0.150528 ms\n",
      "attention_forward_device \t = \t 0.140288 ms\n",
      "residual_forward_device \t = \t 0.249856 ms\n",
      "gelu_forward_device \t = \t 0.171008 ms\n",
      "softmax_forward_device \t = \t 0.001024 ms\n",
      "cross_entropy_backward_device \t = \t 0.04096 ms\n",
      "mlp_backward_input_device \t = \t 0.794624 ms\n",
      "mlp_backward_weight_device \t = \t 3.186688 ms\n",
      "layernorm_backward_device \t = \t 3.227648 ms\n",
      "residual_backward_device \t = \t 0.05632 ms\n",
      "gelu_backward_device \t = \t 1.253376 ms\n",
      "attention_backward_device \t = \t 0.054272 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(15)=10.225663999999998\n",
      "layernorm_forward_device \t = \t 0.33792 ms\n",
      "mlp_forward_device \t = \t 0.095232 ms\n",
      "attention_forward_device \t = \t 0.137216 ms\n",
      "residual_forward_device \t = \t 0.27648 ms\n",
      "gelu_forward_device \t = \t 0.164864 ms\n",
      "softmax_forward_device \t = \t 0.001024 ms\n",
      "cross_entropy_forward_device \t = \t 0.0 ms\n",
      "cross_entropy_backward_device \t = \t 0.0 ms\n",
      "mlp_backward_input_device \t = \t 0.4864 ms\n",
      "mlp_backward_weight_device \t = \t 2.985984 ms\n",
      "layernorm_backward_device \t = \t 3.1744 ms\n",
      "residual_backward_device \t = \t 0.049152 ms\n",
      "gelu_backward_device \t = \t 1.207296 ms\n",
      "attention_backward_device \t = \t 0.054272 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(16)=9.520128\n",
      "layernorm_forward_device \t = \t 0.344064 ms\n",
      "mlp_forward_device \t = \t 0.157696 ms\n",
      "attention_forward_device \t = \t 0.13824 ms\n",
      "residual_forward_device \t = \t 0.167936 ms\n",
      "gelu_forward_device \t = \t 0.161792 ms\n",
      "softmax_forward_device \t = \t 0.018432 ms\n",
      "cross_entropy_backward_device \t = \t 0.045056 ms\n",
      "mlp_backward_input_device \t = \t 1.078272 ms\n",
      "mlp_backward_weight_device \t = \t 2.968576 ms\n",
      "layernorm_backward_device \t = \t 3.21024 ms\n",
      "residual_backward_device \t = \t 0.055296 ms\n",
      "gelu_backward_device \t = \t 1.23392 ms\n",
      "attention_backward_device \t = \t 0.003072 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(17)=10.133504\n",
      "layernorm_forward_device \t = \t 0.351232 ms\n",
      "mlp_forward_device \t = \t 0.164864 ms\n",
      "attention_forward_device \t = \t 0.13824 ms\n",
      "residual_forward_device \t = \t 0.165888 ms\n",
      "gelu_forward_device \t = \t 0.164864 ms\n",
      "softmax_forward_device \t = \t 0.018432 ms\n",
      "cross_entropy_backward_device \t = \t 0.045056 ms\n",
      "mlp_backward_input_device \t = \t 0.605184 ms\n",
      "mlp_backward_weight_device \t = \t 2.952192 ms\n",
      "layernorm_backward_device \t = \t 3.187712 ms\n",
      "residual_backward_device \t = \t 0.067584 ms\n",
      "gelu_backward_device \t = \t 1.288192 ms\n",
      "attention_backward_device \t = \t 0.003072 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(18)=9.702399999999999\n",
      "layernorm_forward_device \t = \t 0.346112 ms\n",
      "mlp_forward_device \t = \t 0.082944 ms\n",
      "attention_forward_device \t = \t 0.137216 ms\n",
      "residual_forward_device \t = \t 0.167936 ms\n",
      "gelu_forward_device \t = \t 0.191488 ms\n",
      "softmax_forward_device \t = \t 0.018432 ms\n",
      "cross_entropy_backward_device \t = \t 0.131072 ms\n",
      "mlp_backward_input_device \t = \t 0.221184 ms\n",
      "mlp_backward_weight_device \t = \t 0.427008 ms\n",
      "layernorm_backward_device \t = \t 3.139584 ms\n",
      "residual_backward_device \t = \t 0.06144 ms\n",
      "gelu_backward_device \t = \t 1.27488 ms\n",
      "attention_backward_device \t = \t 0.062464 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(19)=6.811648000000001\n",
      "layernorm_forward_device \t = \t 0.35328 ms\n",
      "mlp_forward_device \t = \t 0.21504 ms\n",
      "attention_forward_device \t = \t 0.139264 ms\n",
      "residual_forward_device \t = \t 0.16384 ms\n",
      "gelu_forward_device \t = \t 0.19456 ms\n",
      "softmax_forward_device \t = \t 0.019456 ms\n",
      "cross_entropy_backward_device \t = \t 0.191488 ms\n",
      "mlp_backward_input_device \t = \t 1.10592 ms\n",
      "mlp_backward_weight_device \t = \t 0.436224 ms\n",
      "layernorm_backward_device \t = \t 1.554432 ms\n",
      "residual_backward_device \t = \t 0.077824 ms\n",
      "gelu_backward_device \t = \t 1.306624 ms\n",
      "attention_backward_device \t = \t 0.063488 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(20)=6.372352000000001\n",
      "layernorm_forward_device \t = \t 0.361472 ms\n",
      "mlp_forward_device \t = \t 0.203776 ms\n",
      "attention_forward_device \t = \t 0.141312 ms\n",
      "residual_forward_device \t = \t 0.273408 ms\n",
      "gelu_forward_device \t = \t 0.193536 ms\n",
      "softmax_forward_device \t = \t 0.03072 ms\n",
      "cross_entropy_backward_device \t = \t 0.19456 ms\n",
      "mlp_backward_input_device \t = \t 1.119232 ms\n",
      "mlp_backward_weight_device \t = \t 0.64 ms\n",
      "layernorm_backward_device \t = \t 2.702336 ms\n",
      "residual_backward_device \t = \t 0.059392 ms\n",
      "gelu_backward_device \t = \t 1.291264 ms\n",
      "attention_backward_device \t = \t 0.06144 ms\n",
      "adamw_kernel_device \t = \t 0.54784 ms\n",
      "wait_time(21)=7.820288000000001\n",
      "layernorm_forward_device \t = \t 0.352256 ms\n",
      "mlp_forward_device \t = \t 0.140288 ms\n",
      "attention_forward_device \t = \t 0.139264 ms\n",
      "residual_forward_device \t = \t 0.24576 ms\n",
      "gelu_forward_device \t = \t 0.191488 ms\n",
      "softmax_forward_device \t = \t 0.03072 ms\n",
      "cross_entropy_backward_device \t = \t 0.166912 ms\n",
      "mlp_backward_input_device \t = \t 0.687104 ms\n",
      "mlp_backward_weight_device \t = \t 1.393664 ms\n",
      "layernorm_backward_device \t = \t 2.9696 ms\n",
      "residual_backward_device \t = \t 0.055296 ms\n",
      "gelu_backward_device \t = \t 1.312768 ms\n",
      "attention_backward_device \t = \t 0.062464 ms\n",
      "embedding_backward_device \t = \t 0.0 ms\n",
      "adamw_kernel_device \t = \t 0.001024 ms\n",
      "wait_time(22)=7.748608000000001\n",
      "layernorm_forward_device \t = \t 0.35328 ms\n",
      "mlp_forward_device \t = \t 0.169984 ms\n",
      "attention_forward_device \t = \t 0.140288 ms\n",
      "residual_forward_device \t = \t 0.244736 ms\n",
      "gelu_forward_device \t = \t 0.188416 ms\n",
      "softmax_forward_device \t = \t 0.03072 ms\n",
      "cross_entropy_backward_device \t = \t 0.171008 ms\n",
      "mlp_backward_input_device \t = \t 0.86528 ms\n",
      "mlp_backward_weight_device \t = \t 1.197056 ms\n",
      "layernorm_backward_device \t = \t 3.033088 ms\n",
      "residual_backward_device \t = \t 0.05632 ms\n",
      "gelu_backward_device \t = \t 1.288192 ms\n",
      "attention_backward_device \t = \t 0.05632 ms\n",
      "adamw_kernel_device \t = \t 0.54784 ms\n",
      "wait_time(23)=8.342528\n",
      "layernorm_forward_device \t = \t 0.349184 ms\n",
      "mlp_forward_device \t = \t 0.162816 ms\n",
      "attention_forward_device \t = \t 0.165888 ms\n",
      "residual_forward_device \t = \t 0.244736 ms\n",
      "gelu_forward_device \t = \t 0.190464 ms\n",
      "softmax_forward_device \t = \t 0.018432 ms\n",
      "cross_entropy_backward_device \t = \t 0.173056 ms\n",
      "mlp_backward_input_device \t = \t 1.10592 ms\n",
      "mlp_backward_weight_device \t = \t 1.229824 ms\n",
      "layernorm_backward_device \t = \t 3.095552 ms\n",
      "residual_backward_device \t = \t 0.05632 ms\n",
      "gelu_backward_device \t = \t 0.658432 ms\n",
      "attention_backward_device \t = \t 0.055296 ms\n",
      "adamw_kernel_device \t = \t 0.548864 ms\n",
      "wait_time(24)=8.054784000000001\n",
      "layernorm_forward_device \t = \t 0.346112 ms\n",
      "mlp_forward_device \t = \t 0.120832 ms\n",
      "attention_forward_device \t = \t 0.16384 ms\n",
      "residual_forward_device \t = \t 0.246784 ms\n",
      "gelu_forward_device \t = \t 0.191488 ms\n",
      "softmax_forward_device \t = \t 0.018432 ms\n",
      "cross_entropy_backward_device \t = \t 0.147456 ms\n",
      "mlp_backward_input_device \t = \t 0.764928 ms\n",
      "mlp_backward_weight_device \t = \t 1.183744 ms\n",
      "layernorm_backward_device \t = \t 3.049472 ms\n",
      "residual_backward_device \t = \t 0.064512 ms\n",
      "gelu_backward_device \t = \t 1.236992 ms\n",
      "attention_backward_device \t = \t 0.05632 ms\n",
      "adamw_kernel_device \t = \t 0.549888 ms\n",
      "wait_time(25)=8.1408\n",
      "layernorm_forward_device \t = \t 0.346112 ms\n",
      "mlp_forward_device \t = \t 0.154624 ms\n",
      "attention_forward_device \t = \t 0.164864 ms\n",
      "residual_forward_device \t = \t 0.274432 ms\n",
      "gelu_forward_device \t = \t 0.193536 ms\n",
      "softmax_forward_device \t = \t 0.018432 ms\n",
      "cross_entropy_backward_device \t = \t 0.177152 ms\n",
      "mlp_backward_input_device \t = \t 0.713728 ms\n",
      "mlp_backward_weight_device \t = \t 0.564224 ms\n",
      "layernorm_backward_device \t = \t 3.101696 ms\n",
      "residual_backward_device \t = \t 0.070656 ms\n",
      "gelu_backward_device \t = \t 1.226752 ms\n",
      "attention_backward_device \t = \t 0.057344 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(26)=7.614464\n",
      "layernorm_forward_device \t = \t 0.354304 ms\n",
      "mlp_forward_device \t = \t 0.166912 ms\n",
      "attention_forward_device \t = \t 0.166912 ms\n",
      "residual_forward_device \t = \t 0.166912 ms\n",
      "gelu_forward_device \t = \t 0.193536 ms\n",
      "softmax_forward_device \t = \t 0.019456 ms\n",
      "cross_entropy_backward_device \t = \t 0.169984 ms\n",
      "mlp_backward_input_device \t = \t 0.661504 ms\n",
      "mlp_backward_weight_device \t = \t 0.576512 ms\n",
      "layernorm_backward_device \t = \t 3.100672 ms\n",
      "residual_backward_device \t = \t 0.069632 ms\n",
      "gelu_backward_device \t = \t 1.256448 ms\n",
      "attention_backward_device \t = \t 0.057344 ms\n",
      "adamw_kernel_device \t = \t 0.550912 ms\n",
      "wait_time(27)=7.5110399999999995\n"
     ]
    }
   ],
   "source": [
    "for sm_no, sm in enumerate(timer[0]):\n",
    "    wait_times = defaultdict(list)\n",
    "    for instr_no, instr in sm.items():\n",
    "        wait_times[instr[\"kernelName\"]].append(instr[\"bar_exit\"] - instr[\"bar_enter\"])\n",
    "    count = 0\n",
    "    for k, times in wait_times.items():\n",
    "        count += np.sum(times) / (1_000_000)\n",
    "        print(f\"{k} \\t = \\t {np.sum(times) / (1_000_000)} ms\")\n",
    "    print(f\"wait_time({sm_no})={count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "id": "91eb65b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding forward:embedding_forward_device \t = \t 0.31744 ms\n",
      "LayerNorm 1:layernorm_forward_device \t = \t 0.015957333333333334 ms\n",
      "QKV projection:mlp_forward_device \t = \t 0.557312 ms\n",
      "Attention:attention_forward_device \t = \t 0.4753066666666667 ms\n",
      "Attention projection:mlp_forward_device \t = \t 0.18747733333333333 ms\n",
      "Residual 2:residual_forward_device \t = \t 0.010496 ms\n",
      "LayerNorm 2:layernorm_forward_device \t = \t 0.015701333333333335 ms\n",
      "MLP FC:mlp_forward_device \t = \t 0.745472 ms\n",
      "GELU:gelu_forward_device \t = \t 0.022613333333333333 ms\n",
      "MLP projection:mlp_forward_device \t = \t 0.7466666666666666 ms\n",
      "Residual 3:residual_forward_device \t = \t 0.010410666666666665 ms\n",
      "Final LayerNorm:layernorm_forward_device \t = \t 0.016384 ms\n",
      "Logits:mlp_forward_device \t = \t 12.6464 ms\n",
      "Softmax:softmax_forward_device \t = \t 1.778688 ms\n",
      "Cross-entropy backward:cross_entropy_backward_device \t = \t 0.771072 ms\n",
      "Logits backward (input gradient):mlp_backward_input_device \t = \t 13.719552 ms\n",
      "Embedding weight gradient:mlp_backward_weight_device \t = \t 14.42816 ms\n",
      "Final LayerNorm backward:layernorm_backward_device \t = \t 0.029696 ms\n",
      "Residual backward (res_3):residual_backward_device \t = \t 0.013738666666666666 ms\n",
      "MLP projection backward input:mlp_backward_input_device \t = \t 0.9170773333333334 ms\n",
      "MLP projection backward weight:mlp_backward_weight_device \t = \t 0.8453973333333333 ms\n",
      "GELU backward:gelu_backward_device \t = \t 0.08618666666666668 ms\n",
      "MLP FC backward input:mlp_backward_input_device \t = \t 0.9255253333333334 ms\n",
      "MLP FC backward weight:mlp_backward_weight_device \t = \t 0.9045333333333334 ms\n",
      "LayerNorm 2 backward:layernorm_backward_device \t = \t 0.03080533333333333 ms\n",
      "Residual backward (res_2):residual_backward_device \t = \t 0.02013866666666667 ms\n",
      "Attention projection backward input:mlp_backward_input_device \t = \t 0.219648 ms\n",
      "Attention projection backward weight:mlp_backward_weight_device \t = \t 0.22459733333333334 ms\n",
      "Attention backward:attention_backward_device \t = \t 1.499648 ms\n",
      "QKV backward input:mlp_backward_input_device \t = \t 0.6911146666666667 ms\n",
      "QKV backward weight:mlp_backward_weight_device \t = \t 0.6350506666666667 ms\n",
      "LayerNorm 1 backward:layernorm_backward_device \t = \t 0.02653866666666667 ms\n",
      "AdamW update:adamw_kernel_device \t = \t 12.163072 ms\n",
      "total_time(0)=173.799424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sm_no, sm in enumerate(timer[0]):\n",
    "    kern_times = defaultdict(list)\n",
    "    for instr_no, instr in sm.items():\n",
    "        kern_times[instr[\"name\"]+\":\"+instr[\"kernelName\"]].append(instr[\"instr_end\"] - instr[\"bar_exit\"])\n",
    "    count = 0\n",
    "    for k, times in kern_times.items():\n",
    "        count += np.sum(times) / (1_000_000)\n",
    "        print(f\"{k} \\t = \\t {np.mean(times) / (1_000_000)} ms\")\n",
    "    print(f\"total_time({sm_no})={count}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "id": "b31dba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding forward:embedding_forward_device \t = \t 0.31744 ms\n",
      "LayerNorm 1:layernorm_forward_device \t = \t 0.015957333333333334 ms\n",
      "QKV projection:mlp_forward_device \t = \t 0.557312 ms\n",
      "Attention:attention_forward_device \t = \t 0.4753066666666667 ms\n",
      "Attention projection:mlp_forward_device \t = \t 0.18747733333333333 ms\n",
      "Residual 2:residual_forward_device \t = \t 0.010496 ms\n",
      "LayerNorm 2:layernorm_forward_device \t = \t 0.015701333333333335 ms\n",
      "MLP FC:mlp_forward_device \t = \t 0.745472 ms\n",
      "GELU:gelu_forward_device \t = \t 0.022613333333333333 ms\n",
      "MLP projection:mlp_forward_device \t = \t 0.7466666666666666 ms\n",
      "Residual 3:residual_forward_device \t = \t 0.010410666666666665 ms\n",
      "Final LayerNorm:layernorm_forward_device \t = \t 0.016384 ms\n",
      "Logits:mlp_forward_device \t = \t 12.6464 ms\n",
      "Softmax:softmax_forward_device \t = \t 1.778688 ms\n",
      "Cross-entropy backward:cross_entropy_backward_device \t = \t 0.771072 ms\n",
      "Logits backward (input gradient):mlp_backward_input_device \t = \t 13.719552 ms\n",
      "Embedding weight gradient:mlp_backward_weight_device \t = \t 14.42816 ms\n",
      "Final LayerNorm backward:layernorm_backward_device \t = \t 0.029696 ms\n",
      "Residual backward (res_3):residual_backward_device \t = \t 0.013738666666666666 ms\n",
      "MLP projection backward input:mlp_backward_input_device \t = \t 0.9170773333333334 ms\n",
      "MLP projection backward weight:mlp_backward_weight_device \t = \t 0.8453973333333333 ms\n",
      "GELU backward:gelu_backward_device \t = \t 0.08618666666666668 ms\n",
      "MLP FC backward input:mlp_backward_input_device \t = \t 0.9255253333333334 ms\n",
      "MLP FC backward weight:mlp_backward_weight_device \t = \t 0.9045333333333334 ms\n",
      "LayerNorm 2 backward:layernorm_backward_device \t = \t 0.03080533333333333 ms\n",
      "Residual backward (res_2):residual_backward_device \t = \t 0.02013866666666667 ms\n",
      "Attention projection backward input:mlp_backward_input_device \t = \t 0.219648 ms\n",
      "Attention projection backward weight:mlp_backward_weight_device \t = \t 0.22459733333333334 ms\n",
      "Attention backward:attention_backward_device \t = \t 1.499648 ms\n",
      "QKV backward input:mlp_backward_input_device \t = \t 0.6911146666666667 ms\n",
      "QKV backward weight:mlp_backward_weight_device \t = \t 0.6350506666666667 ms\n",
      "LayerNorm 1 backward:layernorm_backward_device \t = \t 0.02653866666666667 ms\n",
      "AdamW update:adamw_kernel_device \t = \t 12.163072 ms\n",
      "total_time(0)=173.799424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sm_no, sm in enumerate(timer[0]):\n",
    "    kern_times = defaultdict(list)\n",
    "    for instr_no, instr in sm.items():\n",
    "        kern_times[instr[\"name\"]+\":\"+instr[\"kernelName\"]].append(instr[\"instr_end\"] - instr[\"bar_exit\"])\n",
    "    count = 0\n",
    "    for k, times in kern_times.items():\n",
    "        count += np.sum(times) / (1_000_000)\n",
    "        print(f\"{k} \\t = \\t {np.mean(times) / (1_000_000)} ms\")\n",
    "    print(f\"total_time({sm_no})={count}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "id": "dda81117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sm_no, sm in enumerate(timer[0]):\n",
    "for instr_no, instr in timer[0][6].items():\n",
    "    if instr[\"spin_wait\"] > 0:\n",
    "        print(timer[0][6][instr[\"instr\"]-1])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "id": "bc6d178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 0, 'sm': 0, 'instr': 126, 'bar_enter': 63802368, 'bar_exit': 63803392, 'instr_end': 78231552, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 1350, 'end_b_x': 1125, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 130, 'bar_enter': 79376384, 'bar_exit': 79376384, 'instr_end': 80262144, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 133, 'bar_enter': 81418240, 'bar_exit': 81432576, 'instr_end': 82380800, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 137, 'bar_enter': 82655232, 'bar_exit': 82691072, 'instr_end': 82921472, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 140, 'bar_enter': 85268480, 'bar_exit': 85269504, 'instr_end': 85934080, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 144, 'bar_enter': 87187456, 'bar_exit': 87222272, 'instr_end': 88097792, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 147, 'bar_enter': 89265152, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 151, 'bar_enter': 90528768, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 154, 'bar_enter': 93041664, 'bar_exit': 93042688, 'instr_end': 93681664, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 158, 'bar_enter': 94921728, 'bar_exit': 94921728, 'instr_end': 95773696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 161, 'bar_enter': 96882688, 'bar_exit': 96900096, 'instr_end': 97809408, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 165, 'bar_enter': 98072576, 'bar_exit': 98107392, 'instr_end': 98328576, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 168, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101239808, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 172, 'bar_enter': 102441984, 'bar_exit': 102473728, 'instr_end': 103314432, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 175, 'bar_enter': 104444928, 'bar_exit': 104445952, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 179, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 182, 'bar_enter': 108161024, 'bar_exit': 108162048, 'instr_end': 108801024, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 186, 'bar_enter': 110036992, 'bar_exit': 110040064, 'instr_end': 110891008, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 189, 'bar_enter': 112006144, 'bar_exit': 112023552, 'instr_end': 112932864, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 193, 'bar_enter': 113198080, 'bar_exit': 113233920, 'instr_end': 113454080, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 196, 'bar_enter': 115729408, 'bar_exit': 115732480, 'instr_end': 116373504, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 200, 'bar_enter': 117564416, 'bar_exit': 117604352, 'instr_end': 118450176, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 203, 'bar_enter': 119577600, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 207, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 210, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123956224, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 214, 'bar_enter': 125177856, 'bar_exit': 125179904, 'instr_end': 126035968, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 217, 'bar_enter': 127153152, 'bar_exit': 127169536, 'instr_end': 128083968, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 221, 'bar_enter': 128348160, 'bar_exit': 128384000, 'instr_end': 128604160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 224, 'bar_enter': 130885632, 'bar_exit': 130885632, 'instr_end': 131520512, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 228, 'bar_enter': 132702208, 'bar_exit': 132737024, 'instr_end': 133565440, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 231, 'bar_enter': 134665216, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 235, 'bar_enter': 135873536, 'bar_exit': 135873536, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 238, 'bar_enter': 138331136, 'bar_exit': 138334208, 'instr_end': 138961920, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 242, 'bar_enter': 140171264, 'bar_exit': 140174336, 'instr_end': 141010944, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 245, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 143007744, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 249, 'bar_enter': 143266816, 'bar_exit': 143301632, 'instr_end': 143518720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 252, 'bar_enter': 145759232, 'bar_exit': 145759232, 'instr_end': 146382848, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 256, 'bar_enter': 147556352, 'bar_exit': 147590144, 'instr_end': 148411392, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 259, 'bar_enter': 149508096, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 263, 'bar_enter': 150704128, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 266, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153776128, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 270, 'bar_enter': 154968064, 'bar_exit': 154972160, 'instr_end': 155802624, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 273, 'bar_enter': 156900352, 'bar_exit': 156903424, 'instr_end': 157791232, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 277, 'bar_enter': 158049280, 'bar_exit': 158084096, 'instr_end': 158300160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 280, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161157120, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 284, 'bar_enter': 162318336, 'bar_exit': 162356224, 'instr_end': 163177472, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 287, 'bar_enter': 164274176, 'bar_exit': 164284416, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 291, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 294, 'bar_enter': 167912448, 'bar_exit': 167914496, 'instr_end': 168536064, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 126, 'bar_enter': 63764480, 'bar_exit': 63803392, 'instr_end': 78247936, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 1126, 'end_b_x': 901, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 130, 'bar_enter': 79376384, 'bar_exit': 79376384, 'instr_end': 80258048, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 133, 'bar_enter': 81417216, 'bar_exit': 81432576, 'instr_end': 82378752, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 137, 'bar_enter': 82655232, 'bar_exit': 82691072, 'instr_end': 82921472, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 140, 'bar_enter': 85167104, 'bar_exit': 85269504, 'instr_end': 85918720, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 144, 'bar_enter': 87185408, 'bar_exit': 87222272, 'instr_end': 88094720, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 147, 'bar_enter': 89274368, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 151, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 154, 'bar_enter': 93039616, 'bar_exit': 93041664, 'instr_end': 93676544, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 158, 'bar_enter': 94920704, 'bar_exit': 94921728, 'instr_end': 95769600, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 161, 'bar_enter': 96884736, 'bar_exit': 96900096, 'instr_end': 97809408, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 165, 'bar_enter': 98072576, 'bar_exit': 98107392, 'instr_end': 98328576, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 168, 'bar_enter': 100500480, 'bar_exit': 100600832, 'instr_end': 101224448, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 172, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103311360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 175, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 179, 'bar_enter': 105655296, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 182, 'bar_enter': 108162048, 'bar_exit': 108162048, 'instr_end': 108796928, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 186, 'bar_enter': 110035968, 'bar_exit': 110040064, 'instr_end': 110887936, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 189, 'bar_enter': 112008192, 'bar_exit': 112023552, 'instr_end': 112932864, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 193, 'bar_enter': 113197056, 'bar_exit': 113233920, 'instr_end': 113454080, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 196, 'bar_enter': 115630080, 'bar_exit': 115731456, 'instr_end': 116357120, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 200, 'bar_enter': 117565440, 'bar_exit': 117604352, 'instr_end': 118447104, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 203, 'bar_enter': 119574528, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 207, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 210, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123950080, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 214, 'bar_enter': 125178880, 'bar_exit': 125179904, 'instr_end': 126030848, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 217, 'bar_enter': 127168512, 'bar_exit': 127169536, 'instr_end': 128081920, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 221, 'bar_enter': 128347136, 'bar_exit': 128384000, 'instr_end': 128604160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 224, 'bar_enter': 130785280, 'bar_exit': 130885632, 'instr_end': 131507200, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 228, 'bar_enter': 132702208, 'bar_exit': 132737024, 'instr_end': 133561344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 231, 'bar_enter': 134668288, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 235, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 238, 'bar_enter': 138332160, 'bar_exit': 138334208, 'instr_end': 138957824, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 242, 'bar_enter': 140171264, 'bar_exit': 140174336, 'instr_end': 141006848, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 245, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 143007744, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 249, 'bar_enter': 143267840, 'bar_exit': 143301632, 'instr_end': 143518720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 252, 'bar_enter': 145658880, 'bar_exit': 145759232, 'instr_end': 146369536, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 256, 'bar_enter': 147556352, 'bar_exit': 147590144, 'instr_end': 148407296, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 259, 'bar_enter': 149510144, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 263, 'bar_enter': 150704128, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 266, 'bar_enter': 153150464, 'bar_exit': 153152512, 'instr_end': 153771008, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 270, 'bar_enter': 154969088, 'bar_exit': 154972160, 'instr_end': 155799552, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 273, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157791232, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 277, 'bar_enter': 158049280, 'bar_exit': 158084096, 'instr_end': 158300160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 280, 'bar_enter': 160435200, 'bar_exit': 160532480, 'instr_end': 161141760, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 284, 'bar_enter': 162318336, 'bar_exit': 162356224, 'instr_end': 163174400, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 287, 'bar_enter': 164275200, 'bar_exit': 164284416, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 291, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 294, 'bar_enter': 167913472, 'bar_exit': 167914496, 'instr_end': 168532992, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 126, 'bar_enter': 63764480, 'bar_exit': 63803392, 'instr_end': 78266368, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 902, 'end_b_x': 677, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 130, 'bar_enter': 79375360, 'bar_exit': 79376384, 'instr_end': 80262144, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 133, 'bar_enter': 81420288, 'bar_exit': 81432576, 'instr_end': 82378752, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 137, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82921472, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 140, 'bar_enter': 85167104, 'bar_exit': 85269504, 'instr_end': 85920768, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 144, 'bar_enter': 87185408, 'bar_exit': 87222272, 'instr_end': 88096768, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 147, 'bar_enter': 89265152, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 151, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 154, 'bar_enter': 93039616, 'bar_exit': 93041664, 'instr_end': 93679616, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 158, 'bar_enter': 94918656, 'bar_exit': 94921728, 'instr_end': 95773696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 161, 'bar_enter': 96897024, 'bar_exit': 96900096, 'instr_end': 97810432, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 165, 'bar_enter': 98108416, 'bar_exit': 98108416, 'instr_end': 98328576, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 168, 'bar_enter': 100500480, 'bar_exit': 100600832, 'instr_end': 101228544, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 172, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103314432, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 175, 'bar_enter': 104436736, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 179, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 182, 'bar_enter': 108162048, 'bar_exit': 108162048, 'instr_end': 108798976, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 186, 'bar_enter': 110036992, 'bar_exit': 110040064, 'instr_end': 110891008, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 189, 'bar_enter': 112021504, 'bar_exit': 112023552, 'instr_end': 112933888, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 193, 'bar_enter': 113232896, 'bar_exit': 113233920, 'instr_end': 113455104, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 196, 'bar_enter': 115630080, 'bar_exit': 115731456, 'instr_end': 116362240, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 200, 'bar_enter': 117568512, 'bar_exit': 117604352, 'instr_end': 118450176, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 203, 'bar_enter': 119577600, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 207, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 210, 'bar_enter': 123312128, 'bar_exit': 123313152, 'instr_end': 123954176, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 214, 'bar_enter': 125178880, 'bar_exit': 125179904, 'instr_end': 126034944, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 217, 'bar_enter': 127152128, 'bar_exit': 127169536, 'instr_end': 128082944, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 221, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128604160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 224, 'bar_enter': 130785280, 'bar_exit': 130885632, 'instr_end': 131510272, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 228, 'bar_enter': 132703232, 'bar_exit': 132737024, 'instr_end': 133564416, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 231, 'bar_enter': 134667264, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 235, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 238, 'bar_enter': 138331136, 'bar_exit': 138334208, 'instr_end': 138961920, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 242, 'bar_enter': 140171264, 'bar_exit': 140174336, 'instr_end': 141011968, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 245, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 143007744, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 249, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143518720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 252, 'bar_enter': 145660928, 'bar_exit': 145759232, 'instr_end': 146371584, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 256, 'bar_enter': 147556352, 'bar_exit': 147590144, 'instr_end': 148411392, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 259, 'bar_enter': 149523456, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 263, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 266, 'bar_enter': 153150464, 'bar_exit': 153152512, 'instr_end': 153774080, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 270, 'bar_enter': 154970112, 'bar_exit': 154972160, 'instr_end': 155801600, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 273, 'bar_enter': 156892160, 'bar_exit': 156903424, 'instr_end': 157791232, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 277, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158300160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 280, 'bar_enter': 160435200, 'bar_exit': 160532480, 'instr_end': 161145856, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 284, 'bar_enter': 162318336, 'bar_exit': 162356224, 'instr_end': 163176448, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 287, 'bar_enter': 164283392, 'bar_exit': 164283392, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 291, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 294, 'bar_enter': 167913472, 'bar_exit': 167913472, 'instr_end': 168536064, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 126, 'bar_enter': 63764480, 'bar_exit': 63803392, 'instr_end': 78269440, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 678, 'end_b_x': 453, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 130, 'bar_enter': 79374336, 'bar_exit': 79376384, 'instr_end': 80260096, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 133, 'bar_enter': 81433600, 'bar_exit': 81433600, 'instr_end': 82379776, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 137, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 140, 'bar_enter': 85166080, 'bar_exit': 85269504, 'instr_end': 85919744, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 144, 'bar_enter': 87187456, 'bar_exit': 87222272, 'instr_end': 88095744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 147, 'bar_enter': 89264128, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 151, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 154, 'bar_enter': 93039616, 'bar_exit': 93041664, 'instr_end': 93681664, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 158, 'bar_enter': 94917632, 'bar_exit': 94921728, 'instr_end': 95769600, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 161, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97810432, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 165, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 168, 'bar_enter': 100500480, 'bar_exit': 100600832, 'instr_end': 101225472, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 172, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103313408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 175, 'bar_enter': 104436736, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 179, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 182, 'bar_enter': 108162048, 'bar_exit': 108162048, 'instr_end': 108798976, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 186, 'bar_enter': 110036992, 'bar_exit': 110040064, 'instr_end': 110888960, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 189, 'bar_enter': 112006144, 'bar_exit': 112023552, 'instr_end': 112933888, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 193, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 196, 'bar_enter': 115631104, 'bar_exit': 115732480, 'instr_end': 116357120, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 200, 'bar_enter': 117567488, 'bar_exit': 117604352, 'instr_end': 118449152, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 203, 'bar_enter': 119588864, 'bar_exit': 119589888, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 207, 'bar_enter': 120803328, 'bar_exit': 120804352, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 210, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123953152, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 214, 'bar_enter': 125179904, 'bar_exit': 125179904, 'instr_end': 126031872, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 217, 'bar_enter': 127150080, 'bar_exit': 127169536, 'instr_end': 128082944, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 221, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 224, 'bar_enter': 130785280, 'bar_exit': 130885632, 'instr_end': 131506176, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 228, 'bar_enter': 132703232, 'bar_exit': 132737024, 'instr_end': 133563392, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 231, 'bar_enter': 134682624, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 235, 'bar_enter': 135874560, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 238, 'bar_enter': 138331136, 'bar_exit': 138334208, 'instr_end': 138962944, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 242, 'bar_enter': 140172288, 'bar_exit': 140174336, 'instr_end': 141008896, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 245, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 143007744, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 249, 'bar_enter': 143298560, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 252, 'bar_enter': 145659904, 'bar_exit': 145759232, 'instr_end': 146368512, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 256, 'bar_enter': 147556352, 'bar_exit': 147590144, 'instr_end': 148408320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 259, 'bar_enter': 149511168, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 263, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 266, 'bar_enter': 153150464, 'bar_exit': 153152512, 'instr_end': 153775104, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 270, 'bar_enter': 154970112, 'bar_exit': 154972160, 'instr_end': 155798528, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 273, 'bar_enter': 156890112, 'bar_exit': 156903424, 'instr_end': 157791232, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 277, 'bar_enter': 158084096, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 280, 'bar_enter': 160436224, 'bar_exit': 160532480, 'instr_end': 161141760, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 284, 'bar_enter': 162319360, 'bar_exit': 162356224, 'instr_end': 163175424, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 287, 'bar_enter': 164274176, 'bar_exit': 164284416, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 291, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 294, 'bar_enter': 167913472, 'bar_exit': 167914496, 'instr_end': 168536064, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 125, 'bar_enter': 63788032, 'bar_exit': 63803392, 'instr_end': 78263296, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 454, 'end_b_x': 229, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 129, 'bar_enter': 79375360, 'bar_exit': 79376384, 'instr_end': 80250880, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 132, 'bar_enter': 81418240, 'bar_exit': 81432576, 'instr_end': 82378752, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 139, 'bar_enter': 85166080, 'bar_exit': 85269504, 'instr_end': 85925888, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 143, 'bar_enter': 87185408, 'bar_exit': 87222272, 'instr_end': 88093696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 146, 'bar_enter': 89265152, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 153, 'bar_enter': 93038592, 'bar_exit': 93041664, 'instr_end': 93680640, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 157, 'bar_enter': 94917632, 'bar_exit': 94921728, 'instr_end': 95762432, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 160, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97809408, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 167, 'bar_enter': 100500480, 'bar_exit': 100600832, 'instr_end': 101230592, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 171, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103312384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 174, 'bar_enter': 104443904, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 181, 'bar_enter': 108160000, 'bar_exit': 108162048, 'instr_end': 108798976, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 185, 'bar_enter': 110035968, 'bar_exit': 110040064, 'instr_end': 110880768, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 188, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112932864, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113463296, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 195, 'bar_enter': 115631104, 'bar_exit': 115731456, 'instr_end': 116367360, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 199, 'bar_enter': 117568512, 'bar_exit': 117604352, 'instr_end': 118448128, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 202, 'bar_enter': 119576576, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 209, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123953152, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 213, 'bar_enter': 125179904, 'bar_exit': 125179904, 'instr_end': 126024704, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 216, 'bar_enter': 127151104, 'bar_exit': 127169536, 'instr_end': 128082944, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 220, 'bar_enter': 128382976, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 223, 'bar_enter': 130785280, 'bar_exit': 130885632, 'instr_end': 131515392, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 227, 'bar_enter': 132703232, 'bar_exit': 132737024, 'instr_end': 133562368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 230, 'bar_enter': 134664192, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 237, 'bar_enter': 138333184, 'bar_exit': 138334208, 'instr_end': 138963968, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 241, 'bar_enter': 140172288, 'bar_exit': 140174336, 'instr_end': 141000704, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 244, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 143006720, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 251, 'bar_enter': 145659904, 'bar_exit': 145759232, 'instr_end': 146377728, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 255, 'bar_enter': 147556352, 'bar_exit': 147590144, 'instr_end': 148408320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 258, 'bar_enter': 149508096, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153774080, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 269, 'bar_enter': 154969088, 'bar_exit': 154972160, 'instr_end': 155791360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 272, 'bar_enter': 156902400, 'bar_exit': 156903424, 'instr_end': 157791232, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 279, 'bar_enter': 160435200, 'bar_exit': 160532480, 'instr_end': 161149952, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 283, 'bar_enter': 162318336, 'bar_exit': 162356224, 'instr_end': 163173376, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 286, 'bar_enter': 164276224, 'bar_exit': 164283392, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167914496, 'instr_end': 168536064, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 125, 'bar_enter': 63788032, 'bar_exit': 63803392, 'instr_end': 78260224, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 230, 'end_b_x': 5, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 129, 'bar_enter': 79375360, 'bar_exit': 79376384, 'instr_end': 80253952, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 132, 'bar_enter': 81418240, 'bar_exit': 81432576, 'instr_end': 82378752, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85917696, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 143, 'bar_enter': 87185408, 'bar_exit': 87222272, 'instr_end': 88097792, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 146, 'bar_enter': 89275392, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90750976, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93681664, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 157, 'bar_enter': 94920704, 'bar_exit': 94921728, 'instr_end': 95761408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 160, 'bar_enter': 96884736, 'bar_exit': 96900096, 'instr_end': 97809408, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98336768, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 167, 'bar_enter': 100600832, 'bar_exit': 100600832, 'instr_end': 101223424, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 171, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103314432, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 178, 'bar_enter': 105654272, 'bar_exit': 105657344, 'instr_end': 105876480, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 181, 'bar_enter': 108160000, 'bar_exit': 108162048, 'instr_end': 108800000, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 185, 'bar_enter': 110034944, 'bar_exit': 110040064, 'instr_end': 110879744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 188, 'bar_enter': 112006144, 'bar_exit': 112023552, 'instr_end': 112934912, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113463296, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115731456, 'instr_end': 116357120, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 199, 'bar_enter': 117566464, 'bar_exit': 117604352, 'instr_end': 118449152, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 202, 'bar_enter': 119574528, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121024512, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 209, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123953152, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 213, 'bar_enter': 125180928, 'bar_exit': 125180928, 'instr_end': 126023680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 216, 'bar_enter': 127168512, 'bar_exit': 127169536, 'instr_end': 128083968, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131506176, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 227, 'bar_enter': 132702208, 'bar_exit': 132737024, 'instr_end': 133564416, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 230, 'bar_enter': 134666240, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136090624, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138334208, 'instr_end': 138962944, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 241, 'bar_enter': 140174336, 'bar_exit': 140174336, 'instr_end': 140999680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 244, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 143006720, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 248, 'bar_enter': 143300608, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 251, 'bar_enter': 145758208, 'bar_exit': 145759232, 'instr_end': 146367488, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 255, 'bar_enter': 147555328, 'bar_exit': 147590144, 'instr_end': 148410368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 258, 'bar_enter': 149511168, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 262, 'bar_enter': 150706176, 'bar_exit': 150706176, 'instr_end': 150920192, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153774080, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 269, 'bar_enter': 154970112, 'bar_exit': 154972160, 'instr_end': 155792384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157792256, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161140736, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 283, 'bar_enter': 162319360, 'bar_exit': 162356224, 'instr_end': 163177472, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 286, 'bar_enter': 164276224, 'bar_exit': 164284416, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165679104, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167914496, 'instr_end': 168538112, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 125, 'bar_enter': 63788032, 'bar_exit': 63803392, 'instr_end': 78222336, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 6, 'end_b_x': 1352, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 129, 'bar_enter': 79375360, 'bar_exit': 79376384, 'instr_end': 80249856, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 132, 'bar_enter': 81419264, 'bar_exit': 81432576, 'instr_end': 82378752, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 86198272, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 143, 'bar_enter': 87185408, 'bar_exit': 87222272, 'instr_end': 88093696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 146, 'bar_enter': 89266176, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 150, 'bar_enter': 90528768, 'bar_exit': 90528768, 'instr_end': 90750976, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93684736, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 157, 'bar_enter': 94919680, 'bar_exit': 94921728, 'instr_end': 95760384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 160, 'bar_enter': 96900096, 'bar_exit': 96900096, 'instr_end': 97810432, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 164, 'bar_enter': 98107392, 'bar_exit': 98108416, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101489664, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 171, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103311360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105877504, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 181, 'bar_enter': 108160000, 'bar_exit': 108162048, 'instr_end': 108803072, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 185, 'bar_enter': 110034944, 'bar_exit': 110040064, 'instr_end': 110877696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 188, 'bar_enter': 112024576, 'bar_exit': 112024576, 'instr_end': 112933888, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 192, 'bar_enter': 113233920, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 195, 'bar_enter': 115731456, 'bar_exit': 115732480, 'instr_end': 116612096, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 199, 'bar_enter': 117567488, 'bar_exit': 117604352, 'instr_end': 118446080, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 202, 'bar_enter': 119574528, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 206, 'bar_enter': 120803328, 'bar_exit': 120804352, 'instr_end': 121023488, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 209, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123958272, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 213, 'bar_enter': 125180928, 'bar_exit': 125180928, 'instr_end': 126021632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 216, 'bar_enter': 127151104, 'bar_exit': 127169536, 'instr_end': 128081920, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 220, 'bar_enter': 128381952, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131765248, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 227, 'bar_enter': 132702208, 'bar_exit': 132737024, 'instr_end': 133560320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 230, 'bar_enter': 134669312, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135873536, 'instr_end': 136090624, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138334208, 'instr_end': 138964992, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 241, 'bar_enter': 140174336, 'bar_exit': 140174336, 'instr_end': 140999680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 244, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 143006720, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 248, 'bar_enter': 143300608, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 251, 'bar_enter': 145757184, 'bar_exit': 145759232, 'instr_end': 146626560, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 255, 'bar_enter': 147555328, 'bar_exit': 147590144, 'instr_end': 148406272, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 258, 'bar_enter': 149526528, 'bar_exit': 149526528, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 262, 'bar_enter': 150706176, 'bar_exit': 150706176, 'instr_end': 150920192, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153779200, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 269, 'bar_enter': 154970112, 'bar_exit': 154972160, 'instr_end': 155790336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 272, 'bar_enter': 156891136, 'bar_exit': 156903424, 'instr_end': 157791232, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158085120, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161392640, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 283, 'bar_enter': 162318336, 'bar_exit': 162356224, 'instr_end': 163172352, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 286, 'bar_enter': 164283392, 'bar_exit': 164284416, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165679104, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 293, 'bar_enter': 167911424, 'bar_exit': 167914496, 'instr_end': 168540160, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 125, 'bar_enter': 63789056, 'bar_exit': 63803392, 'instr_end': 78263296, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 1353, 'end_b_x': 1128, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 129, 'bar_enter': 79340544, 'bar_exit': 79376384, 'instr_end': 80249856, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 132, 'bar_enter': 81429504, 'bar_exit': 81432576, 'instr_end': 82368512, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85987328, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 143, 'bar_enter': 87185408, 'bar_exit': 87222272, 'instr_end': 88094720, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 146, 'bar_enter': 89263104, 'bar_exit': 89277440, 'instr_end': 90211328, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90750976, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93691904, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 157, 'bar_enter': 94885888, 'bar_exit': 94921728, 'instr_end': 95761408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 160, 'bar_enter': 96885760, 'bar_exit': 96900096, 'instr_end': 97799168, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 164, 'bar_enter': 98107392, 'bar_exit': 98108416, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 167, 'bar_enter': 100600832, 'bar_exit': 100600832, 'instr_end': 101293056, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 171, 'bar_enter': 102440960, 'bar_exit': 102473728, 'instr_end': 103312384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 174, 'bar_enter': 104436736, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105876480, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 181, 'bar_enter': 108160000, 'bar_exit': 108162048, 'instr_end': 108811264, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 185, 'bar_enter': 110001152, 'bar_exit': 110040064, 'instr_end': 110879744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 188, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112921600, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113463296, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115732480, 'instr_end': 116424704, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 199, 'bar_enter': 117567488, 'bar_exit': 117604352, 'instr_end': 118448128, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 202, 'bar_enter': 119588864, 'bar_exit': 119589888, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121023488, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 209, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123965440, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 213, 'bar_enter': 125146112, 'bar_exit': 125179904, 'instr_end': 126023680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 216, 'bar_enter': 127150080, 'bar_exit': 127169536, 'instr_end': 128071680, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131571712, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 227, 'bar_enter': 132702208, 'bar_exit': 132737024, 'instr_end': 133562368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 230, 'bar_enter': 134682624, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136090624, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138334208, 'instr_end': 138973184, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 241, 'bar_enter': 140141568, 'bar_exit': 140174336, 'instr_end': 141001728, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 244, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 142996480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 248, 'bar_enter': 143298560, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 251, 'bar_enter': 145758208, 'bar_exit': 145759232, 'instr_end': 146428928, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 255, 'bar_enter': 147555328, 'bar_exit': 147590144, 'instr_end': 148409344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 258, 'bar_enter': 149510144, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150920192, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153786368, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 269, 'bar_enter': 154937344, 'bar_exit': 154972160, 'instr_end': 155790336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157782016, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158085120, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161207296, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 283, 'bar_enter': 162320384, 'bar_exit': 162356224, 'instr_end': 163175424, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 286, 'bar_enter': 164274176, 'bar_exit': 164283392, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165679104, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 293, 'bar_enter': 167911424, 'bar_exit': 167913472, 'instr_end': 168547328, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 125, 'bar_enter': 63757312, 'bar_exit': 63803392, 'instr_end': 78268416, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 1129, 'end_b_x': 903, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 129, 'bar_enter': 79340544, 'bar_exit': 79376384, 'instr_end': 80248832, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 132, 'bar_enter': 81418240, 'bar_exit': 81432576, 'instr_end': 82368512, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 136, 'bar_enter': 82692096, 'bar_exit': 82692096, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85268480, 'instr_end': 85941248, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 143, 'bar_enter': 87185408, 'bar_exit': 87222272, 'instr_end': 88093696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 146, 'bar_enter': 89127936, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90750976, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 153, 'bar_enter': 93039616, 'bar_exit': 93041664, 'instr_end': 93682688, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 157, 'bar_enter': 94888960, 'bar_exit': 94921728, 'instr_end': 95760384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 160, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97799168, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101246976, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 171, 'bar_enter': 102440960, 'bar_exit': 102473728, 'instr_end': 103310336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 174, 'bar_enter': 104308736, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105876480, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 181, 'bar_enter': 108160000, 'bar_exit': 108162048, 'instr_end': 108807168, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 185, 'bar_enter': 110000128, 'bar_exit': 110040064, 'instr_end': 110877696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 188, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 195, 'bar_enter': 115731456, 'bar_exit': 115732480, 'instr_end': 116381696, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 199, 'bar_enter': 117566464, 'bar_exit': 117605376, 'instr_end': 118447104, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 202, 'bar_enter': 119443456, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121024512, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 209, 'bar_enter': 123313152, 'bar_exit': 123313152, 'instr_end': 123960320, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 213, 'bar_enter': 125146112, 'bar_exit': 125179904, 'instr_end': 126022656, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 216, 'bar_enter': 127154176, 'bar_exit': 127169536, 'instr_end': 128071680, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 220, 'bar_enter': 128382976, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131529728, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 227, 'bar_enter': 132702208, 'bar_exit': 132737024, 'instr_end': 133560320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 230, 'bar_enter': 134535168, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 234, 'bar_enter': 135874560, 'bar_exit': 135874560, 'instr_end': 136090624, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138335232, 'instr_end': 138971136, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 241, 'bar_enter': 140140544, 'bar_exit': 140174336, 'instr_end': 140998656, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 244, 'bar_enter': 142105600, 'bar_exit': 142111744, 'instr_end': 142996480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 251, 'bar_enter': 145757184, 'bar_exit': 145759232, 'instr_end': 146390016, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 255, 'bar_enter': 147555328, 'bar_exit': 147590144, 'instr_end': 148407296, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 258, 'bar_enter': 149380096, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150920192, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153777152, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 269, 'bar_enter': 154936320, 'bar_exit': 154972160, 'instr_end': 155790336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 272, 'bar_enter': 156902400, 'bar_exit': 156903424, 'instr_end': 157782016, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 279, 'bar_enter': 160533504, 'bar_exit': 160533504, 'instr_end': 161164288, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 283, 'bar_enter': 162318336, 'bar_exit': 162356224, 'instr_end': 163174400, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 286, 'bar_enter': 164146176, 'bar_exit': 164284416, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 290, 'bar_enter': 165465088, 'bar_exit': 165465088, 'instr_end': 165679104, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167914496, 'instr_end': 168542208, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 125, 'bar_enter': 63758336, 'bar_exit': 63803392, 'instr_end': 78277632, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 904, 'end_b_x': 678, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 129, 'bar_enter': 79339520, 'bar_exit': 79376384, 'instr_end': 80250880, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 132, 'bar_enter': 81417216, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85939200, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 143, 'bar_enter': 87223296, 'bar_exit': 87223296, 'instr_end': 88095744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 146, 'bar_enter': 89135104, 'bar_exit': 89277440, 'instr_end': 90213376, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90752000, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 153, 'bar_enter': 93038592, 'bar_exit': 93041664, 'instr_end': 93697024, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 157, 'bar_enter': 94887936, 'bar_exit': 94921728, 'instr_end': 95761408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 160, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97800192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 164, 'bar_enter': 98107392, 'bar_exit': 98108416, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 167, 'bar_enter': 100600832, 'bar_exit': 100601856, 'instr_end': 101248000, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 171, 'bar_enter': 102472704, 'bar_exit': 102473728, 'instr_end': 103312384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 174, 'bar_enter': 104303616, 'bar_exit': 104444928, 'instr_end': 105347072, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 178, 'bar_enter': 105658368, 'bar_exit': 105658368, 'instr_end': 105877504, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 181, 'bar_enter': 108162048, 'bar_exit': 108162048, 'instr_end': 108816384, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110879744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 188, 'bar_enter': 112006144, 'bar_exit': 112023552, 'instr_end': 112922624, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 192, 'bar_enter': 113232896, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 195, 'bar_enter': 115731456, 'bar_exit': 115731456, 'instr_end': 116377600, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 199, 'bar_enter': 117602304, 'bar_exit': 117604352, 'instr_end': 118447104, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 202, 'bar_enter': 119442432, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121025536, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 209, 'bar_enter': 123313152, 'bar_exit': 123313152, 'instr_end': 123970560, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 213, 'bar_enter': 125147136, 'bar_exit': 125179904, 'instr_end': 126023680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 216, 'bar_enter': 127169536, 'bar_exit': 127169536, 'instr_end': 128071680, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 223, 'bar_enter': 130884608, 'bar_exit': 130885632, 'instr_end': 131524608, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 227, 'bar_enter': 132734976, 'bar_exit': 132737024, 'instr_end': 133562368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 230, 'bar_enter': 134536192, 'bar_exit': 134684672, 'instr_end': 135573504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136091648, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138335232, 'instr_end': 138979328, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 241, 'bar_enter': 140140544, 'bar_exit': 140174336, 'instr_end': 140999680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 244, 'bar_enter': 142108672, 'bar_exit': 142111744, 'instr_end': 142996480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 248, 'bar_enter': 143301632, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 251, 'bar_enter': 145757184, 'bar_exit': 145759232, 'instr_end': 146386944, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 255, 'bar_enter': 147590144, 'bar_exit': 147590144, 'instr_end': 148407296, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 258, 'bar_enter': 149381120, 'bar_exit': 149525504, 'instr_end': 150409216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150921216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153792512, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 269, 'bar_enter': 154937344, 'bar_exit': 154972160, 'instr_end': 155792384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157782016, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158085120, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161159168, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 283, 'bar_enter': 162356224, 'bar_exit': 162356224, 'instr_end': 163174400, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 286, 'bar_enter': 164147200, 'bar_exit': 164283392, 'instr_end': 165165056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165680128, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167913472, 'instr_end': 168551424, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 125, 'bar_enter': 63757312, 'bar_exit': 63803392, 'instr_end': 78269440, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 679, 'end_b_x': 453, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 129, 'bar_enter': 79341568, 'bar_exit': 79376384, 'instr_end': 80246784, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 132, 'bar_enter': 81419264, 'bar_exit': 81432576, 'instr_end': 82367488, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85268480, 'instr_end': 85926912, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 143, 'bar_enter': 87223296, 'bar_exit': 87223296, 'instr_end': 88209408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 146, 'bar_enter': 89128960, 'bar_exit': 89277440, 'instr_end': 90213376, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90752000, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 153, 'bar_enter': 93039616, 'bar_exit': 93041664, 'instr_end': 93697024, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 157, 'bar_enter': 94886912, 'bar_exit': 94921728, 'instr_end': 95759360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 160, 'bar_enter': 96896000, 'bar_exit': 96900096, 'instr_end': 97799168, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101234688, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 171, 'bar_enter': 102471680, 'bar_exit': 102473728, 'instr_end': 103418880, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 174, 'bar_enter': 104303616, 'bar_exit': 104444928, 'instr_end': 105347072, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105877504, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 181, 'bar_enter': 108160000, 'bar_exit': 108162048, 'instr_end': 108815360, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110877696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 188, 'bar_enter': 112022528, 'bar_exit': 112023552, 'instr_end': 112922624, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 192, 'bar_enter': 113232896, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 195, 'bar_enter': 115731456, 'bar_exit': 115732480, 'instr_end': 116367360, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 199, 'bar_enter': 117602304, 'bar_exit': 117604352, 'instr_end': 118553600, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 202, 'bar_enter': 119442432, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121025536, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 209, 'bar_enter': 123313152, 'bar_exit': 123313152, 'instr_end': 123971584, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 213, 'bar_enter': 125146112, 'bar_exit': 125179904, 'instr_end': 126019584, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 216, 'bar_enter': 127151104, 'bar_exit': 127169536, 'instr_end': 128071680, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131516416, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 227, 'bar_enter': 132734976, 'bar_exit': 132737024, 'instr_end': 133665792, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 230, 'bar_enter': 134538240, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136091648, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138334208, 'instr_end': 138978304, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 241, 'bar_enter': 140139520, 'bar_exit': 140174336, 'instr_end': 140997632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 244, 'bar_enter': 142105600, 'bar_exit': 142111744, 'instr_end': 142996480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 251, 'bar_enter': 145757184, 'bar_exit': 145759232, 'instr_end': 146376704, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 255, 'bar_enter': 147590144, 'bar_exit': 147590144, 'instr_end': 148517888, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 258, 'bar_enter': 149395456, 'bar_exit': 149525504, 'instr_end': 150409216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150921216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153790464, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 269, 'bar_enter': 154937344, 'bar_exit': 154972160, 'instr_end': 155788288, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 272, 'bar_enter': 156891136, 'bar_exit': 156903424, 'instr_end': 157780992, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158085120, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 279, 'bar_enter': 160532480, 'bar_exit': 160532480, 'instr_end': 161150976, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 283, 'bar_enter': 162356224, 'bar_exit': 162356224, 'instr_end': 163281920, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 286, 'bar_enter': 164152320, 'bar_exit': 164284416, 'instr_end': 165165056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165680128, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167913472, 'instr_end': 168551424, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 125, 'bar_enter': 63798272, 'bar_exit': 63803392, 'instr_end': 78252032, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 454, 'end_b_x': 228, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 129, 'bar_enter': 79340544, 'bar_exit': 79376384, 'instr_end': 80249856, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 132, 'bar_enter': 81431552, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85268480, 'instr_end': 85936128, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 143, 'bar_enter': 87223296, 'bar_exit': 87223296, 'instr_end': 88103936, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 146, 'bar_enter': 89126912, 'bar_exit': 89277440, 'instr_end': 90213376, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90752000, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 153, 'bar_enter': 93039616, 'bar_exit': 93041664, 'instr_end': 93696000, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 157, 'bar_enter': 94884864, 'bar_exit': 94921728, 'instr_end': 95761408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 160, 'bar_enter': 96885760, 'bar_exit': 96900096, 'instr_end': 97800192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98336768, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101240832, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 171, 'bar_enter': 102472704, 'bar_exit': 102473728, 'instr_end': 103322624, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 174, 'bar_enter': 104303616, 'bar_exit': 104444928, 'instr_end': 105347072, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105877504, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 181, 'bar_enter': 108162048, 'bar_exit': 108162048, 'instr_end': 108815360, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110879744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 188, 'bar_enter': 112006144, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 195, 'bar_enter': 115731456, 'bar_exit': 115732480, 'instr_end': 116378624, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 199, 'bar_enter': 117602304, 'bar_exit': 117604352, 'instr_end': 118458368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 202, 'bar_enter': 119450624, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 206, 'bar_enter': 120803328, 'bar_exit': 120803328, 'instr_end': 121025536, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 209, 'bar_enter': 123312128, 'bar_exit': 123313152, 'instr_end': 123971584, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 213, 'bar_enter': 125146112, 'bar_exit': 125179904, 'instr_end': 126023680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 216, 'bar_enter': 127150080, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131525632, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 227, 'bar_enter': 132734976, 'bar_exit': 132737024, 'instr_end': 133570560, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 230, 'bar_enter': 134548480, 'bar_exit': 134684672, 'instr_end': 135573504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136091648, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 237, 'bar_enter': 138333184, 'bar_exit': 138333184, 'instr_end': 138977280, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 241, 'bar_enter': 140139520, 'bar_exit': 140174336, 'instr_end': 141000704, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 244, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 248, 'bar_enter': 143300608, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 251, 'bar_enter': 145756160, 'bar_exit': 145759232, 'instr_end': 146386944, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 255, 'bar_enter': 147590144, 'bar_exit': 147590144, 'instr_end': 148417536, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 258, 'bar_enter': 149381120, 'bar_exit': 149525504, 'instr_end': 150409216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150921216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153791488, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 269, 'bar_enter': 154936320, 'bar_exit': 154972160, 'instr_end': 155790336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 276, 'bar_enter': 158085120, 'bar_exit': 158085120, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161160192, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 283, 'bar_enter': 162356224, 'bar_exit': 162356224, 'instr_end': 163183616, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 286, 'bar_enter': 164145152, 'bar_exit': 164284416, 'instr_end': 165165056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165680128, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 293, 'bar_enter': 167914496, 'bar_exit': 167914496, 'instr_end': 168552448, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 125, 'bar_enter': 63798272, 'bar_exit': 63803392, 'instr_end': 78238720, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 229, 'end_b_x': 3, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 129, 'bar_enter': 79341568, 'bar_exit': 79376384, 'instr_end': 80248832, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 132, 'bar_enter': 81421312, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 136, 'bar_enter': 82691072, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85269504, 'instr_end': 85940224, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 143, 'bar_enter': 87220224, 'bar_exit': 87222272, 'instr_end': 88104960, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 146, 'bar_enter': 89266176, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 150, 'bar_enter': 90493952, 'bar_exit': 90528768, 'instr_end': 90752000, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 153, 'bar_enter': 93039616, 'bar_exit': 93041664, 'instr_end': 93686784, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 157, 'bar_enter': 94887936, 'bar_exit': 94921728, 'instr_end': 95761408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 160, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97800192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101244928, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 171, 'bar_enter': 102472704, 'bar_exit': 102473728, 'instr_end': 103323648, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 174, 'bar_enter': 104440832, 'bar_exit': 104444928, 'instr_end': 105347072, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 178, 'bar_enter': 105619456, 'bar_exit': 105657344, 'instr_end': 105877504, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 181, 'bar_enter': 108160000, 'bar_exit': 108162048, 'instr_end': 108805120, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 185, 'bar_enter': 110004224, 'bar_exit': 110040064, 'instr_end': 110879744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 188, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 195, 'bar_enter': 115728384, 'bar_exit': 115732480, 'instr_end': 116376576, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 199, 'bar_enter': 117600256, 'bar_exit': 117604352, 'instr_end': 118458368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 202, 'bar_enter': 119576576, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 206, 'bar_enter': 120768512, 'bar_exit': 120803328, 'instr_end': 121025536, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 209, 'bar_enter': 123312128, 'bar_exit': 123313152, 'instr_end': 123961344, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 213, 'bar_enter': 125147136, 'bar_exit': 125179904, 'instr_end': 126022656, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 216, 'bar_enter': 127155200, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 220, 'bar_enter': 128381952, 'bar_exit': 128384000, 'instr_end': 128614400, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 223, 'bar_enter': 130884608, 'bar_exit': 130885632, 'instr_end': 131525632, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 227, 'bar_enter': 132734976, 'bar_exit': 132737024, 'instr_end': 133572608, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 230, 'bar_enter': 134665216, 'bar_exit': 134684672, 'instr_end': 135573504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 234, 'bar_enter': 135840768, 'bar_exit': 135874560, 'instr_end': 136091648, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 237, 'bar_enter': 138333184, 'bar_exit': 138334208, 'instr_end': 138966016, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 241, 'bar_enter': 140139520, 'bar_exit': 140174336, 'instr_end': 141000704, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 244, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 248, 'bar_enter': 143300608, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 251, 'bar_enter': 145758208, 'bar_exit': 145759232, 'instr_end': 146385920, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 255, 'bar_enter': 147588096, 'bar_exit': 147590144, 'instr_end': 148417536, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 258, 'bar_enter': 149510144, 'bar_exit': 149525504, 'instr_end': 150409216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 262, 'bar_enter': 150672384, 'bar_exit': 150706176, 'instr_end': 150921216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153780224, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 269, 'bar_enter': 154937344, 'bar_exit': 154972160, 'instr_end': 155789312, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 272, 'bar_enter': 156904448, 'bar_exit': 156904448, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158085120, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 279, 'bar_enter': 160529408, 'bar_exit': 160532480, 'instr_end': 161160192, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 283, 'bar_enter': 162351104, 'bar_exit': 162356224, 'instr_end': 163184640, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 286, 'bar_enter': 164274176, 'bar_exit': 164283392, 'instr_end': 165165056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 290, 'bar_enter': 165431296, 'bar_exit': 165465088, 'instr_end': 165680128, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 293, 'bar_enter': 167914496, 'bar_exit': 167914496, 'instr_end': 168542208, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 125, 'bar_enter': 63798272, 'bar_exit': 63803392, 'instr_end': 78189568, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 4, 'end_b_x': 1349, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 129, 'bar_enter': 79339520, 'bar_exit': 79376384, 'instr_end': 80245760, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 132, 'bar_enter': 81417216, 'bar_exit': 81432576, 'instr_end': 82368512, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85937152, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 143, 'bar_enter': 87221248, 'bar_exit': 87222272, 'instr_end': 88102912, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 146, 'bar_enter': 89274368, 'bar_exit': 89277440, 'instr_end': 90220544, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 150, 'bar_enter': 90493952, 'bar_exit': 90528768, 'instr_end': 90752000, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 153, 'bar_enter': 93039616, 'bar_exit': 93041664, 'instr_end': 93684736, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 157, 'bar_enter': 94888960, 'bar_exit': 94921728, 'instr_end': 95759360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 160, 'bar_enter': 96884736, 'bar_exit': 96900096, 'instr_end': 97800192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101245952, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 171, 'bar_enter': 102472704, 'bar_exit': 102473728, 'instr_end': 103320576, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105354240, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 178, 'bar_enter': 105621504, 'bar_exit': 105657344, 'instr_end': 105877504, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 181, 'bar_enter': 108161024, 'bar_exit': 108162048, 'instr_end': 108803072, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110876672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 188, 'bar_enter': 112006144, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 195, 'bar_enter': 115728384, 'bar_exit': 115732480, 'instr_end': 116378624, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 199, 'bar_enter': 117599232, 'bar_exit': 117604352, 'instr_end': 118456320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 202, 'bar_enter': 119576576, 'bar_exit': 119588864, 'instr_end': 120501248, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 206, 'bar_enter': 120768512, 'bar_exit': 120803328, 'instr_end': 121025536, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 209, 'bar_enter': 123312128, 'bar_exit': 123313152, 'instr_end': 123958272, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 213, 'bar_enter': 125145088, 'bar_exit': 125179904, 'instr_end': 126021632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 216, 'bar_enter': 127168512, 'bar_exit': 127169536, 'instr_end': 128072704, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 223, 'bar_enter': 130884608, 'bar_exit': 130885632, 'instr_end': 131525632, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 227, 'bar_enter': 132734976, 'bar_exit': 132737024, 'instr_end': 133570560, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 230, 'bar_enter': 134668288, 'bar_exit': 134684672, 'instr_end': 135580672, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 234, 'bar_enter': 135840768, 'bar_exit': 135873536, 'instr_end': 136091648, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138334208, 'instr_end': 138964992, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 241, 'bar_enter': 140139520, 'bar_exit': 140174336, 'instr_end': 140998656, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 244, 'bar_enter': 142105600, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 248, 'bar_enter': 143301632, 'bar_exit': 143302656, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 251, 'bar_enter': 145758208, 'bar_exit': 145759232, 'instr_end': 146385920, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 255, 'bar_enter': 147589120, 'bar_exit': 147590144, 'instr_end': 148415488, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 258, 'bar_enter': 149511168, 'bar_exit': 149525504, 'instr_end': 150415360, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 262, 'bar_enter': 150672384, 'bar_exit': 150706176, 'instr_end': 150921216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153779200, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 269, 'bar_enter': 154937344, 'bar_exit': 154972160, 'instr_end': 155788288, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158085120, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 279, 'bar_enter': 160530432, 'bar_exit': 160532480, 'instr_end': 161159168, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 283, 'bar_enter': 162351104, 'bar_exit': 162356224, 'instr_end': 163181568, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 286, 'bar_enter': 164276224, 'bar_exit': 164284416, 'instr_end': 165171200, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 290, 'bar_enter': 165431296, 'bar_exit': 165465088, 'instr_end': 165680128, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 293, 'bar_enter': 167914496, 'bar_exit': 167914496, 'instr_end': 168540160, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 125, 'bar_enter': 63799296, 'bar_exit': 63803392, 'instr_end': 78220288, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 1350, 'end_b_x': 1124, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 129, 'bar_enter': 79341568, 'bar_exit': 79376384, 'instr_end': 80251904, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 132, 'bar_enter': 81419264, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85268480, 'instr_end': 85932032, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 143, 'bar_enter': 87221248, 'bar_exit': 87222272, 'instr_end': 88108032, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 146, 'bar_enter': 89266176, 'bar_exit': 89277440, 'instr_end': 90221568, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 150, 'bar_enter': 90493952, 'bar_exit': 90528768, 'instr_end': 90752000, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93681664, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 157, 'bar_enter': 94887936, 'bar_exit': 94921728, 'instr_end': 95763456, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 160, 'bar_enter': 96898048, 'bar_exit': 96900096, 'instr_end': 97800192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 167, 'bar_enter': 100600832, 'bar_exit': 100600832, 'instr_end': 101239808, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 171, 'bar_enter': 102471680, 'bar_exit': 102473728, 'instr_end': 103324672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105355264, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 178, 'bar_enter': 105619456, 'bar_exit': 105657344, 'instr_end': 105877504, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 181, 'bar_enter': 108160000, 'bar_exit': 108162048, 'instr_end': 108801024, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 185, 'bar_enter': 110003200, 'bar_exit': 110040064, 'instr_end': 110880768, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 188, 'bar_enter': 112024576, 'bar_exit': 112024576, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 192, 'bar_enter': 113233920, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 195, 'bar_enter': 115728384, 'bar_exit': 115731456, 'instr_end': 116372480, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 199, 'bar_enter': 117599232, 'bar_exit': 117604352, 'instr_end': 118460416, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 202, 'bar_enter': 119575552, 'bar_exit': 119588864, 'instr_end': 120502272, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 206, 'bar_enter': 120769536, 'bar_exit': 120803328, 'instr_end': 121025536, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 209, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123955200, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 213, 'bar_enter': 125147136, 'bar_exit': 125179904, 'instr_end': 126025728, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 216, 'bar_enter': 127151104, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128613376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 223, 'bar_enter': 130886656, 'bar_exit': 130886656, 'instr_end': 131519488, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 227, 'bar_enter': 132733952, 'bar_exit': 132737024, 'instr_end': 133574656, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 230, 'bar_enter': 134666240, 'bar_exit': 134684672, 'instr_end': 135581696, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 234, 'bar_enter': 135839744, 'bar_exit': 135874560, 'instr_end': 136091648, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 237, 'bar_enter': 138333184, 'bar_exit': 138334208, 'instr_end': 138962944, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 241, 'bar_enter': 140139520, 'bar_exit': 140174336, 'instr_end': 141000704, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 244, 'bar_enter': 142106624, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 251, 'bar_enter': 145758208, 'bar_exit': 145759232, 'instr_end': 146381824, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 255, 'bar_enter': 147588096, 'bar_exit': 147590144, 'instr_end': 148420608, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 258, 'bar_enter': 149525504, 'bar_exit': 149525504, 'instr_end': 150414336, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 262, 'bar_enter': 150672384, 'bar_exit': 150706176, 'instr_end': 150921216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 265, 'bar_enter': 153153536, 'bar_exit': 153153536, 'instr_end': 153776128, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 269, 'bar_enter': 154938368, 'bar_exit': 154972160, 'instr_end': 155792384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 20, 'end_b_x': 5, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 272, 'bar_enter': 156890112, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 86, 'end_b_x': 71, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 276, 'bar_enter': 158083072, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 279, 'bar_enter': 160530432, 'bar_exit': 160532480, 'instr_end': 161154048, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 64, 'end_b_x': 53, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 283, 'bar_enter': 162352128, 'bar_exit': 162356224, 'instr_end': 163186688, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 20, 'end_b_x': 6, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 286, 'bar_enter': 164282368, 'bar_exit': 164284416, 'instr_end': 165171200, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 83, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 290, 'bar_enter': 165431296, 'bar_exit': 165465088, 'instr_end': 165680128, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 293, 'bar_enter': 167914496, 'bar_exit': 167914496, 'instr_end': 168537088, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 66, 'end_b_x': 54, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 125, 'bar_enter': 61799424, 'bar_exit': 63803392, 'instr_end': 78226432, 'spin_wait': 2, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 1125, 'end_b_x': 899, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 129, 'bar_enter': 79341568, 'bar_exit': 79376384, 'instr_end': 80248832, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 132, 'bar_enter': 81429504, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 136, 'bar_enter': 82691072, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85268480, 'instr_end': 85926912, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 143, 'bar_enter': 87220224, 'bar_exit': 87222272, 'instr_end': 88102912, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 146, 'bar_enter': 89265152, 'bar_exit': 89277440, 'instr_end': 90220544, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 150, 'bar_enter': 90493952, 'bar_exit': 90528768, 'instr_end': 90752000, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 153, 'bar_enter': 92939264, 'bar_exit': 93041664, 'instr_end': 93665280, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 157, 'bar_enter': 94886912, 'bar_exit': 94921728, 'instr_end': 95758336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 160, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97800192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101234688, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 171, 'bar_enter': 102473728, 'bar_exit': 102473728, 'instr_end': 103320576, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105355264, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 178, 'bar_enter': 105619456, 'bar_exit': 105657344, 'instr_end': 105877504, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 181, 'bar_enter': 108060672, 'bar_exit': 108162048, 'instr_end': 108786688, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110876672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 188, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115732480, 'instr_end': 116368384, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 199, 'bar_enter': 117600256, 'bar_exit': 117604352, 'instr_end': 118456320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 202, 'bar_enter': 119585792, 'bar_exit': 119588864, 'instr_end': 120501248, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 206, 'bar_enter': 120769536, 'bar_exit': 120803328, 'instr_end': 121025536, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 209, 'bar_enter': 123214848, 'bar_exit': 123313152, 'instr_end': 123939840, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 213, 'bar_enter': 125145088, 'bar_exit': 125179904, 'instr_end': 126020608, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 216, 'bar_enter': 127150080, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128614400, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131516416, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 227, 'bar_enter': 132734976, 'bar_exit': 132737024, 'instr_end': 133569536, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 230, 'bar_enter': 134680576, 'bar_exit': 134684672, 'instr_end': 135581696, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 234, 'bar_enter': 135840768, 'bar_exit': 135874560, 'instr_end': 136091648, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 237, 'bar_enter': 138234880, 'bar_exit': 138334208, 'instr_end': 138951680, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 241, 'bar_enter': 140139520, 'bar_exit': 140174336, 'instr_end': 140997632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 244, 'bar_enter': 142105600, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 251, 'bar_enter': 145757184, 'bar_exit': 145759232, 'instr_end': 146378752, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 255, 'bar_enter': 147590144, 'bar_exit': 147590144, 'instr_end': 148416512, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 258, 'bar_enter': 149511168, 'bar_exit': 149525504, 'instr_end': 150414336, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 262, 'bar_enter': 150672384, 'bar_exit': 150706176, 'instr_end': 150921216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 265, 'bar_enter': 153053184, 'bar_exit': 153152512, 'instr_end': 153760768, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 269, 'bar_enter': 154935296, 'bar_exit': 154972160, 'instr_end': 155788288, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 6, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 272, 'bar_enter': 156890112, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 72, 'end_b_x': 57, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 276, 'bar_enter': 158083072, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161150976, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 54, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 283, 'bar_enter': 162352128, 'bar_exit': 162356224, 'instr_end': 163182592, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 7, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 286, 'bar_enter': 164274176, 'bar_exit': 164284416, 'instr_end': 165171200, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 70, 'end_b_x': 56, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 290, 'bar_enter': 165431296, 'bar_exit': 165465088, 'instr_end': 165680128, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 293, 'bar_enter': 167816192, 'bar_exit': 167914496, 'instr_end': 168522752, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 55, 'end_b_x': 43, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 126, 'bar_enter': 61799424, 'bar_exit': 63803392, 'instr_end': 78234624, 'spin_wait': 2, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 900, 'end_b_x': 674, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 130, 'bar_enter': 79342592, 'bar_exit': 79376384, 'instr_end': 80252928, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 133, 'bar_enter': 81418240, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 137, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 140, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85931008, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 144, 'bar_enter': 87219200, 'bar_exit': 87222272, 'instr_end': 88108032, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 147, 'bar_enter': 89265152, 'bar_exit': 89277440, 'instr_end': 90223616, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 151, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90752000, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 154, 'bar_enter': 92939264, 'bar_exit': 93041664, 'instr_end': 93669376, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 158, 'bar_enter': 94889984, 'bar_exit': 94921728, 'instr_end': 95762432, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 161, 'bar_enter': 96882688, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 165, 'bar_enter': 98108416, 'bar_exit': 98108416, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 168, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101238784, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 172, 'bar_enter': 102473728, 'bar_exit': 102473728, 'instr_end': 103324672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 175, 'bar_enter': 104442880, 'bar_exit': 104444928, 'instr_end': 105356288, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 179, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105878528, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 182, 'bar_enter': 108061696, 'bar_exit': 108162048, 'instr_end': 108789760, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 186, 'bar_enter': 110004224, 'bar_exit': 110040064, 'instr_end': 110880768, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 189, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 193, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 196, 'bar_enter': 115729408, 'bar_exit': 115731456, 'instr_end': 116372480, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 200, 'bar_enter': 117602304, 'bar_exit': 117604352, 'instr_end': 118460416, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 203, 'bar_enter': 119575552, 'bar_exit': 119588864, 'instr_end': 120503296, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 207, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121025536, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 210, 'bar_enter': 123213824, 'bar_exit': 123313152, 'instr_end': 123943936, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 214, 'bar_enter': 125147136, 'bar_exit': 125179904, 'instr_end': 126024704, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 217, 'bar_enter': 127153152, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 221, 'bar_enter': 128382976, 'bar_exit': 128384000, 'instr_end': 128614400, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 224, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131520512, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 228, 'bar_enter': 132737024, 'bar_exit': 132737024, 'instr_end': 133573632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 231, 'bar_enter': 134665216, 'bar_exit': 134684672, 'instr_end': 135581696, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 235, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136091648, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 238, 'bar_enter': 138235904, 'bar_exit': 138334208, 'instr_end': 138951680, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 242, 'bar_enter': 140140544, 'bar_exit': 140174336, 'instr_end': 141001728, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 245, 'bar_enter': 142105600, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 249, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 252, 'bar_enter': 145757184, 'bar_exit': 145759232, 'instr_end': 146380800, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 256, 'bar_enter': 147589120, 'bar_exit': 147590144, 'instr_end': 148419584, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 259, 'bar_enter': 149508096, 'bar_exit': 149525504, 'instr_end': 150415360, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 263, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150921216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 266, 'bar_enter': 153054208, 'bar_exit': 153152512, 'instr_end': 153763840, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 270, 'bar_enter': 154935296, 'bar_exit': 154972160, 'instr_end': 155792384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 273, 'bar_enter': 156904448, 'bar_exit': 156904448, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 58, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 277, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 280, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161155072, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 44, 'end_b_x': 33, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 284, 'bar_enter': 162353152, 'bar_exit': 162356224, 'instr_end': 163185664, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 287, 'bar_enter': 164274176, 'bar_exit': 164283392, 'instr_end': 165172224, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 57, 'end_b_x': 43, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 291, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165680128, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 294, 'bar_enter': 167817216, 'bar_exit': 167913472, 'instr_end': 168526848, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 44, 'end_b_x': 32, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 125, 'bar_enter': 61799424, 'bar_exit': 63803392, 'instr_end': 78227456, 'spin_wait': 2, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 675, 'end_b_x': 449, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 129, 'bar_enter': 79342592, 'bar_exit': 79376384, 'instr_end': 80253952, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 132, 'bar_enter': 81417216, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85933056, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 143, 'bar_enter': 87218176, 'bar_exit': 87222272, 'instr_end': 88103936, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 146, 'bar_enter': 89276416, 'bar_exit': 89277440, 'instr_end': 90222592, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 153, 'bar_enter': 92939264, 'bar_exit': 93041664, 'instr_end': 93666304, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 157, 'bar_enter': 94889984, 'bar_exit': 94921728, 'instr_end': 95761408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 160, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101237760, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 171, 'bar_enter': 102473728, 'bar_exit': 102473728, 'instr_end': 103321600, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105355264, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 178, 'bar_enter': 105656320, 'bar_exit': 105657344, 'instr_end': 105886720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 181, 'bar_enter': 108062720, 'bar_exit': 108162048, 'instr_end': 108786688, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 185, 'bar_enter': 110004224, 'bar_exit': 110040064, 'instr_end': 110878720, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 188, 'bar_enter': 112008192, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115732480, 'instr_end': 116371456, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 199, 'bar_enter': 117600256, 'bar_exit': 117604352, 'instr_end': 118457344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 202, 'bar_enter': 119574528, 'bar_exit': 119588864, 'instr_end': 120502272, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121033728, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 209, 'bar_enter': 123214848, 'bar_exit': 123313152, 'instr_end': 123941888, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 213, 'bar_enter': 125147136, 'bar_exit': 125179904, 'instr_end': 126023680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 216, 'bar_enter': 127168512, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128614400, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131518464, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 227, 'bar_enter': 132736000, 'bar_exit': 132737024, 'instr_end': 133570560, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 230, 'bar_enter': 134668288, 'bar_exit': 134684672, 'instr_end': 135581696, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135873536, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 237, 'bar_enter': 138233856, 'bar_exit': 138334208, 'instr_end': 138947584, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 241, 'bar_enter': 140140544, 'bar_exit': 140174336, 'instr_end': 140999680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 244, 'bar_enter': 142112768, 'bar_exit': 142112768, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 248, 'bar_enter': 143301632, 'bar_exit': 143302656, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 251, 'bar_enter': 145755136, 'bar_exit': 145759232, 'instr_end': 146380800, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 255, 'bar_enter': 147591168, 'bar_exit': 147591168, 'instr_end': 148416512, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 258, 'bar_enter': 149511168, 'bar_exit': 149525504, 'instr_end': 150416384, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 265, 'bar_enter': 153054208, 'bar_exit': 153152512, 'instr_end': 153760768, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 269, 'bar_enter': 154935296, 'bar_exit': 154972160, 'instr_end': 155791360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 44, 'end_b_x': 29, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161156096, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 34, 'end_b_x': 23, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 283, 'bar_enter': 162354176, 'bar_exit': 162356224, 'instr_end': 163182592, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 5, 'end_b_x': 15, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 286, 'bar_enter': 164277248, 'bar_exit': 164284416, 'instr_end': 165172224, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 44, 'end_b_x': 30, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 293, 'bar_enter': 167817216, 'bar_exit': 167914496, 'instr_end': 168521728, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 33, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 125, 'bar_enter': 61799424, 'bar_exit': 63803392, 'instr_end': 78210048, 'spin_wait': 2, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 450, 'end_b_x': 224, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 129, 'bar_enter': 79344640, 'bar_exit': 79376384, 'instr_end': 80249856, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 132, 'bar_enter': 81418240, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82930688, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85932032, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 143, 'bar_enter': 87219200, 'bar_exit': 87222272, 'instr_end': 88095744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 146, 'bar_enter': 89266176, 'bar_exit': 89277440, 'instr_end': 90221568, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 153, 'bar_enter': 92939264, 'bar_exit': 93041664, 'instr_end': 93671424, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 157, 'bar_enter': 94889984, 'bar_exit': 94921728, 'instr_end': 95760384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 160, 'bar_enter': 96897024, 'bar_exit': 96900096, 'instr_end': 97800192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98337792, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 167, 'bar_enter': 100600832, 'bar_exit': 100600832, 'instr_end': 101237760, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 171, 'bar_enter': 102473728, 'bar_exit': 102473728, 'instr_end': 103314432, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 174, 'bar_enter': 104436736, 'bar_exit': 104444928, 'instr_end': 105354240, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105886720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 181, 'bar_enter': 108061696, 'bar_exit': 108162048, 'instr_end': 108793856, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 185, 'bar_enter': 110003200, 'bar_exit': 110040064, 'instr_end': 110877696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 188, 'bar_enter': 112021504, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113464320, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115731456, 'instr_end': 116370432, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 199, 'bar_enter': 117600256, 'bar_exit': 117604352, 'instr_end': 118449152, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 202, 'bar_enter': 119575552, 'bar_exit': 119588864, 'instr_end': 120501248, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 206, 'bar_enter': 120803328, 'bar_exit': 120803328, 'instr_end': 121033728, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 209, 'bar_enter': 123213824, 'bar_exit': 123313152, 'instr_end': 123949056, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 213, 'bar_enter': 125147136, 'bar_exit': 125179904, 'instr_end': 126021632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 216, 'bar_enter': 127153152, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128614400, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131517440, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 227, 'bar_enter': 132736000, 'bar_exit': 132737024, 'instr_end': 133562368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 230, 'bar_enter': 134668288, 'bar_exit': 134684672, 'instr_end': 135581696, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 237, 'bar_enter': 138234880, 'bar_exit': 138334208, 'instr_end': 138952704, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 241, 'bar_enter': 140140544, 'bar_exit': 140174336, 'instr_end': 140999680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 244, 'bar_enter': 142105600, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143527936, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 251, 'bar_enter': 145757184, 'bar_exit': 145759232, 'instr_end': 146379776, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 255, 'bar_enter': 147589120, 'bar_exit': 147590144, 'instr_end': 148410368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 258, 'bar_enter': 149523456, 'bar_exit': 149525504, 'instr_end': 150416384, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150929408, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 265, 'bar_enter': 153054208, 'bar_exit': 153152512, 'instr_end': 153769984, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 269, 'bar_enter': 154935296, 'bar_exit': 154972160, 'instr_end': 155790336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 272, 'bar_enter': 156890112, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 30, 'end_b_x': 15, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158309376, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 279, 'bar_enter': 160530432, 'bar_exit': 160532480, 'instr_end': 161154048, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 24, 'end_b_x': 13, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 283, 'bar_enter': 162353152, 'bar_exit': 162356224, 'instr_end': 163175424, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 286, 'bar_enter': 164282368, 'bar_exit': 164283392, 'instr_end': 165174272, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 31, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 293, 'bar_enter': 167817216, 'bar_exit': 167913472, 'instr_end': 168532992, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 22, 'end_b_x': 10, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 125, 'bar_enter': 63761408, 'bar_exit': 63803392, 'instr_end': 78295040, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 225, 'end_b_x': 1570, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 129, 'bar_enter': 79341568, 'bar_exit': 79376384, 'instr_end': 80251904, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 132, 'bar_enter': 81429504, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82919424, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85931008, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 143, 'bar_enter': 87221248, 'bar_exit': 87222272, 'instr_end': 88095744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 146, 'bar_enter': 89263104, 'bar_exit': 89277440, 'instr_end': 90221568, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93664256, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 157, 'bar_enter': 94887936, 'bar_exit': 94921728, 'instr_end': 95761408, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 160, 'bar_enter': 96884736, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 164, 'bar_enter': 98107392, 'bar_exit': 98108416, 'instr_end': 98326528, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101237760, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 171, 'bar_enter': 102473728, 'bar_exit': 102473728, 'instr_end': 103314432, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105355264, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 181, 'bar_enter': 108158976, 'bar_exit': 108162048, 'instr_end': 108785664, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110880768, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 188, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112924672, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113453056, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 195, 'bar_enter': 115728384, 'bar_exit': 115732480, 'instr_end': 116373504, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 199, 'bar_enter': 117605376, 'bar_exit': 117605376, 'instr_end': 118449152, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 202, 'bar_enter': 119587840, 'bar_exit': 119588864, 'instr_end': 120503296, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 206, 'bar_enter': 120803328, 'bar_exit': 120803328, 'instr_end': 121033728, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 209, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123940864, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 213, 'bar_enter': 125144064, 'bar_exit': 125179904, 'instr_end': 126024704, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 216, 'bar_enter': 127150080, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128603136, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131517440, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 227, 'bar_enter': 132737024, 'bar_exit': 132738048, 'instr_end': 133563392, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 230, 'bar_enter': 134680576, 'bar_exit': 134684672, 'instr_end': 135581696, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135873536, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 237, 'bar_enter': 138333184, 'bar_exit': 138334208, 'instr_end': 138945536, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 241, 'bar_enter': 140139520, 'bar_exit': 140174336, 'instr_end': 141001728, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 244, 'bar_enter': 142105600, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143517696, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 251, 'bar_enter': 145756160, 'bar_exit': 145759232, 'instr_end': 146381824, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 255, 'bar_enter': 147590144, 'bar_exit': 147590144, 'instr_end': 148409344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 258, 'bar_enter': 149511168, 'bar_exit': 149525504, 'instr_end': 150416384, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150929408, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 265, 'bar_enter': 153150464, 'bar_exit': 153152512, 'instr_end': 153760768, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 269, 'bar_enter': 154936320, 'bar_exit': 154972160, 'instr_end': 155791360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 16, 'end_b_x': 1, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 276, 'bar_enter': 158083072, 'bar_exit': 158084096, 'instr_end': 158299136, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 279, 'bar_enter': 160528384, 'bar_exit': 160532480, 'instr_end': 161156096, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 14, 'end_b_x': 3, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 283, 'bar_enter': 162357248, 'bar_exit': 162357248, 'instr_end': 163175424, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 2, 'end_b_x': 11, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 286, 'bar_enter': 164274176, 'bar_exit': 164283392, 'instr_end': 165174272, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 18, 'end_b_x': 4, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167914496, 'instr_end': 168521728, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 11, 'end_b_x': 71, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 125, 'bar_enter': 63762432, 'bar_exit': 63803392, 'instr_end': 78321664, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 0, 'end_b_x': 1346, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 129, 'bar_enter': 79338496, 'bar_exit': 79376384, 'instr_end': 80248832, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 132, 'bar_enter': 81421312, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82920448, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85936128, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 143, 'bar_enter': 87221248, 'bar_exit': 87222272, 'instr_end': 88093696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 146, 'bar_enter': 89266176, 'bar_exit': 89277440, 'instr_end': 90221568, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 150, 'bar_enter': 90529792, 'bar_exit': 90529792, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93934592, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 157, 'bar_enter': 94889984, 'bar_exit': 94921728, 'instr_end': 95758336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 160, 'bar_enter': 96882688, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98326528, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101240832, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 171, 'bar_enter': 102473728, 'bar_exit': 102473728, 'instr_end': 103311360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 174, 'bar_enter': 104444928, 'bar_exit': 104445952, 'instr_end': 105355264, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105886720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 181, 'bar_enter': 108158976, 'bar_exit': 108162048, 'instr_end': 109050880, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110876672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 188, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113453056, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 195, 'bar_enter': 115728384, 'bar_exit': 115732480, 'instr_end': 116376576, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 199, 'bar_enter': 117600256, 'bar_exit': 117604352, 'instr_end': 118448128, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 202, 'bar_enter': 119576576, 'bar_exit': 119588864, 'instr_end': 120501248, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121033728, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 209, 'bar_enter': 123312128, 'bar_exit': 123313152, 'instr_end': 124188672, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 213, 'bar_enter': 125145088, 'bar_exit': 125179904, 'instr_end': 126021632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 216, 'bar_enter': 127154176, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 220, 'bar_enter': 128384000, 'bar_exit': 128384000, 'instr_end': 128603136, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131522560, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 227, 'bar_enter': 132737024, 'bar_exit': 132737024, 'instr_end': 133562368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 230, 'bar_enter': 134664192, 'bar_exit': 134684672, 'instr_end': 135581696, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 237, 'bar_enter': 138333184, 'bar_exit': 138334208, 'instr_end': 139201536, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 241, 'bar_enter': 140140544, 'bar_exit': 140174336, 'instr_end': 140997632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 244, 'bar_enter': 142105600, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143517696, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 251, 'bar_enter': 145756160, 'bar_exit': 145759232, 'instr_end': 146384896, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 255, 'bar_enter': 147590144, 'bar_exit': 147590144, 'instr_end': 148408320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 258, 'bar_enter': 149508096, 'bar_exit': 149525504, 'instr_end': 150416384, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 265, 'bar_enter': 153150464, 'bar_exit': 153152512, 'instr_end': 154008576, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 269, 'bar_enter': 154937344, 'bar_exit': 154972160, 'instr_end': 155787264, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 272, 'bar_enter': 156904448, 'bar_exit': 156904448, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 2, 'end_b_x': 83, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158299136, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 279, 'bar_enter': 160530432, 'bar_exit': 160532480, 'instr_end': 161158144, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 4, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 283, 'bar_enter': 162356224, 'bar_exit': 162356224, 'instr_end': 163171328, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 12, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 286, 'bar_enter': 164274176, 'bar_exit': 164283392, 'instr_end': 165171200, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 5, 'end_b_x': 87, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 15, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167914496, 'instr_end': 168781824, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 0, 'end_b_x': 61, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 125, 'bar_enter': 63761408, 'bar_exit': 63803392, 'instr_end': 78331904, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 1347, 'end_b_x': 1122, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 129, 'bar_enter': 79338496, 'bar_exit': 79376384, 'instr_end': 80250880, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 132, 'bar_enter': 81417216, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82920448, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85945344, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 143, 'bar_enter': 87186432, 'bar_exit': 87222272, 'instr_end': 88093696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 146, 'bar_enter': 89273344, 'bar_exit': 89277440, 'instr_end': 90210304, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93733888, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 157, 'bar_enter': 94889984, 'bar_exit': 94921728, 'instr_end': 95759360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 160, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98326528, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101250048, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 171, 'bar_enter': 102440960, 'bar_exit': 102473728, 'instr_end': 103312384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105344000, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 181, 'bar_enter': 108158976, 'bar_exit': 108162048, 'instr_end': 108852224, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110878720, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 188, 'bar_enter': 112006144, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113453056, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 195, 'bar_enter': 115728384, 'bar_exit': 115732480, 'instr_end': 116384768, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 199, 'bar_enter': 117565440, 'bar_exit': 117604352, 'instr_end': 118447104, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 202, 'bar_enter': 119574528, 'bar_exit': 119588864, 'instr_end': 120491008, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121033728, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 209, 'bar_enter': 123312128, 'bar_exit': 123313152, 'instr_end': 124003328, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 213, 'bar_enter': 125144064, 'bar_exit': 125179904, 'instr_end': 126023680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 216, 'bar_enter': 127169536, 'bar_exit': 127169536, 'instr_end': 128072704, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128603136, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131530752, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 227, 'bar_enter': 132704256, 'bar_exit': 132737024, 'instr_end': 133561344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 230, 'bar_enter': 134666240, 'bar_exit': 134684672, 'instr_end': 135571456, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138334208, 'instr_end': 139010048, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 241, 'bar_enter': 140140544, 'bar_exit': 140174336, 'instr_end': 140999680, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 244, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 142997504, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 248, 'bar_enter': 143301632, 'bar_exit': 143302656, 'instr_end': 143517696, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 251, 'bar_enter': 145756160, 'bar_exit': 145759232, 'instr_end': 146393088, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 255, 'bar_enter': 147557376, 'bar_exit': 147590144, 'instr_end': 148409344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 258, 'bar_enter': 149508096, 'bar_exit': 149525504, 'instr_end': 150407168, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150929408, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 265, 'bar_enter': 153150464, 'bar_exit': 153152512, 'instr_end': 153827328, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 269, 'bar_enter': 154937344, 'bar_exit': 154972160, 'instr_end': 155791360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 84, 'end_b_x': 69, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158299136, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 279, 'bar_enter': 160529408, 'bar_exit': 160532480, 'instr_end': 161166336, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 66, 'end_b_x': 55, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 283, 'bar_enter': 162323456, 'bar_exit': 162356224, 'instr_end': 163174400, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 22, 'end_b_x': 7, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 286, 'bar_enter': 164276224, 'bar_exit': 164284416, 'instr_end': 165161984, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 88, 'end_b_x': 73, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 12, 'end_b_x': 8, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167913472, 'instr_end': 168586240, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 62, 'end_b_x': 51, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 125, 'bar_enter': 63763456, 'bar_exit': 63803392, 'instr_end': 78348288, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 1123, 'end_b_x': 898, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 129, 'bar_enter': 79339520, 'bar_exit': 79376384, 'instr_end': 80248832, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 132, 'bar_enter': 81282048, 'bar_exit': 81432576, 'instr_end': 82369536, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82920448, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85269504, 'instr_end': 85935104, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 143, 'bar_enter': 87184384, 'bar_exit': 87222272, 'instr_end': 88092672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 146, 'bar_enter': 89263104, 'bar_exit': 89277440, 'instr_end': 90211328, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93688832, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 157, 'bar_enter': 94888960, 'bar_exit': 94921728, 'instr_end': 95758336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 160, 'bar_enter': 96763904, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98326528, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 167, 'bar_enter': 100600832, 'bar_exit': 100600832, 'instr_end': 101243904, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 171, 'bar_enter': 102440960, 'bar_exit': 102473728, 'instr_end': 103310336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105345024, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 181, 'bar_enter': 108162048, 'bar_exit': 108162048, 'instr_end': 108810240, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 185, 'bar_enter': 110002176, 'bar_exit': 110040064, 'instr_end': 110876672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 188, 'bar_enter': 111888384, 'bar_exit': 112023552, 'instr_end': 112923648, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 192, 'bar_enter': 113231872, 'bar_exit': 113233920, 'instr_end': 113453056, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115732480, 'instr_end': 116377600, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 199, 'bar_enter': 117567488, 'bar_exit': 117604352, 'instr_end': 118445056, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 202, 'bar_enter': 119576576, 'bar_exit': 119588864, 'instr_end': 120491008, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 209, 'bar_enter': 123313152, 'bar_exit': 123313152, 'instr_end': 123963392, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 213, 'bar_enter': 125145088, 'bar_exit': 125179904, 'instr_end': 126021632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 216, 'bar_enter': 127019008, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128603136, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131526656, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 227, 'bar_enter': 132704256, 'bar_exit': 132737024, 'instr_end': 133561344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 230, 'bar_enter': 134668288, 'bar_exit': 134684672, 'instr_end': 135571456, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135873536, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138334208, 'instr_end': 138971136, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 241, 'bar_enter': 140140544, 'bar_exit': 140174336, 'instr_end': 140998656, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 244, 'bar_enter': 141977600, 'bar_exit': 142111744, 'instr_end': 142998528, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143517696, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 251, 'bar_enter': 145756160, 'bar_exit': 145759232, 'instr_end': 146388992, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 255, 'bar_enter': 147558400, 'bar_exit': 147590144, 'instr_end': 148408320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 258, 'bar_enter': 149523456, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153785344, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 269, 'bar_enter': 154937344, 'bar_exit': 154972160, 'instr_end': 155788288, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 272, 'bar_enter': 156762112, 'bar_exit': 156903424, 'instr_end': 157783040, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 70, 'end_b_x': 55, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158085120, 'instr_end': 158299136, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 12, 'end_b_x': 7, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 279, 'bar_enter': 160532480, 'bar_exit': 160533504, 'instr_end': 161161216, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 56, 'end_b_x': 45, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 283, 'bar_enter': 162322432, 'bar_exit': 162356224, 'instr_end': 163172352, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 8, 'end_b_x': 17, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 286, 'bar_enter': 164281344, 'bar_exit': 164283392, 'instr_end': 165161984, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 74, 'end_b_x': 59, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 9, 'end_b_x': 5, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167913472, 'instr_end': 168545280, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 52, 'end_b_x': 41, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 125, 'bar_enter': 63762432, 'bar_exit': 63803392, 'instr_end': 78234624, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 899, 'end_b_x': 674, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 129, 'bar_enter': 79375360, 'bar_exit': 79376384, 'instr_end': 80250880, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 132, 'bar_enter': 81289216, 'bar_exit': 81432576, 'instr_end': 82370560, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82921472, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85950464, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 143, 'bar_enter': 87184384, 'bar_exit': 87222272, 'instr_end': 88096768, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 146, 'bar_enter': 89263104, 'bar_exit': 89277440, 'instr_end': 90211328, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93686784, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 157, 'bar_enter': 94918656, 'bar_exit': 94921728, 'instr_end': 95759360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 160, 'bar_enter': 96752640, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98328576, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 167, 'bar_enter': 100600832, 'bar_exit': 100601856, 'instr_end': 101255168, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 171, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103312384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 174, 'bar_enter': 104436736, 'bar_exit': 104444928, 'instr_end': 105345024, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 181, 'bar_enter': 108162048, 'bar_exit': 108162048, 'instr_end': 108807168, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 185, 'bar_enter': 110040064, 'bar_exit': 110040064, 'instr_end': 110878720, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 188, 'bar_enter': 111874048, 'bar_exit': 112023552, 'instr_end': 112924672, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113454080, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115731456, 'instr_end': 116387840, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 199, 'bar_enter': 117568512, 'bar_exit': 117604352, 'instr_end': 118449152, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 202, 'bar_enter': 119588864, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 206, 'bar_enter': 120803328, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 209, 'bar_enter': 123313152, 'bar_exit': 123313152, 'instr_end': 123962368, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 213, 'bar_enter': 125179904, 'bar_exit': 125179904, 'instr_end': 126021632, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 216, 'bar_enter': 127017984, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128604160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131535872, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 227, 'bar_enter': 132704256, 'bar_exit': 132737024, 'instr_end': 133563392, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 230, 'bar_enter': 134684672, 'bar_exit': 134685696, 'instr_end': 135571456, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135873536, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138334208, 'instr_end': 138964992, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 241, 'bar_enter': 140172288, 'bar_exit': 140174336, 'instr_end': 140998656, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 244, 'bar_enter': 141977600, 'bar_exit': 142111744, 'instr_end': 142998528, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143518720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 251, 'bar_enter': 145757184, 'bar_exit': 145759232, 'instr_end': 146397184, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 255, 'bar_enter': 147556352, 'bar_exit': 147590144, 'instr_end': 148409344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 258, 'bar_enter': 149514240, 'bar_exit': 149525504, 'instr_end': 150407168, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153782272, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 269, 'bar_enter': 154972160, 'bar_exit': 154972160, 'instr_end': 155789312, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 272, 'bar_enter': 156760064, 'bar_exit': 156903424, 'instr_end': 157784064, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 56, 'end_b_x': 41, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 276, 'bar_enter': 158085120, 'bar_exit': 158085120, 'instr_end': 158300160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 8, 'end_b_x': 3, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161171456, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 46, 'end_b_x': 35, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 283, 'bar_enter': 162322432, 'bar_exit': 162356224, 'instr_end': 163175424, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 286, 'bar_enter': 164274176, 'bar_exit': 164284416, 'instr_end': 165163008, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 60, 'end_b_x': 45, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 6, 'end_b_x': 2, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 293, 'bar_enter': 167914496, 'bar_exit': 167914496, 'instr_end': 168543232, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 42, 'end_b_x': 31, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 125, 'bar_enter': 63762432, 'bar_exit': 63803392, 'instr_end': 78237696, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 675, 'end_b_x': 450, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 129, 'bar_enter': 79374336, 'bar_exit': 79376384, 'instr_end': 80362496, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 132, 'bar_enter': 81281024, 'bar_exit': 81432576, 'instr_end': 82370560, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82920448, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85269504, 'instr_end': 85948416, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 143, 'bar_enter': 87183360, 'bar_exit': 87222272, 'instr_end': 88092672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 146, 'bar_enter': 89266176, 'bar_exit': 89277440, 'instr_end': 90211328, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93675520, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 157, 'bar_enter': 94920704, 'bar_exit': 94921728, 'instr_end': 95866880, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 160, 'bar_enter': 96750592, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98328576, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 167, 'bar_enter': 100600832, 'bar_exit': 100600832, 'instr_end': 101254144, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 171, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103310336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 174, 'bar_enter': 104442880, 'bar_exit': 104444928, 'instr_end': 105344000, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 181, 'bar_enter': 108162048, 'bar_exit': 108162048, 'instr_end': 108796928, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 185, 'bar_enter': 110040064, 'bar_exit': 110040064, 'instr_end': 110989312, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 188, 'bar_enter': 111875072, 'bar_exit': 112023552, 'instr_end': 112924672, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113454080, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115732480, 'instr_end': 116386816, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 199, 'bar_enter': 117567488, 'bar_exit': 117604352, 'instr_end': 118445056, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 202, 'bar_enter': 119574528, 'bar_exit': 119588864, 'instr_end': 120491008, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121033728, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 209, 'bar_enter': 123313152, 'bar_exit': 123313152, 'instr_end': 123950080, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 213, 'bar_enter': 125179904, 'bar_exit': 125180928, 'instr_end': 126131200, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 216, 'bar_enter': 127021056, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 220, 'bar_enter': 128382976, 'bar_exit': 128384000, 'instr_end': 128604160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 223, 'bar_enter': 130883584, 'bar_exit': 130885632, 'instr_end': 131534848, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 227, 'bar_enter': 132704256, 'bar_exit': 132737024, 'instr_end': 133561344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 230, 'bar_enter': 134664192, 'bar_exit': 134684672, 'instr_end': 135570432, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 237, 'bar_enter': 138334208, 'bar_exit': 138335232, 'instr_end': 138957824, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 241, 'bar_enter': 140171264, 'bar_exit': 140174336, 'instr_end': 141106176, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 244, 'bar_enter': 141976576, 'bar_exit': 142111744, 'instr_end': 142998528, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 248, 'bar_enter': 143299584, 'bar_exit': 143301632, 'instr_end': 143518720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 251, 'bar_enter': 145756160, 'bar_exit': 145759232, 'instr_end': 146398208, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 255, 'bar_enter': 147558400, 'bar_exit': 147590144, 'instr_end': 148407296, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 258, 'bar_enter': 149508096, 'bar_exit': 149525504, 'instr_end': 150407168, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153771008, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 269, 'bar_enter': 154972160, 'bar_exit': 154973184, 'instr_end': 155896832, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 0, 'end_b_x': 10, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 272, 'bar_enter': 156770304, 'bar_exit': 156903424, 'instr_end': 157784064, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 42, 'end_b_x': 27, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 276, 'bar_enter': 158083072, 'bar_exit': 158084096, 'instr_end': 158300160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 4, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161170432, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 36, 'end_b_x': 25, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 283, 'bar_enter': 162321408, 'bar_exit': 162356224, 'instr_end': 163172352, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 4, 'end_b_x': 13, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 286, 'bar_enter': 164276224, 'bar_exit': 164284416, 'instr_end': 165163008, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 46, 'end_b_x': 31, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 3, 'end_b_x': 23, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 293, 'bar_enter': 167914496, 'bar_exit': 167914496, 'instr_end': 168532992, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 32, 'end_b_x': 21, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 125, 'bar_enter': 63792128, 'bar_exit': 63803392, 'instr_end': 78230528, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 451, 'end_b_x': 226, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 129, 'bar_enter': 79374336, 'bar_exit': 79376384, 'instr_end': 80260096, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 132, 'bar_enter': 81280000, 'bar_exit': 81432576, 'instr_end': 82370560, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 136, 'bar_enter': 82690048, 'bar_exit': 82691072, 'instr_end': 82921472, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 139, 'bar_enter': 85269504, 'bar_exit': 85269504, 'instr_end': 85949440, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 143, 'bar_enter': 87183360, 'bar_exit': 87223296, 'instr_end': 88093696, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 146, 'bar_enter': 89277440, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93682688, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 157, 'bar_enter': 94918656, 'bar_exit': 94921728, 'instr_end': 95769600, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 160, 'bar_enter': 96752640, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 164, 'bar_enter': 98106368, 'bar_exit': 98107392, 'instr_end': 98328576, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101254144, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 171, 'bar_enter': 102440960, 'bar_exit': 102473728, 'instr_end': 103311360, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 174, 'bar_enter': 104436736, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 178, 'bar_enter': 105658368, 'bar_exit': 105658368, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 181, 'bar_enter': 108163072, 'bar_exit': 108163072, 'instr_end': 108804096, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 185, 'bar_enter': 110039040, 'bar_exit': 110040064, 'instr_end': 110887936, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 188, 'bar_enter': 111875072, 'bar_exit': 112023552, 'instr_end': 112924672, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 192, 'bar_enter': 113230848, 'bar_exit': 113233920, 'instr_end': 113454080, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 195, 'bar_enter': 115732480, 'bar_exit': 115732480, 'instr_end': 116387840, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 199, 'bar_enter': 117566464, 'bar_exit': 117604352, 'instr_end': 118448128, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 202, 'bar_enter': 119574528, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 209, 'bar_enter': 123313152, 'bar_exit': 123314176, 'instr_end': 123959296, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 213, 'bar_enter': 125178880, 'bar_exit': 125179904, 'instr_end': 126031872, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 216, 'bar_enter': 127033344, 'bar_exit': 127169536, 'instr_end': 128073728, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 220, 'bar_enter': 128380928, 'bar_exit': 128384000, 'instr_end': 128604160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 223, 'bar_enter': 130886656, 'bar_exit': 130886656, 'instr_end': 131534848, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 227, 'bar_enter': 132703232, 'bar_exit': 132737024, 'instr_end': 133563392, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 230, 'bar_enter': 134666240, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 237, 'bar_enter': 138333184, 'bar_exit': 138334208, 'instr_end': 138962944, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 241, 'bar_enter': 140172288, 'bar_exit': 140174336, 'instr_end': 141009920, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 244, 'bar_enter': 141977600, 'bar_exit': 142111744, 'instr_end': 142998528, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 248, 'bar_enter': 143300608, 'bar_exit': 143301632, 'instr_end': 143518720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 251, 'bar_enter': 145758208, 'bar_exit': 145759232, 'instr_end': 146396160, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 255, 'bar_enter': 147557376, 'bar_exit': 147590144, 'instr_end': 148408320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 258, 'bar_enter': 149512192, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 265, 'bar_enter': 153151488, 'bar_exit': 153152512, 'instr_end': 153780224, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 269, 'bar_enter': 154972160, 'bar_exit': 154972160, 'instr_end': 155798528, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 11, 'end_b_x': 21, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 272, 'bar_enter': 156759040, 'bar_exit': 156903424, 'instr_end': 157784064, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 28, 'end_b_x': 13, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 276, 'bar_enter': 158082048, 'bar_exit': 158084096, 'instr_end': 158300160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 0, 'end_b_x': 19, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161170432, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 26, 'end_b_x': 15, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 283, 'bar_enter': 162321408, 'bar_exit': 162356224, 'instr_end': 163173376, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 14, 'end_b_x': 23, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 286, 'bar_enter': 164275200, 'bar_exit': 164284416, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 32, 'end_b_x': 17, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 0, 'end_b_x': 20, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 293, 'bar_enter': 167912448, 'bar_exit': 167913472, 'instr_end': 168543232, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 22, 'end_b_x': 11, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 125, 'bar_enter': 63803392, 'bar_exit': 63803392, 'instr_end': 78228480, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 227, 'end_b_x': 2, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 129, 'bar_enter': 79377408, 'bar_exit': 79377408, 'instr_end': 80261120, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 132, 'bar_enter': 81418240, 'bar_exit': 81432576, 'instr_end': 82370560, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 136, 'bar_enter': 82655232, 'bar_exit': 82691072, 'instr_end': 82921472, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85268480, 'instr_end': 85939200, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 143, 'bar_enter': 87186432, 'bar_exit': 87222272, 'instr_end': 88095744, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 146, 'bar_enter': 89265152, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 153, 'bar_enter': 93041664, 'bar_exit': 93041664, 'instr_end': 93684736, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 157, 'bar_enter': 94921728, 'bar_exit': 94922752, 'instr_end': 95771648, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 160, 'bar_enter': 96899072, 'bar_exit': 96900096, 'instr_end': 97801216, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 164, 'bar_enter': 98074624, 'bar_exit': 98107392, 'instr_end': 98328576, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101243904, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 171, 'bar_enter': 102439936, 'bar_exit': 102473728, 'instr_end': 103312384, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 174, 'bar_enter': 104435712, 'bar_exit': 104444928, 'instr_end': 105345024, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 181, 'bar_enter': 108161024, 'bar_exit': 108162048, 'instr_end': 108807168, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 185, 'bar_enter': 110036992, 'bar_exit': 110040064, 'instr_end': 110892032, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 188, 'bar_enter': 112023552, 'bar_exit': 112023552, 'instr_end': 112924672, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 192, 'bar_enter': 113199104, 'bar_exit': 113233920, 'instr_end': 113455104, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 195, 'bar_enter': 115730432, 'bar_exit': 115732480, 'instr_end': 116376576, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 199, 'bar_enter': 117564416, 'bar_exit': 117604352, 'instr_end': 118446080, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 202, 'bar_enter': 119575552, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 206, 'bar_enter': 120802304, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 209, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123963392, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 213, 'bar_enter': 125177856, 'bar_exit': 125179904, 'instr_end': 126033920, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 216, 'bar_enter': 127151104, 'bar_exit': 127169536, 'instr_end': 128074752, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 220, 'bar_enter': 128347136, 'bar_exit': 128384000, 'instr_end': 128604160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 223, 'bar_enter': 130886656, 'bar_exit': 130886656, 'instr_end': 131526656, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 227, 'bar_enter': 132702208, 'bar_exit': 132737024, 'instr_end': 133562368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 230, 'bar_enter': 134667264, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 237, 'bar_enter': 138331136, 'bar_exit': 138334208, 'instr_end': 138962944, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 241, 'bar_enter': 140171264, 'bar_exit': 140174336, 'instr_end': 141011968, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 244, 'bar_enter': 142108672, 'bar_exit': 142111744, 'instr_end': 142998528, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 248, 'bar_enter': 143265792, 'bar_exit': 143301632, 'instr_end': 143518720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 251, 'bar_enter': 145759232, 'bar_exit': 145759232, 'instr_end': 146385920, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 255, 'bar_enter': 147556352, 'bar_exit': 147590144, 'instr_end': 148410368, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 258, 'bar_enter': 149526528, 'bar_exit': 149526528, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 265, 'bar_enter': 153150464, 'bar_exit': 153152512, 'instr_end': 153780224, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 269, 'bar_enter': 154968064, 'bar_exit': 154972160, 'instr_end': 155800576, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 22, 'end_b_x': 8, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 272, 'bar_enter': 156890112, 'bar_exit': 156903424, 'instr_end': 157784064, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 14, 'end_b_x': 95, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 276, 'bar_enter': 158049280, 'bar_exit': 158085120, 'instr_end': 158300160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 20, 'end_b_x': 15, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161160192, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 16, 'end_b_x': 4, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 283, 'bar_enter': 162318336, 'bar_exit': 162356224, 'instr_end': 163172352, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 0, 'end_b_x': 9, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 286, 'bar_enter': 164284416, 'bar_exit': 164284416, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 18, 'end_b_x': 3, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 21, 'end_b_x': 17, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 293, 'bar_enter': 167913472, 'bar_exit': 167914496, 'instr_end': 168542208, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 12, 'end_b_x': 1, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 125, 'bar_enter': 63803392, 'bar_exit': 63803392, 'instr_end': 78189568, 'spin_wait': 0, 'exec_time': 14, 'op': 18, 'layer': -1, 'bar_idx': 126, 'expected': 192, 'start_b_x': 3, 'end_b_x': 1349, 'inc': 1, 'name': 'Embedding weight gradient', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 129, 'bar_enter': 79376384, 'bar_exit': 79376384, 'instr_end': 80257024, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 11, 'bar_idx': 130, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 132, 'bar_enter': 81428480, 'bar_exit': 81432576, 'instr_end': 82378752, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 11, 'bar_idx': 133, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 136, 'bar_enter': 82655232, 'bar_exit': 82691072, 'instr_end': 82921472, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 11, 'bar_idx': 137, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 139, 'bar_enter': 85268480, 'bar_exit': 85269504, 'instr_end': 85936128, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 11, 'bar_idx': 140, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 143, 'bar_enter': 87187456, 'bar_exit': 87222272, 'instr_end': 88092672, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 10, 'bar_idx': 144, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 146, 'bar_enter': 89263104, 'bar_exit': 89277440, 'instr_end': 90212352, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 10, 'bar_idx': 147, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 150, 'bar_enter': 90527744, 'bar_exit': 90528768, 'instr_end': 90761216, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 10, 'bar_idx': 151, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 153, 'bar_enter': 93040640, 'bar_exit': 93041664, 'instr_end': 93685760, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 10, 'bar_idx': 154, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 157, 'bar_enter': 94920704, 'bar_exit': 94921728, 'instr_end': 95768576, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 9, 'bar_idx': 158, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 160, 'bar_enter': 96883712, 'bar_exit': 96900096, 'instr_end': 97809408, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 9, 'bar_idx': 161, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 164, 'bar_enter': 98072576, 'bar_exit': 98107392, 'instr_end': 98328576, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 9, 'bar_idx': 165, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 167, 'bar_enter': 100599808, 'bar_exit': 100600832, 'instr_end': 101240832, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 9, 'bar_idx': 168, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 171, 'bar_enter': 102441984, 'bar_exit': 102473728, 'instr_end': 103310336, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 8, 'bar_idx': 172, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 174, 'bar_enter': 104436736, 'bar_exit': 104444928, 'instr_end': 105346048, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 8, 'bar_idx': 175, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 178, 'bar_enter': 105653248, 'bar_exit': 105657344, 'instr_end': 105887744, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 8, 'bar_idx': 179, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 181, 'bar_enter': 108161024, 'bar_exit': 108162048, 'instr_end': 108807168, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 8, 'bar_idx': 182, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 185, 'bar_enter': 110036992, 'bar_exit': 110040064, 'instr_end': 110887936, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 7, 'bar_idx': 186, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 188, 'bar_enter': 112005120, 'bar_exit': 112023552, 'instr_end': 112932864, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 7, 'bar_idx': 189, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 192, 'bar_enter': 113198080, 'bar_exit': 113233920, 'instr_end': 113454080, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 7, 'bar_idx': 193, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 195, 'bar_enter': 115729408, 'bar_exit': 115731456, 'instr_end': 116376576, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 7, 'bar_idx': 196, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 199, 'bar_enter': 117568512, 'bar_exit': 117604352, 'instr_end': 118444032, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 6, 'bar_idx': 200, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 202, 'bar_enter': 119588864, 'bar_exit': 119588864, 'instr_end': 120493056, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 6, 'bar_idx': 203, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 206, 'bar_enter': 120803328, 'bar_exit': 120803328, 'instr_end': 121034752, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 6, 'bar_idx': 207, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 209, 'bar_enter': 123314176, 'bar_exit': 123314176, 'instr_end': 123963392, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 6, 'bar_idx': 210, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 213, 'bar_enter': 125178880, 'bar_exit': 125179904, 'instr_end': 126030848, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 5, 'bar_idx': 214, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 216, 'bar_enter': 127151104, 'bar_exit': 127169536, 'instr_end': 128081920, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 5, 'bar_idx': 217, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 220, 'bar_enter': 128347136, 'bar_exit': 128384000, 'instr_end': 128604160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 5, 'bar_idx': 221, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 223, 'bar_enter': 130885632, 'bar_exit': 130885632, 'instr_end': 131522560, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 5, 'bar_idx': 224, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 227, 'bar_enter': 132703232, 'bar_exit': 132737024, 'instr_end': 133561344, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 4, 'bar_idx': 228, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 230, 'bar_enter': 134681600, 'bar_exit': 134684672, 'instr_end': 135572480, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 4, 'bar_idx': 231, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 234, 'bar_enter': 135873536, 'bar_exit': 135874560, 'instr_end': 136100864, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 4, 'bar_idx': 235, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 237, 'bar_enter': 138331136, 'bar_exit': 138333184, 'instr_end': 138966016, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 4, 'bar_idx': 238, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 241, 'bar_enter': 140172288, 'bar_exit': 140174336, 'instr_end': 141005824, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 3, 'bar_idx': 242, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 244, 'bar_enter': 142107648, 'bar_exit': 142111744, 'instr_end': 143006720, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 3, 'bar_idx': 245, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 248, 'bar_enter': 143265792, 'bar_exit': 143301632, 'instr_end': 143518720, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 3, 'bar_idx': 249, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 251, 'bar_enter': 145758208, 'bar_exit': 145759232, 'instr_end': 146384896, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 3, 'bar_idx': 252, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 255, 'bar_enter': 147556352, 'bar_exit': 147590144, 'instr_end': 148408320, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 2, 'bar_idx': 256, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 258, 'bar_enter': 149510144, 'bar_exit': 149525504, 'instr_end': 150408192, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 2, 'bar_idx': 259, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 262, 'bar_enter': 150705152, 'bar_exit': 150706176, 'instr_end': 150930432, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 2, 'bar_idx': 263, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 265, 'bar_enter': 153150464, 'bar_exit': 153152512, 'instr_end': 153782272, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 2, 'bar_idx': 266, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 269, 'bar_enter': 154969088, 'bar_exit': 154972160, 'instr_end': 155798528, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 1, 'bar_idx': 270, 'expected': 768, 'start_b_x': 9, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 272, 'bar_enter': 156889088, 'bar_exit': 156903424, 'instr_end': 157791232, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 1, 'bar_idx': 273, 'expected': 192, 'start_b_x': 0, 'end_b_x': 82, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 276, 'bar_enter': 158051328, 'bar_exit': 158084096, 'instr_end': 158300160, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 1, 'bar_idx': 277, 'expected': 192, 'start_b_x': 16, 'end_b_x': 11, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 279, 'bar_enter': 160531456, 'bar_exit': 160532480, 'instr_end': 161158144, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 1, 'bar_idx': 280, 'expected': 192, 'start_b_x': 5, 'end_b_x': 65, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 283, 'bar_enter': 162320384, 'bar_exit': 162356224, 'instr_end': 163172352, 'spin_wait': 0, 'exec_time': 0, 'op': 22, 'layer': 0, 'bar_idx': 284, 'expected': 768, 'start_b_x': 10, 'end_b_x': 19, 'inc': 1, 'name': 'MLP projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 286, 'bar_enter': 164274176, 'bar_exit': 164283392, 'instr_end': 165164032, 'spin_wait': 0, 'exec_time': 0, 'op': 25, 'layer': 0, 'bar_idx': 287, 'expected': 192, 'start_b_x': 4, 'end_b_x': 85, 'inc': 1, 'name': 'MLP FC backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 290, 'bar_enter': 165464064, 'bar_exit': 165465088, 'instr_end': 165689344, 'spin_wait': 0, 'exec_time': 0, 'op': 29, 'layer': 0, 'bar_idx': 291, 'expected': 192, 'start_b_x': 18, 'end_b_x': 14, 'inc': 1, 'name': 'Attention projection backward weight', 'kernelName': 'mlp_backward_weight_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 293, 'bar_enter': 167912448, 'bar_exit': 167914496, 'instr_end': 168543232, 'spin_wait': 0, 'exec_time': 0, 'op': 32, 'layer': 0, 'bar_idx': 294, 'expected': 192, 'start_b_x': 2, 'end_b_x': 63, 'inc': 1, 'name': 'QKV backward weight', 'kernelName': 'mlp_backward_weight_device'}\n"
     ]
    }
   ],
   "source": [
    "for sm_no, sm in enumerate(timer[0]):\n",
    "    for instr_no, instr in sm.items():\n",
    "        if instr[\"kernelName\"] == \"mlp_backward_weight_device\":\n",
    "            print(instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "id": "c1c17ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 0, 'sm': 0, 'instr': 138, 'bar_enter': 82922496, 'bar_exit': 82931712, 'instr_end': 84455424, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 152, 'bar_enter': 90763264, 'bar_exit': 90763264, 'instr_end': 92318720, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 166, 'bar_enter': 98329600, 'bar_exit': 98338816, 'instr_end': 99823616, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 180, 'bar_enter': 105888768, 'bar_exit': 105889792, 'instr_end': 107395072, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 194, 'bar_enter': 113456128, 'bar_exit': 113465344, 'instr_end': 114940928, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 208, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122553344, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 222, 'bar_enter': 128606208, 'bar_exit': 128615424, 'instr_end': 130098176, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 236, 'bar_enter': 136101888, 'bar_exit': 136101888, 'instr_end': 137615360, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 250, 'bar_enter': 143519744, 'bar_exit': 143528960, 'instr_end': 144989184, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 264, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152450048, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 278, 'bar_enter': 158301184, 'bar_exit': 158310400, 'instr_end': 159771648, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 0, 'instr': 292, 'bar_enter': 165691392, 'bar_exit': 165691392, 'instr_end': 167188480, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 138, 'bar_enter': 82922496, 'bar_exit': 82931712, 'instr_end': 84435968, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 152, 'bar_enter': 90762240, 'bar_exit': 90762240, 'instr_end': 92298240, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 166, 'bar_enter': 98329600, 'bar_exit': 98338816, 'instr_end': 99831808, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 180, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107400192, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 194, 'bar_enter': 113456128, 'bar_exit': 113465344, 'instr_end': 114963456, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 208, 'bar_enter': 121035776, 'bar_exit': 121036800, 'instr_end': 122561536, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 222, 'bar_enter': 128606208, 'bar_exit': 128615424, 'instr_end': 130087936, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 236, 'bar_enter': 136101888, 'bar_exit': 136101888, 'instr_end': 137619456, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 250, 'bar_enter': 143519744, 'bar_exit': 143528960, 'instr_end': 145029120, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 264, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152413184, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 278, 'bar_enter': 158301184, 'bar_exit': 158310400, 'instr_end': 159769600, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 1, 'instr': 292, 'bar_enter': 165690368, 'bar_exit': 165691392, 'instr_end': 167179264, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 138, 'bar_enter': 82922496, 'bar_exit': 82931712, 'instr_end': 84501504, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 152, 'bar_enter': 90763264, 'bar_exit': 90763264, 'instr_end': 92337152, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 166, 'bar_enter': 98329600, 'bar_exit': 98338816, 'instr_end': 99884032, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 180, 'bar_enter': 105889792, 'bar_exit': 105889792, 'instr_end': 107453440, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 194, 'bar_enter': 113456128, 'bar_exit': 113465344, 'instr_end': 115002368, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 208, 'bar_enter': 121035776, 'bar_exit': 121036800, 'instr_end': 122614784, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 222, 'bar_enter': 128606208, 'bar_exit': 128615424, 'instr_end': 130156544, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 236, 'bar_enter': 136101888, 'bar_exit': 136102912, 'instr_end': 137643008, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 250, 'bar_enter': 143520768, 'bar_exit': 143528960, 'instr_end': 145056768, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 264, 'bar_enter': 150931456, 'bar_exit': 150932480, 'instr_end': 152464384, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 278, 'bar_enter': 158301184, 'bar_exit': 158310400, 'instr_end': 159797248, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 2, 'instr': 292, 'bar_enter': 165691392, 'bar_exit': 165691392, 'instr_end': 167233536, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 138, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84461568, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 152, 'bar_enter': 90763264, 'bar_exit': 90763264, 'instr_end': 92283904, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 166, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99823616, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 180, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107440128, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 194, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114947072, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 208, 'bar_enter': 121035776, 'bar_exit': 121036800, 'instr_end': 122535936, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 222, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130106368, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 236, 'bar_enter': 136102912, 'bar_exit': 136102912, 'instr_end': 137624576, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 250, 'bar_enter': 143529984, 'bar_exit': 143529984, 'instr_end': 144997376, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 264, 'bar_enter': 150931456, 'bar_exit': 150932480, 'instr_end': 152455168, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 278, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159772672, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 3, 'instr': 292, 'bar_enter': 165691392, 'bar_exit': 165691392, 'instr_end': 167146496, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84476928, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 151, 'bar_enter': 90763264, 'bar_exit': 90763264, 'instr_end': 92295168, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99845120, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105889792, 'instr_end': 107443200, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114974720, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121036800, 'instr_end': 122558464, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130125824, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 235, 'bar_enter': 136102912, 'bar_exit': 136102912, 'instr_end': 137629696, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 249, 'bar_enter': 143529984, 'bar_exit': 143529984, 'instr_end': 145018880, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152462336, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158311424, 'instr_end': 159826944, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 4, 'instr': 291, 'bar_enter': 165691392, 'bar_exit': 165691392, 'instr_end': 167173120, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 137, 'bar_enter': 82931712, 'bar_exit': 82932736, 'instr_end': 84508672, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 151, 'bar_enter': 90752000, 'bar_exit': 90762240, 'instr_end': 92341248, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99862528, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 179, 'bar_enter': 105878528, 'bar_exit': 105888768, 'instr_end': 107462656, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114984960, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 207, 'bar_enter': 121025536, 'bar_exit': 121035776, 'instr_end': 122613760, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130150400, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 235, 'bar_enter': 136091648, 'bar_exit': 136101888, 'instr_end': 137647104, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 249, 'bar_enter': 143528960, 'bar_exit': 143528960, 'instr_end': 145070080, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 263, 'bar_enter': 150921216, 'bar_exit': 150931456, 'instr_end': 152472576, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159851520, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 5, 'instr': 291, 'bar_enter': 165680128, 'bar_exit': 165690368, 'instr_end': 167201792, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 137, 'bar_enter': 82931712, 'bar_exit': 82932736, 'instr_end': 84473856, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 151, 'bar_enter': 90752000, 'bar_exit': 90762240, 'instr_end': 92251136, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99817472, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 179, 'bar_enter': 105878528, 'bar_exit': 105888768, 'instr_end': 107374592, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114974720, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 207, 'bar_enter': 121025536, 'bar_exit': 121035776, 'instr_end': 122591232, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 221, 'bar_enter': 128614400, 'bar_exit': 128615424, 'instr_end': 130123776, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 235, 'bar_enter': 136091648, 'bar_exit': 136101888, 'instr_end': 137542656, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 249, 'bar_enter': 143528960, 'bar_exit': 143528960, 'instr_end': 144969728, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 263, 'bar_enter': 150921216, 'bar_exit': 150931456, 'instr_end': 152370176, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159744000, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 6, 'instr': 291, 'bar_enter': 165681152, 'bar_exit': 165690368, 'instr_end': 167200768, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 137, 'bar_enter': 82931712, 'bar_exit': 82932736, 'instr_end': 84478976, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 151, 'bar_enter': 90752000, 'bar_exit': 90762240, 'instr_end': 92264448, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99841024, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 179, 'bar_enter': 105878528, 'bar_exit': 105888768, 'instr_end': 107388928, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114967552, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 207, 'bar_enter': 121025536, 'bar_exit': 121035776, 'instr_end': 122565632, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130124800, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 235, 'bar_enter': 136091648, 'bar_exit': 136101888, 'instr_end': 137541632, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 249, 'bar_enter': 143528960, 'bar_exit': 143529984, 'instr_end': 144975872, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 263, 'bar_enter': 150921216, 'bar_exit': 150931456, 'instr_end': 152428544, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159752192, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 7, 'instr': 291, 'bar_enter': 165680128, 'bar_exit': 165690368, 'instr_end': 167144448, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 137, 'bar_enter': 82931712, 'bar_exit': 82932736, 'instr_end': 84495360, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 151, 'bar_enter': 90752000, 'bar_exit': 90762240, 'instr_end': 92278784, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99861504, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 179, 'bar_enter': 105878528, 'bar_exit': 105888768, 'instr_end': 107416576, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114981888, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 207, 'bar_enter': 121025536, 'bar_exit': 121035776, 'instr_end': 122593280, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130137088, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 235, 'bar_enter': 136091648, 'bar_exit': 136101888, 'instr_end': 137555968, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 249, 'bar_enter': 143528960, 'bar_exit': 143529984, 'instr_end': 145029120, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 263, 'bar_enter': 150921216, 'bar_exit': 150931456, 'instr_end': 152389632, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159814656, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 8, 'instr': 291, 'bar_enter': 165680128, 'bar_exit': 165690368, 'instr_end': 167148544, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84459520, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 151, 'bar_enter': 90753024, 'bar_exit': 90762240, 'instr_end': 92321792, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99869696, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 179, 'bar_enter': 105879552, 'bar_exit': 105888768, 'instr_end': 107442176, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 115003392, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 207, 'bar_enter': 121026560, 'bar_exit': 121035776, 'instr_end': 122597376, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130100224, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 235, 'bar_enter': 136092672, 'bar_exit': 136101888, 'instr_end': 137607168, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 249, 'bar_enter': 143529984, 'bar_exit': 143529984, 'instr_end': 144977920, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 263, 'bar_enter': 150922240, 'bar_exit': 150931456, 'instr_end': 152452096, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159814656, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 9, 'instr': 291, 'bar_enter': 165681152, 'bar_exit': 165690368, 'instr_end': 167219200, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84481024, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 151, 'bar_enter': 90753024, 'bar_exit': 90762240, 'instr_end': 92241920, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99848192, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 179, 'bar_enter': 105879552, 'bar_exit': 105888768, 'instr_end': 107365376, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114957312, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 207, 'bar_enter': 121026560, 'bar_exit': 121035776, 'instr_end': 122503168, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 221, 'bar_enter': 128614400, 'bar_exit': 128615424, 'instr_end': 130128896, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 235, 'bar_enter': 136092672, 'bar_exit': 136101888, 'instr_end': 137550848, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 249, 'bar_enter': 143528960, 'bar_exit': 143529984, 'instr_end': 145025024, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 263, 'bar_enter': 150922240, 'bar_exit': 150931456, 'instr_end': 152375296, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159839232, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 10, 'instr': 291, 'bar_enter': 165682176, 'bar_exit': 165690368, 'instr_end': 167130112, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84500480, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 151, 'bar_enter': 90753024, 'bar_exit': 90762240, 'instr_end': 92301312, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99861504, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 179, 'bar_enter': 105879552, 'bar_exit': 105888768, 'instr_end': 107377664, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114985984, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 207, 'bar_enter': 121026560, 'bar_exit': 121035776, 'instr_end': 122551296, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130145280, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 235, 'bar_enter': 136092672, 'bar_exit': 136101888, 'instr_end': 137583616, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 249, 'bar_enter': 143529984, 'bar_exit': 143529984, 'instr_end': 145038336, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 263, 'bar_enter': 150922240, 'bar_exit': 150931456, 'instr_end': 152389632, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159812608, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 11, 'instr': 291, 'bar_enter': 165682176, 'bar_exit': 165690368, 'instr_end': 167163904, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84518912, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 151, 'bar_enter': 90753024, 'bar_exit': 90762240, 'instr_end': 92299264, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99887104, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 179, 'bar_enter': 105879552, 'bar_exit': 105888768, 'instr_end': 107382784, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 115013632, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 207, 'bar_enter': 121026560, 'bar_exit': 121035776, 'instr_end': 122578944, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130165760, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 235, 'bar_enter': 136092672, 'bar_exit': 136101888, 'instr_end': 137547776, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 249, 'bar_enter': 143528960, 'bar_exit': 143528960, 'instr_end': 145059840, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 263, 'bar_enter': 150922240, 'bar_exit': 150931456, 'instr_end': 152370176, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159837184, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 12, 'instr': 291, 'bar_enter': 165681152, 'bar_exit': 165690368, 'instr_end': 167192576, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84526080, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 151, 'bar_enter': 90753024, 'bar_exit': 90762240, 'instr_end': 92235776, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99893248, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 179, 'bar_enter': 105879552, 'bar_exit': 105888768, 'instr_end': 107353088, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 115022848, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 207, 'bar_enter': 121026560, 'bar_exit': 121035776, 'instr_end': 122494976, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130173952, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 235, 'bar_enter': 136092672, 'bar_exit': 136101888, 'instr_end': 137542656, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 249, 'bar_enter': 143528960, 'bar_exit': 143528960, 'instr_end': 145024000, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 263, 'bar_enter': 150922240, 'bar_exit': 150931456, 'instr_end': 152418304, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159789056, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 13, 'instr': 291, 'bar_enter': 165682176, 'bar_exit': 165690368, 'instr_end': 167213056, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84514816, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 151, 'bar_enter': 90753024, 'bar_exit': 90762240, 'instr_end': 92252160, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99857408, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 179, 'bar_enter': 105879552, 'bar_exit': 105888768, 'instr_end': 107371520, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 115016704, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 207, 'bar_enter': 121026560, 'bar_exit': 121035776, 'instr_end': 122528768, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130165760, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 235, 'bar_enter': 136092672, 'bar_exit': 136101888, 'instr_end': 137560064, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 249, 'bar_enter': 143528960, 'bar_exit': 143529984, 'instr_end': 145006592, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 263, 'bar_enter': 150922240, 'bar_exit': 150931456, 'instr_end': 152384512, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159776768, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 44, 'end_b_x': 9, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 14, 'instr': 291, 'bar_enter': 165682176, 'bar_exit': 165690368, 'instr_end': 167148544, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 44, 'end_b_x': 8, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84521984, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 151, 'bar_enter': 90753024, 'bar_exit': 90762240, 'instr_end': 92284928, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99886080, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 179, 'bar_enter': 105879552, 'bar_exit': 105888768, 'instr_end': 107409408, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 115008512, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 207, 'bar_enter': 121026560, 'bar_exit': 121035776, 'instr_end': 122532864, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130166784, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 235, 'bar_enter': 136092672, 'bar_exit': 136101888, 'instr_end': 137541632, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 249, 'bar_enter': 143529984, 'bar_exit': 143529984, 'instr_end': 145018880, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 263, 'bar_enter': 150922240, 'bar_exit': 150931456, 'instr_end': 152374272, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159795200, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 10, 'end_b_x': 23, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 15, 'instr': 291, 'bar_enter': 165682176, 'bar_exit': 165690368, 'instr_end': 167163904, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 9, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 138, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84540416, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 152, 'bar_enter': 90753024, 'bar_exit': 90762240, 'instr_end': 92300288, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 166, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99902464, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 180, 'bar_enter': 105879552, 'bar_exit': 105888768, 'instr_end': 107436032, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 194, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 115026944, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 208, 'bar_enter': 121026560, 'bar_exit': 121035776, 'instr_end': 122540032, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 222, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130179072, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 236, 'bar_enter': 136092672, 'bar_exit': 136101888, 'instr_end': 137562112, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 250, 'bar_enter': 143529984, 'bar_exit': 143529984, 'instr_end': 145070080, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 264, 'bar_enter': 150922240, 'bar_exit': 150931456, 'instr_end': 152411136, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 278, 'bar_enter': 158311424, 'bar_exit': 158311424, 'instr_end': 159852544, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 24, 'end_b_x': 37, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 16, 'instr': 292, 'bar_enter': 165682176, 'bar_exit': 165690368, 'instr_end': 167146496, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 22, 'end_b_x': 34, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84504576, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90763264, 'instr_end': 92258304, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99811328, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107381760, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 114948096, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 207, 'bar_enter': 121034752, 'bar_exit': 121035776, 'instr_end': 122528768, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128616448, 'instr_end': 130098176, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136101888, 'instr_end': 137565184, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 249, 'bar_enter': 143529984, 'bar_exit': 143529984, 'instr_end': 145009664, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152393728, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159789056, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 38, 'end_b_x': 3, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 17, 'instr': 291, 'bar_enter': 165691392, 'bar_exit': 165691392, 'instr_end': 167127040, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 35, 'end_b_x': 47, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 137, 'bar_enter': 82932736, 'bar_exit': 82932736, 'instr_end': 84524032, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 151, 'bar_enter': 90763264, 'bar_exit': 90763264, 'instr_end': 92282880, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 165, 'bar_enter': 98338816, 'bar_exit': 98338816, 'instr_end': 99886080, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107400192, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 193, 'bar_enter': 113465344, 'bar_exit': 113465344, 'instr_end': 115001344, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 207, 'bar_enter': 121034752, 'bar_exit': 121035776, 'instr_end': 122532864, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 221, 'bar_enter': 128615424, 'bar_exit': 128615424, 'instr_end': 130170880, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136102912, 'instr_end': 137588736, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 249, 'bar_enter': 143529984, 'bar_exit': 143529984, 'instr_end': 145064960, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152416256, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 277, 'bar_enter': 158310400, 'bar_exit': 158310400, 'instr_end': 159796224, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 4, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 18, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165691392, 'instr_end': 167170048, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 0, 'end_b_x': 13, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 137, 'bar_enter': 82921472, 'bar_exit': 82931712, 'instr_end': 84543488, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90762240, 'instr_end': 92341248, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 165, 'bar_enter': 98328576, 'bar_exit': 98338816, 'instr_end': 99900416, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107418624, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 193, 'bar_enter': 113455104, 'bar_exit': 113465344, 'instr_end': 115032064, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122589184, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 221, 'bar_enter': 128604160, 'bar_exit': 128615424, 'instr_end': 130187264, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136101888, 'instr_end': 137604096, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 249, 'bar_enter': 143518720, 'bar_exit': 143528960, 'instr_end': 145077248, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152428544, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 277, 'bar_enter': 158300160, 'bar_exit': 158310400, 'instr_end': 159850496, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 18, 'end_b_x': 31, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 19, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165690368, 'instr_end': 167199744, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 14, 'end_b_x': 27, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 137, 'bar_enter': 82921472, 'bar_exit': 82931712, 'instr_end': 84433920, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90762240, 'instr_end': 92277760, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 165, 'bar_enter': 98328576, 'bar_exit': 98338816, 'instr_end': 99802112, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107354112, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 193, 'bar_enter': 113455104, 'bar_exit': 113465344, 'instr_end': 114922496, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 207, 'bar_enter': 121034752, 'bar_exit': 121035776, 'instr_end': 122511360, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 221, 'bar_enter': 128604160, 'bar_exit': 128615424, 'instr_end': 130078720, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136101888, 'instr_end': 137575424, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 249, 'bar_enter': 143518720, 'bar_exit': 143528960, 'instr_end': 145040384, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152410112, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 277, 'bar_enter': 158300160, 'bar_exit': 158310400, 'instr_end': 159802368, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 32, 'end_b_x': 45, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 20, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165690368, 'instr_end': 167148544, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 28, 'end_b_x': 41, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 137, 'bar_enter': 82921472, 'bar_exit': 82931712, 'instr_end': 84426752, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90762240, 'instr_end': 92259328, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 165, 'bar_enter': 98328576, 'bar_exit': 98338816, 'instr_end': 99811328, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107363328, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 193, 'bar_enter': 113455104, 'bar_exit': 113465344, 'instr_end': 114941952, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122527744, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 221, 'bar_enter': 128605184, 'bar_exit': 128615424, 'instr_end': 130083840, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136101888, 'instr_end': 137580544, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 249, 'bar_enter': 143518720, 'bar_exit': 143528960, 'instr_end': 145008640, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152375296, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 277, 'bar_enter': 158300160, 'bar_exit': 158310400, 'instr_end': 159828992, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 46, 'end_b_x': 11, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 21, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165690368, 'instr_end': 167140352, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 42, 'end_b_x': 7, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 137, 'bar_enter': 82921472, 'bar_exit': 82931712, 'instr_end': 84477952, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90763264, 'instr_end': 92296192, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 165, 'bar_enter': 98328576, 'bar_exit': 98338816, 'instr_end': 99861504, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107411456, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 193, 'bar_enter': 113455104, 'bar_exit': 113465344, 'instr_end': 114982912, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122569728, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 221, 'bar_enter': 128605184, 'bar_exit': 128615424, 'instr_end': 130136064, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136101888, 'instr_end': 137601024, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 249, 'bar_enter': 143518720, 'bar_exit': 143528960, 'instr_end': 145037312, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152424448, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 277, 'bar_enter': 158300160, 'bar_exit': 158310400, 'instr_end': 159810560, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 12, 'end_b_x': 25, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 22, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165690368, 'instr_end': 167191552, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 8, 'end_b_x': 21, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 137, 'bar_enter': 82922496, 'bar_exit': 82931712, 'instr_end': 84440064, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90762240, 'instr_end': 92248064, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 165, 'bar_enter': 98329600, 'bar_exit': 98338816, 'instr_end': 99825664, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107397120, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 193, 'bar_enter': 113456128, 'bar_exit': 113465344, 'instr_end': 114953216, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122572800, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 221, 'bar_enter': 128606208, 'bar_exit': 128615424, 'instr_end': 130097152, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136102912, 'instr_end': 137583616, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 249, 'bar_enter': 143519744, 'bar_exit': 143528960, 'instr_end': 145058816, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152413184, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 277, 'bar_enter': 158301184, 'bar_exit': 158310400, 'instr_end': 159833088, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 26, 'end_b_x': 39, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 23, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165690368, 'instr_end': 167204864, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 22, 'end_b_x': 35, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 137, 'bar_enter': 82922496, 'bar_exit': 82931712, 'instr_end': 84436992, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90762240, 'instr_end': 92261376, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 165, 'bar_enter': 98329600, 'bar_exit': 98338816, 'instr_end': 99806208, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107401216, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 193, 'bar_enter': 113456128, 'bar_exit': 113465344, 'instr_end': 114934784, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122523648, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 221, 'bar_enter': 128606208, 'bar_exit': 128615424, 'instr_end': 130084864, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 235, 'bar_enter': 136102912, 'bar_exit': 136102912, 'instr_end': 137590784, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 249, 'bar_enter': 143519744, 'bar_exit': 143528960, 'instr_end': 144988160, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152423424, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 277, 'bar_enter': 158301184, 'bar_exit': 158310400, 'instr_end': 159792128, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 40, 'end_b_x': 4, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 24, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165690368, 'instr_end': 167151616, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 36, 'end_b_x': 1, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 137, 'bar_enter': 82922496, 'bar_exit': 82931712, 'instr_end': 84471808, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 151, 'bar_enter': 90763264, 'bar_exit': 90763264, 'instr_end': 92300288, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 165, 'bar_enter': 98329600, 'bar_exit': 98338816, 'instr_end': 99830784, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107421696, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 193, 'bar_enter': 113456128, 'bar_exit': 113465344, 'instr_end': 114942976, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122570752, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 221, 'bar_enter': 128606208, 'bar_exit': 128615424, 'instr_end': 130115584, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 235, 'bar_enter': 136102912, 'bar_exit': 136102912, 'instr_end': 137606144, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 249, 'bar_enter': 143519744, 'bar_exit': 143528960, 'instr_end': 145030144, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152434688, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 277, 'bar_enter': 158301184, 'bar_exit': 158310400, 'instr_end': 159814656, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 5, 'end_b_x': 17, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 25, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165691392, 'instr_end': 167163904, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 2, 'end_b_x': 15, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 137, 'bar_enter': 82922496, 'bar_exit': 82931712, 'instr_end': 84433920, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90763264, 'instr_end': 92322816, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 165, 'bar_enter': 98329600, 'bar_exit': 98338816, 'instr_end': 99863552, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107440128, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 193, 'bar_enter': 113456128, 'bar_exit': 113465344, 'instr_end': 114935808, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122567680, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 221, 'bar_enter': 128606208, 'bar_exit': 128615424, 'instr_end': 130084864, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136102912, 'instr_end': 137628672, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 249, 'bar_enter': 143519744, 'bar_exit': 143528960, 'instr_end': 145022976, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152452096, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 277, 'bar_enter': 158301184, 'bar_exit': 158310400, 'instr_end': 159821824, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 18, 'end_b_x': 30, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 26, 'instr': 291, 'bar_enter': 165691392, 'bar_exit': 165691392, 'instr_end': 167207936, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 16, 'end_b_x': 29, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 137, 'bar_enter': 82922496, 'bar_exit': 82931712, 'instr_end': 84441088, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 11, 'bar_idx': 138, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 151, 'bar_enter': 90762240, 'bar_exit': 90763264, 'instr_end': 92334080, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 10, 'bar_idx': 152, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 165, 'bar_enter': 98329600, 'bar_exit': 98338816, 'instr_end': 99873792, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 9, 'bar_idx': 166, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 179, 'bar_enter': 105888768, 'bar_exit': 105888768, 'instr_end': 107459584, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 8, 'bar_idx': 180, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 193, 'bar_enter': 113456128, 'bar_exit': 113465344, 'instr_end': 114926592, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 7, 'bar_idx': 194, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 207, 'bar_enter': 121035776, 'bar_exit': 121035776, 'instr_end': 122597376, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 6, 'bar_idx': 208, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 221, 'bar_enter': 128606208, 'bar_exit': 128615424, 'instr_end': 130084864, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 5, 'bar_idx': 222, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 235, 'bar_enter': 136101888, 'bar_exit': 136101888, 'instr_end': 137643008, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 4, 'bar_idx': 236, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 249, 'bar_enter': 143519744, 'bar_exit': 143528960, 'instr_end': 144958464, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 3, 'bar_idx': 250, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 263, 'bar_enter': 150931456, 'bar_exit': 150931456, 'instr_end': 152462336, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 2, 'bar_idx': 264, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 277, 'bar_enter': 158301184, 'bar_exit': 158310400, 'instr_end': 159833088, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 1, 'bar_idx': 278, 'expected': 576, 'start_b_x': 31, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n",
      "{'step': 0, 'sm': 27, 'instr': 291, 'bar_enter': 165690368, 'bar_exit': 165691392, 'instr_end': 167223296, 'spin_wait': 0, 'exec_time': 1, 'op': 30, 'layer': 0, 'bar_idx': 292, 'expected': 576, 'start_b_x': 30, 'end_b_x': 43, 'inc': 1, 'name': 'Attention backward', 'kernelName': 'attention_backward_device'}\n"
     ]
    }
   ],
   "source": [
    "for sm_no, sm in enumerate(timer[0]):\n",
    "    for instr_no, instr in sm.items():\n",
    "        if instr[\"name\"] == \"Attention backward\":\n",
    "            print(instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "id": "4735c9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27,  2,  5,  8, 11, 14, 17, 20,\n",
       "       23, 26,  1,  4,  7, 10, 13, 16, 19, 22, 25,  0,  3,  6,  9, 12, 15,\n",
       "       18, 21, 24, 27,  2,  5,  8, 11, 14, 17, 20, 23, 26,  1,  4,  7, 10,\n",
       "       13, 16, 19, 22, 25,  0,  3,  6,  9, 12, 15, 18, 21, 24])"
      ]
     },
     "execution_count": 1101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.arange(65) * 3 ) % 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "id": "c750a139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 0,\n",
       " 'sm': 6,\n",
       " 'instr': 207,\n",
       " 'bar_enter': 121025536,\n",
       " 'bar_exit': 121035776,\n",
       " 'instr_end': 122591232,\n",
       " 'spin_wait': 0,\n",
       " 'exec_time': 1,\n",
       " 'op': 30,\n",
       " 'layer': 6,\n",
       " 'bar_idx': 208,\n",
       " 'expected': 576,\n",
       " 'start_b_x': 32,\n",
       " 'end_b_x': 45,\n",
       " 'inc': 1,\n",
       " 'name': 'Attention backward',\n",
       " 'kernelName': 'attention_backward_device'}"
      ]
     },
     "execution_count": 1102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer[0][6][207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "id": "b9c536a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 20, 109)"
      ]
     },
     "execution_count": 1103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4 * 768),(4 * 768) % 109, (4 * 768)//28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "id": "7672557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, \n",
      "28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, \n",
      "56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, \n",
      "84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, \n",
      "112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, \n",
      "140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, \n",
      "168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, \n",
      "196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, \n",
      "224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, \n",
      "252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, \n",
      "280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, \n",
      "308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, \n",
      "336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, \n",
      "364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, \n",
      "392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, \n",
      "420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, \n",
      "448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, \n",
      "476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, \n",
      "504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, \n",
      "532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, \n",
      "560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, \n",
      "588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, \n",
      "616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, \n",
      "644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, \n",
      "672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, \n",
      "700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, \n",
      "728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, \n",
      "756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, \n",
      "784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, \n",
      "812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, \n",
      "840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, \n",
      "868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, \n",
      "896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, \n",
      "924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, \n",
      "952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, \n",
      "980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, \n",
      "1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, \n",
      "1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, \n",
      "1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, \n",
      "1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, \n",
      "1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, \n",
      "1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, \n",
      "1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, \n",
      "1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, \n",
      "1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, \n",
      "1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, \n",
      "1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, \n",
      "1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, \n",
      "1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, \n",
      "1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, \n",
      "1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, \n",
      "1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, \n",
      "1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, \n",
      "1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, \n",
      "1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, \n",
      "1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, \n",
      "1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, \n",
      "1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, \n",
      "1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, \n",
      "1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, \n",
      "1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, \n",
      "1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, \n",
      "1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, \n",
      "1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, \n",
      "1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, \n",
      "1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, \n",
      "1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, \n",
      "1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, \n",
      "1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, \n",
      "1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, \n",
      "1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, \n",
      "1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, \n",
      "2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, \n",
      "2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, \n",
      "2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, \n",
      "2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, \n",
      "2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, \n",
      "2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, \n",
      "2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, \n",
      "2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, \n",
      "2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, \n",
      "2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, \n",
      "2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, \n",
      "2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, \n",
      "2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, \n",
      "2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, \n",
      "2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, \n",
      "2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, \n",
      "2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, \n",
      "2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, \n",
      "2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, \n",
      "2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, \n",
      "2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, \n",
      "2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, \n",
      "2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, \n",
      "2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, \n",
      "2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, \n",
      "2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, \n",
      "2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, \n",
      "2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, \n",
      "2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, \n",
      "2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, \n",
      "2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, \n",
      "2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, \n",
      "2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, \n",
      "2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, \n",
      "2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, \n",
      "2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, \n",
      "3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, \n",
      "3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, \n"
     ]
    }
   ],
   "source": [
    "for i in range(110):\n",
    "    for j in range(28):\n",
    "        print(i * 28 + j, end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "id": "9b76e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280, 308, 336, 364, 392, 420, 448, 476, 504, 532, 560, 588, 616, 644, 672, 700, 728, 756, 784, 812, 840, 868, 896, 924, 952, 980, 1008, 1036, 1064, 1092, 1120, 1148, 1176, 1204, 1232, 1260, 1288, 1316, 1344, 1372, 1400, 1428, 1456, 1484, 1512, 1540, 1568, 1596, 1624, 1652, 1680, 1708, 1736, 1764, 1792, 1820, 1848, 1876, 1904, 1932, 1960, 1988, 2016, 2044, 2072, 2100, 2128, 2156, 2184, 2212, 2240, 2268, 2296, 2324, 2352, 2380, 2408, 2436, 2464, 2492, 2520, 2548, 2576, 2604, 2632, 2660, 2688, 2716, 2744, 2772, 2800, 2828, 2856, 2884, 2912, 2940, 2968, 2996, 3024, 3052, 1, 29, 57, 85, 113, 141, 169, 197, 225, 253, 281, 309, 337, 365, 393, 421, 449, 477, 505, 533, 561, 589, 617, 645, 673, 701, 729, 757, 785, 813, 841, 869, 897, 925, 953, 981, 1009, 1037, 1065, 1093, 1121, 1149, 1177, 1205, 1233, 1261, 1289, 1317, 1345, 1373, 1401, 1429, 1457, 1485, 1513, 1541, 1569, 1597, 1625, 1653, 1681, 1709, 1737, 1765, 1793, 1821, 1849, 1877, 1905, 1933, 1961, 1989, 2017, 2045, 2073, 2101, 2129, 2157, 2185, 2213, 2241, 2269, 2297, 2325, 2353, 2381, 2409, 2437, 2465, 2493, 2521, 2549, 2577, 2605, 2633, 2661, 2689, 2717, 2745, 2773, 2801, 2829, 2857, 2885, 2913, 2941, 2969, 2997, 3025, 3053, 2, 30, 58, 86, 114, 142, 170, 198, 226, 254, 282, 310, 338, 366, 394, 422, 450, 478, 506, 534, 562, 590, 618, 646, 674, 702, 730, 758, 786, 814, 842, 870, 898, 926, 954, 982, 1010, 1038, 1066, 1094, 1122, 1150, 1178, 1206, 1234, 1262, 1290, 1318, 1346, 1374, 1402, 1430, 1458, 1486, 1514, 1542, 1570, 1598, 1626, 1654, 1682, 1710, 1738, 1766, 1794, 1822, 1850, 1878, 1906, 1934, 1962, 1990, 2018, 2046, 2074, 2102, 2130, 2158, 2186, 2214, 2242, 2270, 2298, 2326, 2354, 2382, 2410, 2438, 2466, 2494, 2522, 2550, 2578, 2606, 2634, 2662, 2690, 2718, 2746, 2774, 2802, 2830, 2858, 2886, 2914, 2942, 2970, 2998, 3026, 3054, 3, 31, 59, 87, 115, 143, 171, 199, 227, 255, 283, 311, 339, 367, 395, 423, 451, 479, 507, 535, 563, 591, 619, 647, 675, 703, 731, 759, 787, 815, 843, 871, 899, 927, 955, 983, 1011, 1039, 1067, 1095, 1123, 1151, 1179, 1207, 1235, 1263, 1291, 1319, 1347, 1375, 1403, 1431, 1459, 1487, 1515, 1543, 1571, 1599, 1627, 1655, 1683, 1711, 1739, 1767, 1795, 1823, 1851, 1879, 1907, 1935, 1963, 1991, 2019, 2047, 2075, 2103, 2131, 2159, 2187, 2215, 2243, 2271, 2299, 2327, 2355, 2383, 2411, 2439, 2467, 2495, 2523, 2551, 2579, 2607, 2635, 2663, 2691, 2719, 2747, 2775, 2803, 2831, 2859, 2887, 2915, 2943, 2971, 2999, 3027, 3055, 4, 32, 60, 88, 116, 144, 172, 200, 228, 256, 284, 312, 340, 368, 396, 424, 452, 480, 508, 536, 564, 592, 620, 648, 676, 704, 732, 760, 788, 816, 844, 872, 900, 928, 956, 984, 1012, 1040, 1068, 1096, 1124, 1152, 1180, 1208, 1236, 1264, 1292, 1320, 1348, 1376, 1404, 1432, 1460, 1488, 1516, 1544, 1572, 1600, 1628, 1656, 1684, 1712, 1740, 1768, 1796, 1824, 1852, 1880, 1908, 1936, 1964, 1992, 2020, 2048, 2076, 2104, 2132, 2160, 2188, 2216, 2244, 2272, 2300, 2328, 2356, 2384, 2412, 2440, 2468, 2496, 2524, 2552, 2580, 2608, 2636, 2664, 2692, 2720, 2748, 2776, 2804, 2832, 2860, 2888, 2916, 2944, 2972, 3000, 3028, 3056, 5, 33, 61, 89, 117, 145, 173, 201, 229, 257, 285, 313, 341, 369, 397, 425, 453, 481, 509, 537, 565, 593, 621, 649, 677, 705, 733, 761, 789, 817, 845, 873, 901, 929, 957, 985, 1013, 1041, 1069, 1097, 1125, 1153, 1181, 1209, 1237, 1265, 1293, 1321, 1349, 1377, 1405, 1433, 1461, 1489, 1517, 1545, 1573, 1601, 1629, 1657, 1685, 1713, 1741, 1769, 1797, 1825, 1853, 1881, 1909, 1937, 1965, 1993, 2021, 2049, 2077, 2105, 2133, 2161, 2189, 2217, 2245, 2273, 2301, 2329, 2357, 2385, 2413, 2441, 2469, 2497, 2525, 2553, 2581, 2609, 2637, 2665, 2693, 2721, 2749, 2777, 2805, 2833, 2861, 2889, 2917, 2945, 2973, 3001, 3029, 3057, 6, 34, 62, 90, 118, 146, 174, 202, 230, 258, 286, 314, 342, 370, 398, 426, 454, 482, 510, 538, 566, 594, 622, 650, 678, 706, 734, 762, 790, 818, 846, 874, 902, 930, 958, 986, 1014, 1042, 1070, 1098, 1126, 1154, 1182, 1210, 1238, 1266, 1294, 1322, 1350, 1378, 1406, 1434, 1462, 1490, 1518, 1546, 1574, 1602, 1630, 1658, 1686, 1714, 1742, 1770, 1798, 1826, 1854, 1882, 1910, 1938, 1966, 1994, 2022, 2050, 2078, 2106, 2134, 2162, 2190, 2218, 2246, 2274, 2302, 2330, 2358, 2386, 2414, 2442, 2470, 2498, 2526, 2554, 2582, 2610, 2638, 2666, 2694, 2722, 2750, 2778, 2806, 2834, 2862, 2890, 2918, 2946, 2974, 3002, 3030, 3058, 7, 35, 63, 91, 119, 147, 175, 203, 231, 259, 287, 315, 343, 371, 399, 427, 455, 483, 511, 539, 567, 595, 623, 651, 679, 707, 735, 763, 791, 819, 847, 875, 903, 931, 959, 987, 1015, 1043, 1071, 1099, 1127, 1155, 1183, 1211, 1239, 1267, 1295, 1323, 1351, 1379, 1407, 1435, 1463, 1491, 1519, 1547, 1575, 1603, 1631, 1659, 1687, 1715, 1743, 1771, 1799, 1827, 1855, 1883, 1911, 1939, 1967, 1995, 2023, 2051, 2079, 2107, 2135, 2163, 2191, 2219, 2247, 2275, 2303, 2331, 2359, 2387, 2415, 2443, 2471, 2499, 2527, 2555, 2583, 2611, 2639, 2667, 2695, 2723, 2751, 2779, 2807, 2835, 2863, 2891, 2919, 2947, 2975, 3003, 3031, 3059, 8, 36, 64, 92, 120, 148, 176, 204, 232, 260, 288, 316, 344, 372, 400, 428, 456, 484, 512, 540, 568, 596, 624, 652, 680, 708, 736, 764, 792, 820, 848, 876, 904, 932, 960, 988, 1016, 1044, 1072, 1100, 1128, 1156, 1184, 1212, 1240, 1268, 1296, 1324, 1352, 1380, 1408, 1436, 1464, 1492, 1520, 1548, 1576, 1604, 1632, 1660, 1688, 1716, 1744, 1772, 1800, 1828, 1856, 1884, 1912, 1940, 1968, 1996, 2024, 2052, 2080, 2108, 2136, 2164, 2192, 2220, 2248, 2276, 2304, 2332, 2360, 2388, 2416, 2444, 2472, 2500, 2528, 2556, 2584, 2612, 2640, 2668, 2696, 2724, 2752, 2780, 2808, 2836, 2864, 2892, 2920, 2948, 2976, 3004, 3032, 3060, 9, 37, 65, 93, 121, 149, 177, 205, 233, 261, 289, 317, 345, 373, 401, 429, 457, 485, 513, 541, 569, 597, 625, 653, 681, 709, 737, 765, 793, 821, 849, 877, 905, 933, 961, 989, 1017, 1045, 1073, 1101, 1129, 1157, 1185, 1213, 1241, 1269, 1297, 1325, 1353, 1381, 1409, 1437, 1465, 1493, 1521, 1549, 1577, 1605, 1633, 1661, 1689, 1717, 1745, 1773, 1801, 1829, 1857, 1885, 1913, 1941, 1969, 1997, 2025, 2053, 2081, 2109, 2137, 2165, 2193, 2221, 2249, 2277, 2305, 2333, 2361, 2389, 2417, 2445, 2473, 2501, 2529, 2557, 2585, 2613, 2641, 2669, 2697, 2725, 2753, 2781, 2809, 2837, 2865, 2893, 2921, 2949, 2977, 3005, 3033, 3061, 10, 38, 66, 94, 122, 150, 178, 206, 234, 262, 290, 318, 346, 374, 402, 430, 458, 486, 514, 542, 570, 598, 626, 654, 682, 710, 738, 766, 794, 822, 850, 878, 906, 934, 962, 990, 1018, 1046, 1074, 1102, 1130, 1158, 1186, 1214, 1242, 1270, 1298, 1326, 1354, 1382, 1410, 1438, 1466, 1494, 1522, 1550, 1578, 1606, 1634, 1662, 1690, 1718, 1746, 1774, 1802, 1830, 1858, 1886, 1914, 1942, 1970, 1998, 2026, 2054, 2082, 2110, 2138, 2166, 2194, 2222, 2250, 2278, 2306, 2334, 2362, 2390, 2418, 2446, 2474, 2502, 2530, 2558, 2586, 2614, 2642, 2670, 2698, 2726, 2754, 2782, 2810, 2838, 2866, 2894, 2922, 2950, 2978, 3006, 3034, 3062, 11, 39, 67, 95, 123, 151, 179, 207, 235, 263, 291, 319, 347, 375, 403, 431, 459, 487, 515, 543, 571, 599, 627, 655, 683, 711, 739, 767, 795, 823, 851, 879, 907, 935, 963, 991, 1019, 1047, 1075, 1103, 1131, 1159, 1187, 1215, 1243, 1271, 1299, 1327, 1355, 1383, 1411, 1439, 1467, 1495, 1523, 1551, 1579, 1607, 1635, 1663, 1691, 1719, 1747, 1775, 1803, 1831, 1859, 1887, 1915, 1943, 1971, 1999, 2027, 2055, 2083, 2111, 2139, 2167, 2195, 2223, 2251, 2279, 2307, 2335, 2363, 2391, 2419, 2447, 2475, 2503, 2531, 2559, 2587, 2615, 2643, 2671, 2699, 2727, 2755, 2783, 2811, 2839, 2867, 2895, 2923, 2951, 2979, 3007, 3035, 3063, 12, 40, 68, 96, 124, 152, 180, 208, 236, 264, 292, 320, 348, 376, 404, 432, 460, 488, 516, 544, 572, 600, 628, 656, 684, 712, 740, 768, 796, 824, 852, 880, 908, 936, 964, 992, 1020, 1048, 1076, 1104, 1132, 1160, 1188, 1216, 1244, 1272, 1300, 1328, 1356, 1384, 1412, 1440, 1468, 1496, 1524, 1552, 1580, 1608, 1636, 1664, 1692, 1720, 1748, 1776, 1804, 1832, 1860, 1888, 1916, 1944, 1972, 2000, 2028, 2056, 2084, 2112, 2140, 2168, 2196, 2224, 2252, 2280, 2308, 2336, 2364, 2392, 2420, 2448, 2476, 2504, 2532, 2560, 2588, 2616, 2644, 2672, 2700, 2728, 2756, 2784, 2812, 2840, 2868, 2896, 2924, 2952, 2980, 3008, 3036, 3064, 13, 41, 69, 97, 125, 153, 181, 209, 237, 265, 293, 321, 349, 377, 405, 433, 461, 489, 517, 545, 573, 601, 629, 657, 685, 713, 741, 769, 797, 825, 853, 881, 909, 937, 965, 993, 1021, 1049, 1077, 1105, 1133, 1161, 1189, 1217, 1245, 1273, 1301, 1329, 1357, 1385, 1413, 1441, 1469, 1497, 1525, 1553, 1581, 1609, 1637, 1665, 1693, 1721, 1749, 1777, 1805, 1833, 1861, 1889, 1917, 1945, 1973, 2001, 2029, 2057, 2085, 2113, 2141, 2169, 2197, 2225, 2253, 2281, 2309, 2337, 2365, 2393, 2421, 2449, 2477, 2505, 2533, 2561, 2589, 2617, 2645, 2673, 2701, 2729, 2757, 2785, 2813, 2841, 2869, 2897, 2925, 2953, 2981, 3009, 3037, 3065, 14, 42, 70, 98, 126, 154, 182, 210, 238, 266, 294, 322, 350, 378, 406, 434, 462, 490, 518, 546, 574, 602, 630, 658, 686, 714, 742, 770, 798, 826, 854, 882, 910, 938, 966, 994, 1022, 1050, 1078, 1106, 1134, 1162, 1190, 1218, 1246, 1274, 1302, 1330, 1358, 1386, 1414, 1442, 1470, 1498, 1526, 1554, 1582, 1610, 1638, 1666, 1694, 1722, 1750, 1778, 1806, 1834, 1862, 1890, 1918, 1946, 1974, 2002, 2030, 2058, 2086, 2114, 2142, 2170, 2198, 2226, 2254, 2282, 2310, 2338, 2366, 2394, 2422, 2450, 2478, 2506, 2534, 2562, 2590, 2618, 2646, 2674, 2702, 2730, 2758, 2786, 2814, 2842, 2870, 2898, 2926, 2954, 2982, 3010, 3038, 3066, 15, 43, 71, 99, 127, 155, 183, 211, 239, 267, 295, 323, 351, 379, 407, 435, 463, 491, 519, 547, 575, 603, 631, 659, 687, 715, 743, 771, 799, 827, 855, 883, 911, 939, 967, 995, 1023, 1051, 1079, 1107, 1135, 1163, 1191, 1219, 1247, 1275, 1303, 1331, 1359, 1387, 1415, 1443, 1471, 1499, 1527, 1555, 1583, 1611, 1639, 1667, 1695, 1723, 1751, 1779, 1807, 1835, 1863, 1891, 1919, 1947, 1975, 2003, 2031, 2059, 2087, 2115, 2143, 2171, 2199, 2227, 2255, 2283, 2311, 2339, 2367, 2395, 2423, 2451, 2479, 2507, 2535, 2563, 2591, 2619, 2647, 2675, 2703, 2731, 2759, 2787, 2815, 2843, 2871, 2899, 2927, 2955, 2983, 3011, 3039, 3067, 16, 44, 72, 100, 128, 156, 184, 212, 240, 268, 296, 324, 352, 380, 408, 436, 464, 492, 520, 548, 576, 604, 632, 660, 688, 716, 744, 772, 800, 828, 856, 884, 912, 940, 968, 996, 1024, 1052, 1080, 1108, 1136, 1164, 1192, 1220, 1248, 1276, 1304, 1332, 1360, 1388, 1416, 1444, 1472, 1500, 1528, 1556, 1584, 1612, 1640, 1668, 1696, 1724, 1752, 1780, 1808, 1836, 1864, 1892, 1920, 1948, 1976, 2004, 2032, 2060, 2088, 2116, 2144, 2172, 2200, 2228, 2256, 2284, 2312, 2340, 2368, 2396, 2424, 2452, 2480, 2508, 2536, 2564, 2592, 2620, 2648, 2676, 2704, 2732, 2760, 2788, 2816, 2844, 2872, 2900, 2928, 2956, 2984, 3012, 3040, 3068, 17, 45, 73, 101, 129, 157, 185, 213, 241, 269, 297, 325, 353, 381, 409, 437, 465, 493, 521, 549, 577, 605, 633, 661, 689, 717, 745, 773, 801, 829, 857, 885, 913, 941, 969, 997, 1025, 1053, 1081, 1109, 1137, 1165, 1193, 1221, 1249, 1277, 1305, 1333, 1361, 1389, 1417, 1445, 1473, 1501, 1529, 1557, 1585, 1613, 1641, 1669, 1697, 1725, 1753, 1781, 1809, 1837, 1865, 1893, 1921, 1949, 1977, 2005, 2033, 2061, 2089, 2117, 2145, 2173, 2201, 2229, 2257, 2285, 2313, 2341, 2369, 2397, 2425, 2453, 2481, 2509, 2537, 2565, 2593, 2621, 2649, 2677, 2705, 2733, 2761, 2789, 2817, 2845, 2873, 2901, 2929, 2957, 2985, 3013, 3041, 3069, 18, 46, 74, 102, 130, 158, 186, 214, 242, 270, 298, 326, 354, 382, 410, 438, 466, 494, 522, 550, 578, 606, 634, 662, 690, 718, 746, 774, 802, 830, 858, 886, 914, 942, 970, 998, 1026, 1054, 1082, 1110, 1138, 1166, 1194, 1222, 1250, 1278, 1306, 1334, 1362, 1390, 1418, 1446, 1474, 1502, 1530, 1558, 1586, 1614, 1642, 1670, 1698, 1726, 1754, 1782, 1810, 1838, 1866, 1894, 1922, 1950, 1978, 2006, 2034, 2062, 2090, 2118, 2146, 2174, 2202, 2230, 2258, 2286, 2314, 2342, 2370, 2398, 2426, 2454, 2482, 2510, 2538, 2566, 2594, 2622, 2650, 2678, 2706, 2734, 2762, 2790, 2818, 2846, 2874, 2902, 2930, 2958, 2986, 3014, 3042, 3070, 19, 47, 75, 103, 131, 159, 187, 215, 243, 271, 299, 327, 355, 383, 411, 439, 467, 495, 523, 551, 579, 607, 635, 663, 691, 719, 747, 775, 803, 831, 859, 887, 915, 943, 971, 999, 1027, 1055, 1083, 1111, 1139, 1167, 1195, 1223, 1251, 1279, 1307, 1335, 1363, 1391, 1419, 1447, 1475, 1503, 1531, 1559, 1587, 1615, 1643, 1671, 1699, 1727, 1755, 1783, 1811, 1839, 1867, 1895, 1923, 1951, 1979, 2007, 2035, 2063, 2091, 2119, 2147, 2175, 2203, 2231, 2259, 2287, 2315, 2343, 2371, 2399, 2427, 2455, 2483, 2511, 2539, 2567, 2595, 2623, 2651, 2679, 2707, 2735, 2763, 2791, 2819, 2847, 2875, 2903, 2931, 2959, 2987, 3015, 3043, 3071, 20, 48, 76, 104, 132, 160, 188, 216, 244, 272, 300, 328, 356, 384, 412, 440, 468, 496, 524, 552, 580, 608, 636, 664, 692, 720, 748, 776, 804, 832, 860, 888, 916, 944, 972, 1000, 1028, 1056, 1084, 1112, 1140, 1168, 1196, 1224, 1252, 1280, 1308, 1336, 1364, 1392, 1420, 1448, 1476, 1504, 1532, 1560, 1588, 1616, 1644, 1672, 1700, 1728, 1756, 1784, 1812, 1840, 1868, 1896, 1924, 1952, 1980, 2008, 2036, 2064, 2092, 2120, 2148, 2176, 2204, 2232, 2260, 2288, 2316, 2344, 2372, 2400, 2428, 2456, 2484, 2512, 2540, 2568, 2596, 2624, 2652, 2680, 2708, 2736, 2764, 2792, 2820, 2848, 2876, 2904, 2932, 2960, 2988, 3016, 3044, 3072, 49, 77, 105, 133, 161, 189, 217, 245, 273, 301, 329, 357, 385, 413, 441, 469, 497, 525, 553, 581, 609, 637, 665, 693, 721, 749, 777, 805, 833, 861, 889, 917, 945, 973, 1001, 1029, 1057, 1085, 1113, 1141, 1169, 1197, 1225, 1253, 1281, 1309, 1337, 1365, 1393, 1421, 1449, 1477, 1505, 1533, 1561, 1589, 1617, 1645, 1673, 1701, 1729, 1757, 1785, 1813, 1841, 1869, 1897, 1925, 1953, 1981, 2009, 2037, 2065, 2093, 2121, 2149, 2177, 2205, 2233, 2261, 2289, 2317, 2345, 2373, 2401, 2429, 2457, 2485, 2513, 2541, 2569, 2597, 2625, 2653, 2681, 2709, 2737, 2765, 2793, 2821, 2849, 2877, 2905, 2933, 2961, 2989, 3017, 3045, 3073, 50, 78, 106, 134, 162, 190, 218, 246, 274, 302, 330, 358, 386, 414, 442, 470, 498, 526, 554, 582, 610, 638, 666, 694, 722, 750, 778, 806, 834, 862, 890, 918, 946, 974, 1002, 1030, 1058, 1086, 1114, 1142, 1170, 1198, 1226, 1254, 1282, 1310, 1338, 1366, 1394, 1422, 1450, 1478, 1506, 1534, 1562, 1590, 1618, 1646, 1674, 1702, 1730, 1758, 1786, 1814, 1842, 1870, 1898, 1926, 1954, 1982, 2010, 2038, 2066, 2094, 2122, 2150, 2178, 2206, 2234, 2262, 2290, 2318, 2346, 2374, 2402, 2430, 2458, 2486, 2514, 2542, 2570, 2598, 2626, 2654, 2682, 2710, 2738, 2766, 2794, 2822, 2850, 2878, 2906, 2934, 2962, 2990, 3018, 3046, 3074, 51, 79, 107, 135, 163, 191, 219, 247, 275, 303, 331, 359, 387, 415, 443, 471, 499, 527, 555, 583, 611, 639, 667, 695, 723, 751, 779, 807, 835, 863, 891, 919, 947, 975, 1003, 1031, 1059, 1087, 1115, 1143, 1171, 1199, 1227, 1255, 1283, 1311, 1339, 1367, 1395, 1423, 1451, 1479, 1507, 1535, 1563, 1591, 1619, 1647, 1675, 1703, 1731, 1759, 1787, 1815, 1843, 1871, 1899, 1927, 1955, 1983, 2011, 2039, 2067, 2095, 2123, 2151, 2179, 2207, 2235, 2263, 2291, 2319, 2347, 2375, 2403, 2431, 2459, 2487, 2515, 2543, 2571, 2599, 2627, 2655, 2683, 2711, 2739, 2767, 2795, 2823, 2851, 2879, 2907, 2935, 2963, 2991, 3019, 3047, 3075, 52, 80, 108, 136, 164, 192, 220, 248, 276, 304, 332, 360, 388, 416, 444, 472, 500, 528, 556, 584, 612, 640, 668, 696, 724, 752, 780, 808, 836, 864, 892, 920, 948, 976, 1004, 1032, 1060, 1088, 1116, 1144, 1172, 1200, 1228, 1256, 1284, 1312, 1340, 1368, 1396, 1424, 1452, 1480, 1508, 1536, 1564, 1592, 1620, 1648, 1676, 1704, 1732, 1760, 1788, 1816, 1844, 1872, 1900, 1928, 1956, 1984, 2012, 2040, 2068, 2096, 2124, 2152, 2180, 2208, 2236, 2264, 2292, 2320, 2348, 2376, 2404, 2432, 2460, 2488, 2516, 2544, 2572, 2600, 2628, 2656, 2684, 2712, 2740, 2768, 2796, 2824, 2852, 2880, 2908, 2936, 2964, 2992, 3020, 3048, 3076, 53, 81, 109, 137, 165, 193, 221, 249, 277, 305, 333, 361, 389, 417, 445, 473, 501, 529, 557, 585, 613, 641, 669, 697, 725, 753, 781, 809, 837, 865, 893, 921, 949, 977, 1005, 1033, 1061, 1089, 1117, 1145, 1173, 1201, 1229, 1257, 1285, 1313, 1341, 1369, 1397, 1425, 1453, 1481, 1509, 1537, 1565, 1593, 1621, 1649, 1677, 1705, 1733, 1761, 1789, 1817, 1845, 1873, 1901, 1929, 1957, 1985, 2013, 2041, 2069, 2097, 2125, 2153, 2181, 2209, 2237, 2265, 2293, 2321, 2349, 2377, 2405, 2433, 2461, 2489, 2517, 2545, 2573, 2601, 2629, 2657, 2685, 2713, 2741, 2769, 2797, 2825, 2853, 2881, 2909, 2937, 2965, 2993, 3021, 3049, 3077, 54, 82, 110, 138, 166, 194, 222, 250, 278, 306, 334, 362, 390, 418, 446, 474, 502, 530, 558, 586, 614, 642, 670, 698, 726, 754, 782, 810, 838, 866, 894, 922, 950, 978, 1006, 1034, 1062, 1090, 1118, 1146, 1174, 1202, 1230, 1258, 1286, 1314, 1342, 1370, 1398, 1426, 1454, 1482, 1510, 1538, 1566, 1594, 1622, 1650, 1678, 1706, 1734, 1762, 1790, 1818, 1846, 1874, 1902, 1930, 1958, 1986, 2014, 2042, 2070, 2098, 2126, 2154, 2182, 2210, 2238, 2266, 2294, 2322, 2350, 2378, 2406, 2434, 2462, 2490, 2518, 2546, 2574, 2602, 2630, 2658, 2686, 2714, 2742, 2770, 2798, 2826, 2854, 2882, 2910, 2938, 2966, 2994, 3022, 3050, 3078, 55, 83, 111, 139, 167, 195, 223, 251, 279, 307, 335, 363, 391, 419, 447, 475, 503, 531, 559, 587, 615, 643, 671, 699, 727, 755, 783, 811, 839, 867, 895, 923, 951, 979, 1007, 1035, 1063, 1091, 1119, 1147, 1175, 1203, 1231, 1259, 1287, 1315, 1343, 1371, 1399, 1427, 1455, 1483, 1511, 1539, 1567, 1595, 1623, 1651, 1679, 1707, 1735, 1763, 1791, 1819, 1847, 1875, 1903, 1931, 1959, 1987, 2015, 2043, 2071, 2099, 2127, 2155, 2183, 2211, 2239, 2267, 2295, 2323, 2351, 2379, 2407, 2435, 2463, 2491, 2519, 2547, 2575, 2603, 2631, 2659, 2687, 2715, 2743, 2771, 2799, 2827, 2855, 2883, 2911, 2939, 2967, 2995, 3023, 3051, 3079, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "val = [0 for i in range(3800)]\n",
    "for i in range(110):\n",
    "    tot = 0\n",
    "    for j in range(28):\n",
    "        count = 110\n",
    "        if j >= 20:\n",
    "            count -= 1\n",
    "        # print(i + tot, end=\", \")\n",
    "        val[i + tot] = i * 28 + j\n",
    "        tot += count\n",
    "    # print()\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "id": "7619f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "id": "cef77f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = [\n",
    "    {\n",
    "        \"col\": 0,\n",
    "        \"y0\": 0.0,\n",
    "        \"y1\": 1.2,\n",
    "        \"value\": 10,\n",
    "        \"info\": {\"id\": 1, \"label\": \"A\", \"score\": 10}\n",
    "    },\n",
    "    {\n",
    "        \"col\": 0,\n",
    "        \"y0\": 1.2,\n",
    "        \"y1\": 2.0,\n",
    "        \"value\": 40,\n",
    "        \"info\": {\"id\": 2, \"label\": \"B\", \"score\": 40}\n",
    "    },\n",
    "    {\n",
    "        \"col\": 1,\n",
    "        \"y0\": 0.0,\n",
    "        \"y1\": 0.6,\n",
    "        \"value\": 25,\n",
    "        \"info\": {\"id\": 3, \"label\": \"C\", \"score\": 25}\n",
    "    },\n",
    "]\n",
    "\n",
    "boxes = []\n",
    "for sm in timer[0]:\n",
    "    for instr_no, instr in sm.items():\n",
    "        boxes.append({\n",
    "            \"col\" : instr[\"sm\"],\n",
    "            \"y0\" : -instr[\"bar_exit\"] / 1_000_000,\n",
    "            \"y1\" : -instr[\"instr_end\"] / 1_000_000,\n",
    "            \"value\" : instr[\"op\"],\n",
    "            \"info\" : instr\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "id": "dd58f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import plotly.express as px\n",
    "\n",
    "PALETTE = px.colors.qualitative.Alphabet\n",
    "\n",
    "def hash_color(value):\n",
    "    h = int(hashlib.md5(str(value).encode()).hexdigest(), 16)\n",
    "    return PALETTE[h % len(PALETTE)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "id": "37b28bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colorscale = px.colors.sequential.Viridis\n",
    "# vmin, vmax = 0, 100\n",
    "\n",
    "# fig = go.Figure()\n",
    "\n",
    "# for b in boxes:\n",
    "#     x = b[\"col\"]\n",
    "#     y0, y1 = b[\"y0\"], b[\"y1\"]\n",
    "#     i = b[\"info\"]\n",
    "#     info = f\"{i[\"name\"]}<br/>{i[\"kernelName\"]}<br/>exec_time:{i[\"exec_time\"]}ms<br/>spin_wait:{i[\"spin_wait\"]}ms<br/>\"\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=[x-0.5, x+0.5, x+0.5, x-0.5, x-0.5],\n",
    "#             y=[y0, y0, y1, y1, y0],\n",
    "#             fill=\"toself\",\n",
    "#             mode=\"lines\",\n",
    "#             line=dict(width=1, color=\"rgba(0,0,0,0.6)\",),\n",
    "#               # semi-transparent black\n",
    "#             fillcolor=hash_color(b[\"value\"]),\n",
    "#             hoveron=\"fills\",   #  REQUIRED\n",
    "#             name=f\"\",\n",
    "#             # # customdata=[b[\"info\"]],\n",
    "#             hovertemplate=(\n",
    "#                 # \"\"<b>Box info</b><br>\"\n",
    "#                 # \"%{customdata}<br>\"\n",
    "#                 # \"y: %{y}<extra></extra>\"\"\n",
    "#                 \"TEST %{x}, %{y}\"\n",
    "#             ),\n",
    "#             showlegend=False,\n",
    "#             # fill=\"toself\",\n",
    "#             # mode=\"lines\",\n",
    "#             # hoveron=\"fills\",     # required\n",
    "#             # hoverinfo=\"skip\",    # skip point hover\n",
    "#             # hovertemplate=\"TEST<extra></extra>\",\n",
    "\n",
    "            \n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# fig.update_layout(\n",
    "#         height=4000,          # <- make this BIG (200010000 px is fine)\n",
    "#     xaxis=dict(\n",
    "#         title=\"Column\",\n",
    "#         tickmode=\"linear\"\n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title=\"Y\",\n",
    "#         autorange=True,\n",
    "#         fixedrange=False   # allow zoom + scroll\n",
    "#     ),\n",
    "#     # hoverlabel=dict(\n",
    "#     #     bgcolor=\"white\",\n",
    "#     #     font_size=12\n",
    "#     # ),\n",
    "#     hovermode=\"y\"\n",
    "\n",
    "# )\n",
    "\n",
    "# fig.write_html(\n",
    "#     \"grid_boxes.html\",\n",
    "#     include_plotlyjs=\"cdn\",   # smaller file, loads plotly from CDN\n",
    "#     full_html=True\n",
    "# )\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "id": "cf2430bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure()\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[0, 1, 1, 0, 0],\n",
    "#         y=[0, 0, 1, 1, 0],\n",
    "#         mode=\"lines\",\n",
    "#         fill=\"toself\",\n",
    "#         line=dict(width=1, color=\"black\"),\n",
    "#         fillcolor=\"rgba(255,0,0,0.6)\",\n",
    "#         name=\"hi\",\n",
    "#         hoveron=\"fills\",\n",
    "#         hovertemplate=\"HOVER WORKS<extra></extra>\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# fig.update_layout(\n",
    "#     hovermode=\"closest\",\n",
    "#     width=400,\n",
    "#     height=400,\n",
    "# )\n",
    "# fig.write_html(\n",
    "#     \"test.html\",\n",
    "#     include_plotlyjs=\"cdn\",   # smaller file, loads plotly from CDN\n",
    "#     full_html=True\n",
    "# )\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "id": "921eab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bokeh in /venv/main/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /venv/main/lib/python3.12/site-packages (from bokeh) (3.1.4)\n",
      "Requirement already satisfied: contourpy>=1.2 in /venv/main/lib/python3.12/site-packages (from bokeh) (1.3.3)\n",
      "Requirement already satisfied: narwhals>=1.13 in /venv/main/lib/python3.12/site-packages (from bokeh) (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /venv/main/lib/python3.12/site-packages (from bokeh) (2.1.2)\n",
      "Requirement already satisfied: packaging>=16.8 in /venv/main/lib/python3.12/site-packages (from bokeh) (25.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /venv/main/lib/python3.12/site-packages (from bokeh) (2.3.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /venv/main/lib/python3.12/site-packages (from bokeh) (11.0.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /venv/main/lib/python3.12/site-packages (from bokeh) (6.0.2)\n",
      "Requirement already satisfied: tornado>=6.2 in /venv/main/lib/python3.12/site-packages (from bokeh) (6.5.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /venv/main/lib/python3.12/site-packages (from bokeh) (2025.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from Jinja2>=2.9->bokeh) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas>=1.2->bokeh) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas>=1.2->bokeh) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas>=1.2->bokeh) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "id": "a6890c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import numpy as np\n",
    "\n",
    "def pastel_palette(n, saturation=0.65, lightness=0.75):\n",
    "    hues = np.linspace(0, 1, n, endpoint=False)\n",
    "    colors = []\n",
    "    for h in hues:\n",
    "        r, g, b = colorsys.hls_to_rgb(h, lightness, saturation)\n",
    "        colors.append(f\"#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}\")\n",
    "    return colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0c29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "id": "8fc1f384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/gpt2.cu/build/grid_boxes.html'"
      ]
     },
     "execution_count": 1113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, output_file, save\n",
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper\n",
    "from bokeh.palettes import Turbo256\n",
    "\n",
    "# boxes = [\n",
    "#     dict(col=0, y0=0.0, y1=1.2, value=10, info=\"{'id':1,'label':'A'}\"),\n",
    "#     dict(col=0, y0=1.2, y1=2.0, value=40, info=\"{'id':2,'label':'B'}\"),\n",
    "#     dict(col=1, y0=0.0, y1=0.6, value=25, info=\"{'id':3,'label':'C'}\"),\n",
    "# ]\n",
    "\n",
    "from bokeh.palettes import Turbo256\n",
    "import numpy as np\n",
    "\n",
    "# get sorted unique values\n",
    "unique_vals = sorted({b[\"value\"] for b in boxes})\n",
    "palette = pastel_palette(len(unique_vals))\n",
    "\n",
    "value_to_color = dict(zip(unique_vals, palette))\n",
    "\n",
    "\n",
    "\n",
    "source = ColumnDataSource({\n",
    "    \"x0\":   [b[\"col\"]-0.5   for b in boxes],\n",
    "    \"x1\":   [b[\"col\"]+0.5   for b in boxes],\n",
    "    \"y0\":    [b[\"y0\"]    for b in boxes],\n",
    "    \"y1\":    [b[\"y1\"]    for b in boxes],\n",
    "    \"value\": [b[\"value\"] for b in boxes],\n",
    "    \"color\": [value_to_color[b[\"value\"]] for b in boxes],\n",
    "\n",
    "    \"info\":  [f\"{b[\"info\"][\"name\"]}\\n{b[\"info\"][\"kernelName\"]}\\nexec_time:{b[\"info\"][\"exec_time\"]}ms\"  for b in boxes],\n",
    "})\n",
    "\n",
    "# mapper = LinearColorMapper(\n",
    "#     palette=Turbo256,\n",
    "#     low=0,\n",
    "#     high=100\n",
    "# )\n",
    "\n",
    "p = figure(\n",
    "    width=800,\n",
    "    height=3000,          # tall  scrolls naturally\n",
    "    tools=\"pan,wheel_zoom,reset\",\n",
    ")\n",
    "\n",
    "p.quad(\n",
    "    left=\"x0\",\n",
    "    right=\"x1\",\n",
    "    bottom=\"y0\",\n",
    "    top=\"y1\",\n",
    "    source=source,\n",
    "    fill_color=\"color\",   #  direct mapping\n",
    "    line_color=\"black\",\n",
    "    line_width=0.5\n",
    ")\n",
    "\n",
    "p.add_tools(\n",
    "    HoverTool(\n",
    "        tooltips=[\n",
    "            (\"value\", \"@value\"),\n",
    "            (\"info\", \"@info\"),\n",
    "            (\"y-range\", \"@y0  @y1\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------- Y axis ----------\n",
    "p.yaxis.axis_label =  \"time (ms)                   |\"\n",
    "p.yaxis.axis_label_text_font_size = \"22pt\"\n",
    "p.yaxis.axis_label_text_font_style = \"normal\"\n",
    "\n",
    "p.yaxis.major_label_text_font_size = \"16pt\"\n",
    "\n",
    "# Move y-axis title upward\n",
    "p.yaxis.axis_label_standoff = -50\n",
    "p.yaxis.axis_label_align = \"end\"\n",
    "\n",
    "# ---------- X axis ----------\n",
    "p.xaxis.major_label_text_font_size = \"8pt\"\n",
    "\n",
    "\n",
    "num_cols = 28\n",
    "\n",
    "p.xaxis.ticker = list(range(num_cols))\n",
    "p.xaxis.major_label_overrides = {\n",
    "    i: f\"{i}\" for i in range(num_cols)\n",
    "}\n",
    "\n",
    "\n",
    "# Move x-axis to the top\n",
    "# move existing x-axis to the top\n",
    "p.add_layout(p.xaxis[0], 'above')\n",
    "\n",
    "# remove bottom x-axis\n",
    "# p.xaxis.visible = False\n",
    "\n",
    "y_min = min(b[\"y0\"] for b in boxes)\n",
    "\n",
    "from bokeh.models import Range1d\n",
    "p.y_range = Range1d(start=y_min, end=0)\n",
    "\n",
    "from bokeh.models import CustomJSTickFormatter\n",
    "\n",
    "p.yaxis.formatter = CustomJSTickFormatter(\n",
    "    code=\"\"\"\n",
    "        if (tick <= 0) {\n",
    "            return Math.abs(tick).toFixed(2);\n",
    "        } else {\n",
    "            return \"\";\n",
    "        }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from bokeh.models import AdaptiveTicker\n",
    "\n",
    "p.yaxis.ticker = AdaptiveTicker(\n",
    "    desired_num_ticks=20,   # try 1220 for tall plots\n",
    "    min_interval=0,\n",
    ")\n",
    "\n",
    "\n",
    "output_file(\"grid_boxes.html\")\n",
    "save(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec649908",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
